\documentclass[12pt]{article}
\usepackage{amsmath,amssymb,parskip,qtree,custom}
\usepackage[margin=.8in]{geometry}
\begin{document}

\title{CS 240 --- Data Structures and Data Management}
\author{Kevin James}
\date{\vspace{-2ex}Spring 2014}
\maketitle\HRule

\section{Algorithms}
An {\bf algorithm} is a step-by-step process for carrying out a set of operations given an arbitrary problem instance. An algorithm {\bf solves} a problem if, for every instance of the problem, the algorithm finds a valid solution in finite time.

A {\bf program} is an implementation of an algorithm using a specified programming language.

For each problem we can have several algorithms and for each algorithm we can have several programs (implementations).

In practice, given a problem:
\begin{enumerate}
\item Design an algorithm.
\item Assess the correctness of that algorithm.
\item If the algorithm is acceptable, implement it. Otherwise, return to step 1.
\end{enumerate}

When determining the efficiency of algorithms, we tend to be primarily concerned with either the runtime or the memory requirements. In this course, we will focus mostly on the runtime.

To perform runtime analysis, we may simply implement the algorithm and use some method to determine the end-to-end time of the program. Unfortunately, this approach has many variables: test system, programming language, programmer skill, compiler choice, input selection, \dots. This, of course, makes manual implementation a bad approach.

An idealized implementation uses a {\bf Random Access Machine (RAM)} model. RAM systems have constant time access to memory locations and constant time primitive operations, thus the running time is determinable (as the number of memory operations plus the number of primitive operations).

We can also generally use {\bf order notation} to compare multiple algorithms. For the most part, we compare assuming $n$ is very large, since for small values of $n$ the runtime will be miniscule regardless of algorithm.

A timing function is a function $T_A$ such that $T_A : \{Q\} \to \mathbb{R} > 0$. We denote the runtime of a function as $T(f(x))$, for example: $T(3 \times 4)$ may be equal to $0.8ns = 8\text{ops}$. The return value is the number of operations required in the worst-case scenario.

Example: given $T_A(n) = 1 000 000n + 2 000 000 000$ and $T_B(n) = 0.01n^2$, which is `better'? For $n < 100 000 000$, algorithm $B$ is better. Since we only care about large inputs, though, we say $A$ is better overall.

\begin{example}
Prove that $2010n^2 + 1388 = O(n^3)$.

\begin{proof}
We want to prove that for all positive $c \in \mathbb{Z}$, there exists some $n_0$ such that for all $n > n_0$, $2010 n^2 + 1388 \leq cn^3$. So for $n > 1388$ we have $2010n^2 + 1388 \leq 2011n^2 \leq cn^3$ which in turn gives us $2011n^2 \leq cn^3 \iff 2011 \leq cn$. Then this holds given $n_0 = 2010$.
\end{proof}
\end{example}

\begin{definition}
$f(n) = O(g(n))$ if there exists a positive real number $c$ and an integer $n_0 > 0$ such that $\forall n \geq n_0$, $f(n) \leq cg(n)$.
\end{definition}

More concretely, we can say that $f(n) = O(af(n))$ and $b f(n) = O(f(n))$. It's also worth noting that order notation is transitive (e.g.\ $f(n) = O(g(n))$ and $g(n) = O(h(n))$ implies $f(n) = O(h(n))$).

We use five different symbols to denote order notation:
\begin{itemize}
\item $o$ denotes a function \emph{always less} than a given order
\item $O$ denotes a function \emph{less than or equal} to a given order
\item $\Theta$ denotes a function \emph{exactly equal} to a given order
\item $\Omega$ denotes a function \emph{greater than or equal} to a given order
\item $\omega$ denotes a function \emph{always greater} than a given order
\end{itemize}

More concretely, we have the formulae
\begin{definition}[$o$]
$f(n) = o(g(n))$ if for all $c > 0$ there exists a constant $n_0 > 0$ such that $f(n) < c \cdot g(n)$ for all $n \geq n_0$.
\end{definition}

\begin{definition}[$O$]
$f(n) = O(g(n))$ if there exist constants $c > 0$, $n_0 > 0$ such that $f(n) \leq c \cdot g(n)$ for all $n \geq n_0$.
\end{definition}

\begin{definition}[$\Theta$]
$f(n) = \Theta(g(n))$ if there exists constants $c_1, c_2 > 0$ such that $c_1 \cdot g(n) \leq f(n) \leq c_2 \cdot g(n)$.
\end{definition}

\begin{definition}[$\Omega$]
$f(n) = \Omega(g(n))$ if there exist constants $c > 0$, $n_0 > 0$ such that $f(n) \geq c \cdot g(n)$ for all $n \geq n_0$.
\end{definition}

\begin{definition}[$\omega$]
$f(n) = \omega(g(n))$ if for all $c > 0$ there exists a constant $n_0 > 0$ such that $f(n) > c \cdot g(n)$ for all $n \geq n_0$.
\end{definition}

\begin{example}
For the psuedo-function
\begin{verbatim}
function(n):
    sum = 0
    for i=1 to n:
        for j=i to n:
            sum = sum + (i-j)^2
            sum = sum^2
    return sum
\end{verbatim}
we find the order equation
\begin{align*}
&= \Theta(1) + \sum_{i=1}^n \sum_{j=i}^n \Theta(1) + \Theta(1)\\
&= \Theta(1) \sum_{i=1}^n \sum_{j=1}^n 1\\
&= \Theta(1) \sum_{i=1}^n (n-i+1)\\
&= \Theta(1) \bigg( \sum_{i=1}^n n - \sum_{i=1}^n i + \sum_{i=1}^n 1 \bigg)\\
&= \Theta(1) \bigl( n^2 + i^n + n \bigl)\\
&= \Theta(n^2) + \Theta(i^n) + \Theta(n)\\
&= \Theta(i^n)
\end{align*}
\end{example}

\begin{example}
For the psuedo-function
\begin{verbatim}
function(n):
    sum = 0
    for i=1 to n:
        j = i
        while j >= 1:
            sum = sum + i/j
            j = j/2
    return sum
\end{verbatim}
we find the order equation
\begin{align*}
\sum_{i=1}^n \sum_{j=1}^{\log_2 i} c &= \sum_{i=1}^n (c \log_2 i)\\
&= c\bigl(\log 1 + \log 2 + \log 3 + \cdots + \log n\bigl)\\
\text{all }n\text{ of our terms are below }\log n &\hspace{40pt} \text{half of our }n\text{ terms are above }\frac{n}{2}\\
= O(n\log n) &\hspace{40pt}= \Omega(\frac{n}{2}\log\frac{n}{2})\\
&\hspace{40pt}= \Omega(n\log n)\\
&= \Theta(n\log n)
\end{align*}
\end{example}

Since it is too hard to describe the runtime for every possible input, we decide to only describe the worst case behaviour. The worst case gives us a \emph{guarantee} for required completion time and tends to describe most cases.

More formally, we take \[ \max_{I} \{T_A(I)\} \]

\section{Data Types}
\subsection{Stacks}
A {\bf stack} is a collection of items which supports the operations \code{push} (insert an item), \code{pop} (remove the most recently inserted item), \code{peek} (view the last item), and \code{isEmpty}.

The most common ways to implement a stack are as a
\begin{itemize}
\item {\bf linked list}: a pointer to the top of the stack is maintained and is moved whenever an item is inserted or removed, or an
\item {\bf array}: the last item is easy to access, though sometimes resizing the array will be necessary
\end{itemize}

\subsubsection{Dynamic Arrays}
Linked lists support $O(1)$ insertion and deletion, $O(n)$ accessing. Arrays are vice-versa. {\bf Dynamic arrays} offer a compromise: $O(1)$ for both, but can only insert or delete from the end of the list.

\subsection{Queues}
A {\bf queue} is a data structure where you \code{insert} at the end of the list and \code{dequeue} from the front. Implementations are the same as a stack: either an array or a linked list may be used, though a pointer or reference to the first item is necessary.

\subsubsection{Priority Queues}
A {\bf priority queue} is similar to a queue, but each element has a priority attached to it (a numerical ``score''). The elementes are dequeue'd in order of priority. A priority queue supports
\begin{itemize}
\item \code{insert(x, p)}: inserts an element $x$ with priority $p$
\item \code{delete()}: deletes the element with the highest priority $p$
\item \code{peek()}: views the top element
\end{itemize}

One of the most useful applications of priority queues is for sorting: by inserting all elements from an array and then deleting them, we will have the elements returned in a correctly sorted order. Then we simply need to examine the runtime efficiency of \code{insert} and \code{delete} to determine the speed of our sorting algorithm.

There are two common implementations of priority queues: unsorted arrays and heaps.

In an {\bf unsorted array}, insertion takes $O(1)$ time, since the element is simply placed at the end of the array. Deletion must walk the array, then replace the deleted (read: smallest it has seen as it walked) with the last element in the array. Thus, this is $O(n) + O(1) = O(n)$ time. Sorting, then, takes $O(n^2)$ time.

A better method is to use heaps.

\subsection{Heaps}
A {\bf heap} is a \emph{complete} binary tree with the \emph{heap property}. To have a complete binary tree, the tree must be structured such that for each node in the tree that mode must have zero or two children, unless that node is the rightmost leaf in the bottom level (in which case it may have only one). Furthermore, deeper leaves must be leftmost in the tree and all the leaves in the bottom two levels must be consecutive.

The {\bf heap property} is the principle such that all nodes have a lower priority than that of their parent. In the case of multiple elements with the same priority, arbitrary element positioning is possible.

\begin{figure}[ht]
\Tree
[.5
    [.10
        [.12 ] [.14 ]
    ]
    [.7
        [.13 ] []
    ]
]
\caption{\label{fig:minPQheap} A min-PQ tree that satisfies the heap properties}
\end{figure}

This is not a binary search tree, since the larger element does not need to be on the right. In fact, there is no relationship of any kind with a node's siblings---the only relationship is with its parent.

When we remove an item from a heap, we must replace that item with the current lowest element. This will guarantee that the tree remains in a complete form (that all holes are filled). After replacing it with this element, the newly moved element must be sorted: recursively compare its priority with that of its children; if any children have a higher priority, it should swap with its parent. Otherwise, stop recursing.

When inserting, we use the opposite process: the new element is placed at the base of the tree, then it is recursively compared and potentially swapped with its parent until it is in a valid position.

Note that we do not implement tree-like data structures as such, but rather as arrays. The array follows ta format such that the tree represented in figure~\ref{fig:minPQheap} would be placed in an array as $[5, 10, 7, 12, 14, 13]$. More concretely: the parent of a node at index $n$ is at index $\lfloor \frac{n-1}{2} \rfloor$. Its children are located at indices $2n + 1$ and $2n + 2$.

Heap functions have worst case running times bounded as follows
\begin{itemize}
\item Heap insertion: $\Theta(\log n)$
\item Heap deletion: $\Theta(\log n)$
\item Top-down heap creation: $\Theta(n\log n)$
\item Bottom-up heap creation: $O(n)$
\end{itemize}

\subsubsection{Heap Insert}
The item to-be-inserted must be placed in the only possible location to preserve integrity: i.e.\ the bottom level in the first available position. However, this may violate heap ordering procedure; we must perform bottom-up heap swaps until the ordering is satisfied.

To perform a bottom-up heap swap, we must
\begin{itemize}
\item compare node with parent
\item if the node is larger than the parent
\begin{itemize}
\item swap
\item recurse
\end{itemize}
\item else
\begin{itemize}
\item quit
\end{itemize}
\end{itemize}

These swaps can be performed in $\Theta(1)$ time and since the maximum number of swaps is equal to the height of the heap we have an overall time complexity of $\Theta(\log n)$. This worst case would occur when the item to-be-inserted is larger than anything else on the heap, since it would need to swap all the way to the top.

\subsubsection{Heap Delete}
The item to-be-deleted can simply be removed form our heap; however, we must replace this vacated position with the lowest item in our heap. This satisfies the structural integrity of the heap. Afterward, we must follow top-down heap swapping in order to satisfy ordering integrity.

To perform a top-down heap swap, we must
\begin{itemize}
\item compare node with children
\item if the node is smaller than at least one child
\begin{itemize}
\item swap with the largest child
\item recurse
\end{itemize}
\item else
\begin{itemize}
\item quit
\end{itemize}
\end{itemize}

Again, these swaps can be performed in $\Theta(1)$ time and we can perform no more than $\lfloor\log n\rfloor$ swaps. As such, we have worst case time complexity $\Theta(\log n)$ when the deleted item is at the top of the heap and the replacement item is the smallest in the heap.

\subsubsection{Heap Creation}
Given an array in no particular order, our goal is to adjust the ordering such that we have a valid heap.

The obvious approach is to call the heap insertion method once for each item in our input array. This is the {\bf top-down} approach to heap creation.

This can generally done in place, as we can treat the input array as a structurally valid but orderingly incorrect binary tree. As such, each insertion operation simply `ignores' values to its right. Since we are calling out $\Theta(\log n)$ insertion method $n$ times, we see that top-down heap creatoin has runtime complexity $\Theta(n\log n)$.

More formally, we see we have an upper bound such that the cost of the swaps is proportional to the depth of the node (i.e.\ which level it is on) and the number of nodes to process. This depth is given by $O(\log n)$ and the number of nodes by $n$, so we have $O(n\log n)$ as our upper bound. Our lower bound is given as such: the worst case occurs when the input array is sorted in ascending order (i.e.\ requires we perform the maximal number of swaps for each insertion). In this case, we would be required to swap $\lfloor\log n\rfloor$ times for each of our $n$ insertions.

\begin{lemma}
\label{heaphalf}
At least half of the nodes in a heap are in the last two levels (have depth $d \geq h - 1$)
\end{lemma}

Based on lemma~\ref{heaphalf}, we can ignore all levels above the bottom two when computing the lower bound. The worst case number of swaps at level $h-1$ or $h$ is at least $h-1$, thus the overall number of swaps $s \geq \frac{n}{2} (h-1)$ which is $\Omega(n\log n)$.

We can also creation a heap in the {\bf bottom-up} fashion as such: the entire array is transformed into a heap in one iteration, by finding the elements from largest to smallest and inserting them in that order.

We begin in the same way as for top-down creation, by placing our array into a incomplete binary tree (or associating our already-made array with a binary tree). Now, we start with the element in the last position and perform a top-down swap operation for each element in the array (recursing backwards).

This is very similar to the top-down heap creation method, but has different boundaries.

Since our swaps can now be made in the opposite direction, our lowest level of the tree no longer needs to swap and our second lowest level can only possible swap once. As such, over half of our tree has a maximal swapping runtime of $O(1)$ and the remainder of the tree has $O(\log n)$, whereas the top-down approach uses the constant boundary $O(\log n)$.

More concretely, the total number of swaps we can perform is equal to
\begin{align*}
\sum_{i=0}^h i2^{h-i} &< n\sum_{i=0}^\infty \frac{i}{2^i}\\
&< 2n
\end{align*}
and thus we have the upper bound of $O(n)$.

The lower bound is at least $\Omega(n)$ (since we must run our heap creation on $n$ elements), thus we have an overall runtime complexity of $\Theta(n)$.

\section{Sorting}
\subsection{PQ Sort}
As mentioned above, we can sort data using a priority queue. Both \code{insert} and \code{delete} take $O(\log n)$ time, as does recursively sorting any newly moved elements. Thus, for sorting $n$ elements we see that PQ sort will take $O(n\log n)$ time, which is the same as mergesort.

\subsection{Heap Sort}
Heap sort is a specialized PQ sort.

The heap sort algorithm is
\begin{verbatim}
heapSort(A, n):
    H = new Heap(n)
    for i in range(0, n - 1):
        heapInsert(H, A[i])
    for i in range(0, n - 1):
        A[n-1 - i] = heapDeleteMax(H, n-1)
\end{verbatim}
or given our ``heapify'' algorithm
\begin{verbatim}
heapSort(A, n):
    H = heapify(A, n)
    for i in range(0, n - 1):
        A[n-1 - i] = heapDeleteMax(H, n-1)
\end{verbatim}
which gives us a complexity of
\begin{align*}
O(n) + \sum_{i=0}^{n-1} \Theta(\log n) &= O(n) + \Theta(n\log n)\\
&= \Theta(n\log n)
\end{align*}

\subsection{Quick Sort}
The quick sort algorithm works by having one function which selects the optimal pivot point, moving elements such that all items left of the pivot are smaller and vice-versa, then recursing on each side.

\subsubsection{Selecting}
Given an array $A$, we want to find the $k$th smallest (or largest) element. We do this by recursively taking the first element (the {\bf pivot}), then sorting the elements such that any elements smaller than the pivot are on its left (and vice versa). This continues until there are fewer than $k$ elements on the requested side.

\begin{verbatim}
quickSelect(A, k):
    p = randomPivot(A)
    i = partition(A, p)
    if k < i:
        quickSelect(A[0, i-1], k)
    elif k > i:
        quickSelect(A[i+1, n-1], k - i - 1)
    else:
        return p
\end{verbatim}

When selecting, it is only required that we recurse on one side of the pivot (i.e.\ the side which contains the element we are searching for).

The partition function is defined thusly
\begin{verbatim}
partition(A, p):
    swap(A[0], A[p])
    i = 1
    j = n - 1
    while true:
        while i < n and A[i] < A[0]:
            i = i + 1
        while j >= 1 and A[j] > A[0]:
            j = j - 1
        if j < i:
            break
        else:
            swap(A[i], A[j])
\end{verbatim}

In the \emph{worst case}, we have $\Theta(n^2)$ time. This would occur if the selected pivot is always $\max(A)$ and we are searching for the smallest element, for example in the array $[n, n-1, n-2, \cdots, 3, 2, 1]$.

In the \emph{average case}, the probability of selecting a ``good'' pivot is $50\%$. After $r$ recursive calls, this means the array was halved approximately $\frac{r}{2}$ times. After $4\log n$ calls, we must be left with an array of size $1$. Thus we have
\begin{align*}
f(n) &\leq \half T(n) + \half T\bigg(\frac{3n}{4}\bigg) + cn\\
&\leq 2cn + 2cn\bigg(\frac{3n}{4}\bigg) + 2c\bigg(\frac{9n}{16}\bigg) + \cdots + T(1)\\
&\leq \half \bigg[\half T(n) + \half T\bigg(\frac{3n}{4}\bigg) + cn\bigg] + \half \bigg[\half T\bigg(\frac{3n}{4}\bigg) + \half T\bigg(\frac{9n}{16}\bigg) + c \frac{3n}{4}\bigg]\\
&\leq \quarter T(n) + \half T\bigg(\frac{3n}{4}\bigg) + \half cn + \quarter T\bigg(\frac{9n}{16}\bigg) + c\frac{3n}{8}+ cn\\
&\leq T(1) + 2cn\displaystyle\sum_{i=0}^\infty {\bigg(\frac{3}{4}\bigg)}^i\\
&= \Theta(n)
\end{align*}

\subsubsection{Sorting}
The general quick sort algorithm is
\begin{verbatim}
quickSort(A, n):
    if n == 0:
        return
    p = randomPivot(A)
    i = partition(A, p)
    quickSort(A[0, i-1])
    quickSort(A[i+1, n])
\end{verbatim}

The worst case is $\Theta(n^2)$ and the best case is $\Theta(n\log n)$. Fortunately, the worst case is extremely unlikely and the average case is also $\Theta(n\log n)$.

\subsection{Counting Sort}
Assume each element $e$ in an array satisfies $0 \leq e \leq k-1$ for some known value $k$. Then we can use the {\bf counting sort} algorithm on this array, implemented as follows:
\begin{verbatim}
countingSort(A[1..n], k):
    C = [0]*k
    for i = 0 to n-1:
        C[A[i]]++
    for i = 1 to k-1:
        C[i] += C[i-1]
    B = copy(A)
    for i = reverse(0 to n-1):
        C[B[i]]--
        A[C[B[i]]] = B[i]
\end{verbatim}

Counting sort has time complexity $\Theta(n+k)$, space complexity $\Theta(n+k)$, and is a {\bf stable} algorithm (equal items are not reordered).

\subsection{Radix Sort}
Assume we have a set of (probably) unique integers.  Then we can use the {\bf radix sort} algorithm on this set, implemented as follows:
\begin{verbatim}
radixSort(A[1..n], d, k):
    for i = 0 to d - 1:
        countingSort(A, k) //with each x_i as the key
\end{verbatim}

Radix sort has time complexity $\Theta\bigl(d(n+k)\bigl)$ and space complexity $\Theta(n+k)$.

\section{Dictionary}
A {\bf dictionary} is a collection of items, each of which contain a {\bf key} and some {\bf data} and is referred to as a {\bf key-value pair}. Keys can be compared and are (typically) unique.

We define the following functions for a dictionary:
\begin{itemize}
\item \code{search(k)}
\item \code{insert(k, v)}
\item \code{delete(k)}
\item and optionally: \code{join}, \code{isEmpty}, \code{size}, \dots
\end{itemize}

We can implement a dictionary in several ways
\begin{itemize}
\item as an unordered array: has $\Theta(n)$ search and deletion time and $\Theta(1)$ insertion time
\item as a sorted array: has $\Theta(\log n)$ search time and $\Theta(n)$ insertion and deletion time
\item as a linked list: has $\Theta(n)$ search and deletion time and $\Theta(1)$ insertion time
\end{itemize}

\section{Search Trees}
\subsection{Binary Search Trees}
A {\bf BST} is either empty or contains a KVP, left-child BST, and right-child BST. Every key $k_L$ in $T.left$ is less than the root key, every key $k_R$ in $T.right$ is greater. These properties are recursive.

\begin{figure}[ht]
\Tree
[.15
    [.6
        []
        [.10
            [.8 ] [.14 ]
        ]
    ]
    [.25
        [.23 ]
        [.29
            [.27 ] [.50 ]
        ]
    ]
]
\caption{\label{fig:binaryTree} A sample binary search tree}
\end{figure}

\code{search}, \code{insert} and \code{delete} all have time complexities $\Theta(h)$, where $h$ is the height of the tree. $h$ has a best-case value of $\log n$ and a worst-case value of $n$.

\end{document}
