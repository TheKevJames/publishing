\documentclass[12pt]{article}
\usepackage{amsmath,amssymb,bookmark,parskip,qtree,custom}
\usepackage[margin=.8in]{geometry}
\allowdisplaybreaks
\hypersetup{colorlinks,
    citecolor=black,
    filecolor=black,
    linkcolor=black,
    urlcolor=black
}

\begin{document}

\title{CS 240 --- Data Structures and Data Management}
\author{Kevin James}
\date{\vspace{-2ex}Spring 2014}
\maketitle\HRule

\tableofcontents
\newpage

\section{Algorithms}
An {\bf algorithm} is a step-by-step process for carrying out a set of operations given an arbitrary problem instance. An algorithm {\bf solves} a problem if, for every instance of the problem, the algorithm finds a valid solution in finite time.

A {\bf program} is an implementation of an algorithm using a specified programming language.

For each problem we can have several algorithms and for each algorithm we can have several programs (implementations).

In practice, given a problem:
\begin{enumerate}
\item Design an algorithm.
\item Assess the correctness of that algorithm.
\item If the algorithm is acceptable, implement it. Otherwise, return to step 1.
\end{enumerate}

When determining the efficiency of algorithms, we tend to be primarily concerned with either the runtime or the memory requirements. In this course, we will focus mostly on the runtime.

To perform runtime analysis, we may simply implement the algorithm and use some method to determine the end-to-end time of the program. Unfortunately, this approach has many variables: test system, programming language, programmer skill, compiler choice, input selection, \dots. This, of course, makes manual implementation a bad approach.

An idealized implementation uses a {\bf Random Access Machine (RAM)} model. RAM systems have constant time access to memory locations and constant time primitive operations, thus the running time is determinable (as the number of memory operations plus the number of primitive operations).

We can also generally use {\bf order notation} to compare multiple algorithms. For the most part, we compare assuming $n$ is very large, since for small values of $n$ the runtime will be miniscule regardless of algorithm.

A timing function is a function $T_A$ such that $T_A : \{Q\} \to \mathbb{R} > 0$. We denote the runtime of a function as $T(f(x))$, for example: $T(3 \times 4)$ may be equal to $0.8ns = 8\text{ops}$. The return value is the number of operations required in the worst-case scenario.

Example: given $T_A(n) = 1 000 000n + 2 000 000 000$ and $T_B(n) = 0.01n^2$, which is `better'? For $n < 100 000 000$, algorithm $B$ is better. Since we only care about large inputs, though, we say $A$ is better overall.

\begin{definition}
$f(n) \in O(g(n))$ if there exists a positive real number $c$ and an integer $n_0 > 0$ such that $\forall n \geq n_0$, $f(n) \leq cg(n)$.
\end{definition}

\begin{example}
Prove that $2010n^2 + 1388 \in O(n^3)$.
\begin{proof}
We want to prove that for all positive $c \in \mathbb{Z}$, there exists some $n_0$ such that for all $n \geq n_0$, $2010 n^2 + 1388 \leq cn^3$. So for $n > 1388$ we have $2010n^2 + 1388 \leq 2011n^2 \leq cn^3$ which in turn gives us $2011n^2 \leq cn^3 \iff 2011 \leq cn$. Then this holds given $n_0 = 2011$.
\end{proof}
\end{example}

More concretely, we can say that $f(n) \in O(af(n))$ and $b f(n) \in O(f(n))$. It's also worth noting that order notation is transitive (e.g.\ $f(n) \in O(g(n))$ and $g(n) \in O(h(n))$ implies $f(n) \in O(h(n))$).

We use five different symbols to denote order notation:
\begin{itemize}
\item $o$ denotes a function \emph{always less} than a given order
\item $O$ denotes a function \emph{less than or equal} to a given order
\item $\Theta$ denotes a function \emph{exactly equal} to a given order
\item $\Omega$ denotes a function \emph{greater than or equal} to a given order
\item $\omega$ denotes a function \emph{always greater} than a given order
\end{itemize}

More concretely, we have the formulae
\begin{definition}[$o$]
$f(n) \in o(g(n))$ if for all $c > 0$ there exists a constant $n_0 > 0$ such that $f(n) < c \cdot g(n)$ for all $n \geq n_0$.
\end{definition}

\begin{definition}[$O$]
$f(n) \in O(g(n))$ if there exist constants $c > 0$, $n_0 > 0$ such that $f(n) \leq c \cdot g(n)$ for all $n \geq n_0$.
\end{definition}

\begin{definition}[$\Theta$]
$f(n) \in \Theta(g(n))$ if there exists constants $c_1, c_2 > 0$ such that $c_1 \cdot g(n) \leq f(n) \leq c_2 \cdot g(n)$.
\end{definition}

\begin{definition}[$\Omega$]
$f(n) \in \Omega(g(n))$ if there exist constants $c > 0$, $n_0 > 0$ such that $f(n) \geq c \cdot g(n)$ for all $n \geq n_0$.
\end{definition}

\begin{definition}[$\omega$]
$f(n) \in \omega(g(n))$ if for all $c > 0$ there exists a constant $n_0 > 0$ such that $f(n) > c \cdot g(n)$ for all $n \geq n_0$.
\end{definition}

\begin{example}
For the psuedo-function
\begin{verbatim}
function(n):
    sum = 0
    for i=1 to n:
        for j=i to n:
            sum = sum + (i-j)^2
            sum = sum^2
    return sum
\end{verbatim}
we find the order equation
\begin{align*}
\Theta(1) + \sum_{i=1}^n \sum_{j=i}^n \Theta(1) + \Theta(1) &= \Theta(1) \sum_{i=1}^n \sum_{j=1}^n 1\\
&= \Theta(1) \sum_{i=1}^n (n-i+1)\\
&= \Theta(1) \bigg( \sum_{i=1}^n n - \sum_{i=1}^n i + \sum_{i=1}^n 1 \bigg)\\
&= \Theta(1) \bigl( n^2 + i^n + n \bigl)\\
&= \Theta(n^2) + \Theta(i^n) + \Theta(n)\\
&= \Theta(i^n)
\end{align*}
\end{example}

\begin{example}
For the psuedo-function
\begin{verbatim}
function(n):
    sum = 0
    for i=1 to n:
        j = i
        while j >= 1:
            sum = sum + i/j
            j = j/2
    return sum
\end{verbatim}
we find the order equation
\begin{align*}
\sum_{i=1}^n \sum_{j=1}^{\log_2 i} c &= \sum_{i=1}^n (c \log_2 i)\\
&= c\bigl(\log 1 + \log 2 + \log 3 + \cdots + \log n\bigl)\\
\text{all }n\text{ of our terms are below }\log n &\hspace{40pt} \text{half of our }n\text{ terms are above }\frac{n}{2}\\
= O(n\log n) &\hspace{40pt}= \Omega(\frac{n}{2}\log\frac{n}{2})\\
&\hspace{40pt}= \Omega(n\log n)\\
&= \Theta(n\log n)
\end{align*}
\end{example}

Since it is too hard to describe the runtime for every possible input, we decide to only describe the worst case behaviour. The worst case gives us a \emph{guarantee} for required completion time and tends to describe most cases.

More formally, we take \[ \max_{I} \{T_A(I)\} \]

\section{Data Types}
\subsection{Stacks}
A {\bf stack} is a collection of items which supports the operations \code{push} (insert an item), \code{pop} (remove the most recently inserted item), \code{peek} (view the last item), and \code{isEmpty}.

The most common ways to implement a stack are as a
\begin{itemize}
\item {\bf linked list}: a pointer to the top of the stack is maintained and is moved whenever an item is inserted or removed, or an
\item {\bf array}: the last item is easy to access, though sometimes resizing the array will be necessary
\end{itemize}

\subsubsection{Dynamic Arrays}
Linked lists support $O(1)$ insertion and deletion, $O(n)$ accessing. Arrays are vice-versa. {\bf Dynamic arrays} offer a compromise: $O(1)$ for both, but can only insert or delete from the end of the list.

\subsection{Queues}
A {\bf queue} is a data structure where you \code{insert} at the end of the list and \code{dequeue} from the front. Implementations are the same as a stack: either an array or a linked list may be used, though a pointer or reference to the first item is necessary.

\subsubsection{Priority Queues}
A {\bf priority queue} is similar to a queue, but each element has a priority attached to it (a numerical ``score''). The elements are dequeue'd in order of priority. A priority queue supports
\begin{itemize}
\item \code{insert(x, p)}: inserts an element $x$ with priority $p$
\item \code{delete()}: deletes the element with the highest priority $p$
\item \code{peek()}: views the top element
\end{itemize}

One of the most useful applications of priority queues is for sorting: by inserting all elements from an array and then deleting them, we will have the elements returned in a correctly sorted order. Then we simply need to examine the runtime efficiency of \code{insert} and \code{delete} to determine the speed of our sorting algorithm.

There are two common implementations of priority queues: unsorted arrays and heaps.

In an {\bf unsorted array}, insertion takes $O(1)$ time, since the element is simply placed at the end of the array. Deletion must walk the array, then replace the deleted (read: smallest it has seen as it walked) with the last element in the array. Thus, this is $O(n) + O(1) = O(n)$ time. Sorting, then, takes $O(n^2)$ time.

A better method is to use heaps.

\subsection{Heaps}
A {\bf heap} is a \emph{complete} binary tree with the \emph{heap property}. To have a complete binary tree, the tree must be structured such that for each node in the tree that mode must have zero or two children, unless that node is the rightmost leaf in the bottom level (in which case it may have only one). Furthermore, deeper leaves must be leftmost in the tree and all the leaves in the bottom two levels must be consecutive.

The {\bf heap property} is the principle such that all nodes have a lower priority than that of their parent. In the case of multiple elements with the same priority, arbitrary element positioning is possible.

\begin{figure}[ht]
\Tree
[.5
    [.10
        [.12 ] [.14 ]
    ]
    [.7
        [.13 ] []
    ]
]
\caption{\label{fig:minPQheap} A min-PQ tree that satisfies the heap properties}
\end{figure}

This is not a binary search tree, since the larger element does not need to be on the right. In fact, there is no relationship of any kind with a node's siblings---the only relationship is with its parent.

When we remove an item from a heap, we must replace that item with the current lowest element. This will guarantee that the tree remains in a complete form (that all holes are filled). After replacing it with this element, the newly moved element must be sorted: recursively compare its priority with that of its children; if any children have a higher priority, it should swap with its parent. Otherwise, stop recursing.

When inserting, we use the opposite process: the new element is placed at the base of the tree, then it is recursively compared and potentially swapped with its parent until it is in a valid position.

Note that we do not implement tree-like data structures as such, but rather as arrays. The array follows the format such that the tree represented in figure~\ref{fig:minPQheap} would be placed in an array as $[5, 10, 7, 12, 14, 13]$. More concretely: the parent of a node at index $n$ is at index $\lfloor \frac{n-1}{2} \rfloor$. Its children are located at indices $2n + 1$ and $2n + 2$.

Heap functions have worst case running times bounded as follows
\begin{itemize}
\item Heap insertion: $\Theta(\log n)$
\item Heap deletion: $\Theta(\log n)$
\item Top-down heap creation: $\Theta(n\log n)$
\item Bottom-up heap creation: $O(n)$
\end{itemize}

\subsubsection{Heap Insert}
The item to-be-inserted must be placed in the only possible location to preserve integrity: i.e.\ the bottom level in the first available position. However, this may violate heap ordering procedure; we must perform bottom-up heap swaps until the ordering is satisfied.

To perform a bottom-up heap swap, we must
\begin{itemize}
\item compare node with parent
\item if the node is larger than the parent
\begin{itemize}
\item swap
\item recurse
\end{itemize}
\item else
\begin{itemize}
\item quit
\end{itemize}
\end{itemize}

These swaps can be performed in $\Theta(1)$ time and since the maximum number of swaps is equal to the height of the heap we have an overall time complexity of $\Theta(\log n)$. This worst case would occur when the item to-be-inserted is larger than anything else on the heap, since it would need to swap all the way to the top.

\subsubsection{Heap Delete}
The item to-be-deleted can simply be removed form our heap; however, we must replace this vacated position with the lowest item in our heap. This satisfies the structural integrity of the heap. Afterward, we must follow top-down heap swapping in order to satisfy ordering integrity.

To perform a top-down heap swap, we must
\begin{itemize}
\item compare node with children
\item if the node is smaller than at least one child
\begin{itemize}
\item swap with the largest child
\item recurse
\end{itemize}
\item else
\begin{itemize}
\item quit
\end{itemize}
\end{itemize}

Again, these swaps can be performed in $\Theta(1)$ time and we can perform no more than $\lfloor\log n\rfloor$ swaps. As such, we have worst case time complexity $\Theta(\log n)$ when the deleted item is at the top of the heap and the replacement item is the smallest in the heap.

\subsubsection{Heap Creation}
Given an array in no particular order, our goal is to adjust the ordering such that we have a valid heap.

The obvious approach is to call the heap insertion method once for each item in our input array. This is the {\bf top-down} approach to heap creation.

This can generally done in place, as we can treat the input array as a structurally valid but orderingly incorrect binary tree. As such, each insertion operation simply `ignores' values to its right. Since we are calling our $\Theta(\log n)$ insertion method $n$ times, we see that top-down heap creation has runtime complexity $\Theta(n\log n)$.

More formally, we see we have an upper bound such that the cost of the swaps is proportional to the depth of the node (i.e.\ which level it is on) and the number of nodes to process. This depth is given by $O(\log n)$ and the number of nodes by $n$, so we have $O(n\log n)$ as our upper bound. Our lower bound is given as such: the worst case occurs when the input array is sorted in ascending order (i.e.\ requires we perform the maximal number of swaps for each insertion). In this case, we would be required to swap $\lfloor\log n\rfloor$ times for each of our $n$ insertions.

\begin{lemma}
\label{heaphalf}
At least half of the nodes in a heap are in the last two levels (have depth $d \geq h - 1$)
\end{lemma}

Based on lemma~\ref{heaphalf}, we can ignore all levels above the bottom two when computing the lower bound. The worst case number of swaps at level $h-1$ or $h$ is at least $h-1$, thus the overall number of swaps $s \geq \frac{n}{2} (h-1)$ which is $\Omega(n\log n)$.

We can also creation a heap in the {\bf bottom-up} fashion as such: the entire array is transformed into a heap in one iteration, by finding the elements from largest to smallest and inserting them in that order.

We begin in the same way as for top-down creation, by placing our array into a incomplete binary tree (or associating our already-made array with a binary tree). Now, we start with the element in the last position and perform a top-down swap operation for each element in the array (recursing backwards).

This is very similar to the top-down heap creation method, but has different boundaries.

Since our swaps can now be made in the opposite direction, our lowest level of the tree no longer needs to swap and our second lowest level can only possible swap once. As such, over half of our tree has a maximal swapping runtime of $O(1)$ and the remainder of the tree has $O(\log n)$, whereas the top-down approach uses the constant boundary $O(\log n)$.

More concretely, the total number of swaps we can perform is equal to
\begin{align*}
\sum_{i=0}^h i2^{h-i} &< n\sum_{i=0}^\infty \frac{i}{2^i}\\
&< 2n
\end{align*}
and thus we have the upper bound of $O(n)$.

The lower bound is at least $\Omega(n)$ (since we must run our heap creation on $n$ elements), thus we have an overall runtime complexity of $\Theta(n)$.

\section{Sorting}
There exist many sorting algorithms:
\begin{table}[ht]
\centering
  \begin{tabular}{|l|c|c|c|c|}
  \hline
  Algorithm & Time Complexity & Stable & In Place & Memory Complexity \\
  \hline
  \hline
  Selection Sort & $\Theta(n^2)$ & Maybe & Yes & $O(1)$ \\
  Insertion Sort & $\Theta(n^2)$ & Yes & Yes & $O(1)$ \\
  Merge Sort & $\Theta(n\log n)$ & Yes & No & $O(n)$ \\
  Heap Sort & $\Theta(n\log n)$ & No & Yes & $O(1)$ \\
  Quick Sort & $\Theta(n\log n)$ & Maybe & Maybe & $O(\log n)$ or $O(n)$ \\
  Counting Sort & $\Theta(n+k)$ & Yes & No & $O(n+k)$ \\
  Radix Sort Sort & $\Theta(d(n+k))$ & Yes & No & $O(n+k)$ \\
  \hline
  \end{tabular}
\end{table}

For a comparison-based algorithm, we have a lower bound of $\Omega(n\log n)$. For non-comparison-based algorithms, we can achieve $O(n)$ ($O(n+k)$ and $O(d(n+k))$ can be made to be $O(n)$ with a proper selection of $k$ and $d$).

\subsection{PQ Sort}
As mentioned above, we can sort data using a priority queue. Both \code{insert} and \code{delete} take $O(\log n)$ time, as does recursively sorting any newly moved elements. Thus, for sorting $n$ elements we see that PQ sort will take $O(n\log n)$ time, which is the same as mergesort.

\subsection{Heap Sort}
Heap sort is a specialized PQ sort.

The heap sort algorithm is
\begin{verbatim}
heapSort(A, n):
    H = new Heap(n)
    for i in range(0, n - 1):
        heapInsert(H, A[i])
    for i in range(0, n - 1):
        A[n-1 - i] = heapDeleteMax(H, n-1)
\end{verbatim}
or given our ``heapify'' algorithm
\begin{verbatim}
heapSort(A, n):
    H = heapify(A, n)
    for i in range(0, n - 1):
        A[n-1 - i] = heapDeleteMax(H, n-1)
\end{verbatim}
which gives us a complexity of
\begin{align*}
O(n) + \sum_{i=0}^{n-1} \Theta(\log n) &= O(n) + \Theta(n\log n)\\
&= \Theta(n\log n)
\end{align*}

\subsection{Quick Sort}
The quick sort algorithm works by having one function which selects the optimal pivot point, moving elements such that all items left of the pivot are smaller and vice-versa, then recursing on each side.

\subsubsection{Selecting}
Given an array $A$, we want to find the $k$th smallest (or largest) element. We do this by recursively taking the first element (the {\bf pivot}), then sorting the elements such that any elements smaller than the pivot are on its left (and vice versa). This continues until there are fewer than $k$ elements on the requested side.

\begin{verbatim}
quickSelect(A, k):
    p = randomPivot(A)
    i = partition(A, p)
    if k < i:
        quickSelect(A[0, i-1], k)
    elif k > i:
        quickSelect(A[i+1, n-1], k - i - 1)
    else:
        return p
\end{verbatim}

When selecting, it is only required that we recurse on one side of the pivot (i.e.\ the side which contains the element we are searching for).

The partition function is defined thusly
\begin{verbatim}
partition(A, p):
    swap(A[0], A[p])
    i = 1
    j = n - 1
    while true:
        while i < n and A[i] < A[0]:
            i = i + 1
        while j >= 1 and A[j] > A[0]:
            j = j - 1
        if j < i:
            break
        else:
            swap(A[i], A[j])
\end{verbatim}

In the \emph{worst case}, we have $\Theta(n^2)$ time. This would occur if the selected pivot is always $\max(A)$ and we are searching for the smallest element, for example in the array $[n, n-1, n-2, \dots, 3, 2, 1]$.

In the \emph{average case}, the probability of selecting a ``good'' pivot is $50\%$. After $r$ recursive calls, this means the array was halved approximately $\frac{r}{2}$ times. After $4\log n$ calls, we must be left with an array of size $1$. Thus we have
\begin{align*}
f(n) &\leq \half T(n) + \half T\bigg(\frac{3n}{4}\bigg) + cn\\
&\leq 2cn + 2cn\bigg(\frac{3n}{4}\bigg) + 2c\bigg(\frac{9n}{16}\bigg) + \cdots + T(1)\\
&\leq \half \bigg[\half T(n) + \half T\bigg(\frac{3n}{4}\bigg) + cn\bigg] + \half \bigg[\half T\bigg(\frac{3n}{4}\bigg) + \half T\bigg(\frac{9n}{16}\bigg) + c \frac{3n}{4}\bigg]\\
&\leq \quarter T(n) + \half T\bigg(\frac{3n}{4}\bigg) + \half cn + \quarter T\bigg(\frac{9n}{16}\bigg) + c\frac{3n}{8}+ cn\\
&\leq T(1) + 2cn\displaystyle\sum_{i=0}^\infty {\bigg(\frac{3}{4}\bigg)}^i\\
&= \Theta(n)
\end{align*}

\subsubsection{Sorting}
The general quick sort algorithm is
\begin{verbatim}
quickSort(A, n):
    if n == 0:
        return
    p = randomPivot(A)
    i = partition(A, p)
    quickSort(A[0, i-1])
    quickSort(A[i+1, n])
\end{verbatim}

The worst case is $\Theta(n^2)$ and the best case is $\Theta(n\log n)$. Fortunately, the worst case is extremely unlikely and the average case is also $\Theta(n\log n)$.

\subsection{Counting Sort}
Assume each element $e$ in an array satisfies $0 \leq e \leq k-1$ for some known value $k$. Then we can use the {\bf counting sort} algorithm on this array, implemented as follows:
\begin{verbatim}
countingSort(A[1..n], k):
    C = [0]*k
    for i = 0 to n-1:
        C[A[i]]++
    for i = 1 to k-1:
        C[i] += C[i-1]
    B = copy(A)
    for i = reverse(0 to n-1):
        C[B[i]]--
        A[C[B[i]]] = B[i]
\end{verbatim}

Counting sort has time complexity $\Theta(n+k)$, space complexity $\Theta(n+k)$, and is a {\bf stable} algorithm (equal items are not reordered).

\subsection{Radix Sort}
Assume we have a set of (probably) unique integers.  Then we can use the {\bf radix sort} algorithm on this set, implemented as follows:
\begin{verbatim}
radixSort(A[1..n], d, k):
    for i = 0 to d - 1:
        countingSort(A, k) //with each x_i as the key
\end{verbatim}

We can perform radix-sort from either the left or right sides (LSD vs.\ MSD). The MSD-Radix sort, though, can only be performed if all integers have the same number of digits (or are padded with zeroes).

Radix sort has time complexity $\Theta\bigl(d(n+k)\bigl)$ and space complexity $\Theta(n+k)$.

\section{Dictionaries}
A {\bf dictionary} (``associative array'') is a collection of items, each of which contain a {\bf key} and some {\bf data} and is referred to as a {\bf key-value pair}. Keys can be compared and are (typically) unique.

We define the following functions for a dictionary:
\begin{itemize}
\item \code{search(k)}
\item \code{insert(k, v)}
\item \code{delete(k)}
\item and optionally: \code{join}, \code{isEmpty}, \code{size}, \dots
\end{itemize}

We can implement a dictionary in several ways
\begin{itemize}
\item as an unordered array: has $\Theta(n)$ search and deletion time and $\Theta(1)$ insertion time
\item as a sorted array: has $\Theta(\log n)$ search time and $\Theta(n)$ insertion and deletion time
\item as a linked list: has $\Theta(n)$ search and deletion time and $\Theta(1)$ insertion time
\end{itemize}

\subsection{Hashing}
One problem with dictionaries is that we can only address directly if the keys are integers. If we use a {\bf hashing function} $h: U \to \{0, 1, \dots, M-1\}$ (assuming all keys come from some universe $U$), we find $M$ unique keys which we can use for addressing. Unfortunately, $h$ is not injective and we may have many keys mapping to the same integer.

A {\bf hash table dictionary} has an array $T$ of size $M$ (the hash table). An item with key $k$ is stored in $T[h(k)]$, and we can have \code{search}, \code{insert}, and \code{delete} all cost $O(1)$. The challenges to this approach are selecting a good hashing algorithm and dealing with {\bf collisions} (when $h(k_1) = h(k_2)$).

There are two common hashing functions (assuming all keys are integers or can be mapped to integers):
\begin{itemize}
\item the {\bf division method} $h(k) = k \pmod M$ where $M$ is a prime not close to a power of two, and
\item the {\bf multiplication method} $h(k) = \lfloor M(kA - \lfloor kA \rfloor)\rfloor$ from some constant floating-point number $0 < A < 1$. Knuth suggests $A = \frac{\sqrt{5} - 1}{2}$.
\end{itemize}

Even the best functions may have collisions. There are two basic strategies for dealing with the case where we are attempting to insert an item into an already existing location. We could allow multiple items at each location (buckets) or allow each item to go into multiple locations (open addressing). The average cost of collision operations is denoted by the {\bf load factor} $\alpha = \frac{n}{M}$ and we will generally want to rebuild our entire hash table (i.e.\ rehash) when the load factor gets either too large or too small. This should cost roughly $\Theta(n + M)$.

The expected time of the first collision is after $\sqrt{\frac{\pi M}{2}}$ insertions, where $M$ is the size of the table. $M H_M$ items must be inserted before each slot in the table must have at least one item, where $H_M$ is roughly $\log M$.

\subsubsection{Chaining}
To use {\bf chaining} to solve our collisions, each table entry must be a bucket containing zero or more KVPs. This could be implemented with any dictionary (or recursive hashtable). The simplest approach is an unsorted linked list in each bucket.

Assuming we have a uniform hashing function, the average size of each bucket is $\alpha = \frac{n}{M}$. Then we have
\begin{itemize}
\item \code{search} has $\Theta(1+\alpha)$ average-case time and $\Theta(n)$ worst-case time
\item \code{insert} has $O(1)$ worst-case time
\item \code{delete} has $\Theta(1+\alpha)$ average-case time and $\Theta(n)$ worst-case time
\end{itemize}

If we maintain $M \in \Theta(n)$, the average cost for each of these functions is $O(1)$. This can be accomplished by rehashing when $n < c_0M$ or $n > c_1M$ for some $0 < c_0 < c_1$.

\subsubsection{Open Addressing}
We can also use {\bf open addressing} to solve our collision problems. In this case, each hash table entry holds only one item but any key $k$ can be placed in multiple locations. \code{search} and \code{insert} follow a {\bf probe sequence} of possible locations for key $k$: $h(k,0), h(k,1), h(k,2), \dots$. \code{delete} becomes problematic: we must distinguish between \emph{empty} and \emph{deleted} locations. The simplest idea is lineary probing: $h(k,i) = (h(k) + i) \pmod M$.

\begin{theorem}
With linear probing, the average number of probess required to search in a hash table of size $M$ with $\alpha$ load factor $[n = \alpha M$ keys$]$ is \half $\bigl(1 + \frac{1}{1-\alpha}\bigl)$ when successful and \half $\bigl(1 + \frac{1}{{(1-\alpha)}^2}\bigl)$ when unsuccessful.

If the average cost of searches is $t$, then for we set $\alpha < 1 - \frac{1}{\sqrt{t}}$.
\end{theorem}

\subsubsection{Double Hashing}
Suppose we have two hash functions $h_1$ and $h_2$ and that these functions are independant. Then we can define $h(k,i) = h_1(k) + ih_2(k) \pmod m$. Our three standard functions work in the same way as linear probing, but follow this different probe sequence.

In this case, we avoid creating large islands of numbers which must be repeatedly walked through.

\begin{theorem}
With double hashing, the average number of probess required to search in a hash table of size $M$ with $\alpha$ load factor $[n = \alpha M$ keys$]$ is $\frac{1}{\alpha} \ln \bigl(\frac{1}{1-\alpha}\bigl)$ when successful and $\frac{1}{1-\alpha}$ when unsuccessful.

If the average cost of searches is $t$, then for we set $\alpha < 1 - \frac{1}{t}$.
\end{theorem}

\subsubsection{Cuckoo Hashing}
The Cuckoo hashing algorithm also uses two hashing algorithms: the idea, though, is to always insert a new item into $h_1(k)$. If this overwrites another element, we simply re-insert the old element into its alternate position.

The ``alternate position'' is defined as follows: the two hash functions are used to create two completely seperate hash tables. Thus, each element can exist in any one of two possible {\bf nests}.

This gives us $O(1)$ search and deletion as well as amortized $O(1)$ insertion, though it does require mostly random hash functions.

\subsection{Memory Usage}
If we have a very large dictionary which must be stored in external memory, we want to minimize the number of page read/writes and faults.

Linear probing generally requires each hash table to be on the same page; thus, each $\alpha$ must be small and we find ourselves wasting space. We also find ourselves commonly rehashing an entire table.

\subsubsection{Extendible Hashing}
If external memory is stored in blocks of size $S$, our goal should be to access as few as possible. Similar to a B tree of height $1$ and max leaf-size $S$, we can do the following:

We store the {\bf directory} (root node) in internal memory. This directory contains a hashtable of size $2^d$, where $d$ is the order. Each directory points to a block stored in external memory containing at most $S$ items, sorted by hash value.

To search for a key $k$ in the dictionary, we find block $B$ with the first $d$ bits of $H(k)$ as such: $\lfloor \frac{h(k)}{2^{L-d}} \rfloor$. Then, we simply perform a binary search in block $B$ for our hash value. This algorithm is $\Theta(\log S)$ and generates one page fault.

To insert into this dictionary, we search for $h(k)$ to find the proper block for insertion. IF the block has space, we insert our key. Otherwise, we perform a {\bf block split}: we seperate $B$ into $B_0$ and $B_1$ where the items are split by the $k_B$th bit, then update our dictionary references. Note that if our dictionary does not have enough space for a split, we must first double its size and update references accordingly (this is called a {\bf dictionary grow}).

To delete from this dictionary, we perform the reverse operation from insertion: we search for block $B$ and remove $k$ from it. If $n_B$ is too small, we perform a {\bf block merge}, and if every block $B$ has local depth $k_B \leq d - 1$, we perform a {\bf dictionary shrink}.

Both insertion and deletion can be performed in $\Theta(S)$ time, though a dictionary grow/shrink takes $\Theta(2^d)$ time. These operations generate one or two page faults, depending on whether there is a block split/merge.

\subsection{Balanced Search Trees}
We can use either hash tables or balanced search trees for these types of operations. The advantages are as follows:

{\bf BSTs}:
\begin{itemize}
\item $O(\log n)$ worst-case time
\item no assumptions, special functions, or known input proportions
\item no wasted space
\item never need to rebuild entire structure
\end{itemize}

{\bf Hash Tables}:
\begin{itemize}
\item $O(1)$ average cost
\item flexible load factor parameters
\item Cuckoo hashing achieves $O(1)$ worst-case for search and delete
\end{itemize}

Note that both approaches can be adapted to minimize page faults.

\subsection{Multi-Dimensional Data}
An item is said to be multi-dimensional if it has $d > 1$ aspects (coordinates). Each item, then, corresponds to an item in $d$-dimensional space.

Though we can use ordered arrays for one dimensional range searches, we prefer a balanced BST (e.g.\ AVL tree) when generalizing to higher dimensions. Using a balanced tree, we can perform a range search in $O(\log n)$ time and output the answers in $O(k + \log n)$, where $k$ is the number of answers.

\section{Trees}
\subsection{Binary Trees}
Binary trees are the most common tree structure. Their defining factor is nodes with a single value.

\subsubsection{Binary Search Trees}
A {\bf BST} is either empty or contains a KVP, left-child BST, and right-child BST.\@ Every key $k_L$ in $T.left$ is less than the root key, every key $k_R$ in $T.right$ is greater. These properties are recursive.

\begin{figure}[ht]
\Tree
[.15
    [.6
        []
        [.10
            [.8 ] [.14 ]
        ]
    ]
    [.25
        [.23 ]
        [.29
            [.27 ] [.50 ]
        ]
    ]
]
\caption{\label{fig:binaryTree} A sample binary search tree}
\end{figure}

\code{search}, \code{insert} and \code{delete} all have time complexities $\Theta(h)$, where $h$ is the height of the tree. $h$ has a best-case value of $\log n$ and a worst-case value of $n$.

There exist the following BST variations:
\begin{itemize}
\item Randomized BST
\item Self-Balancing BST
\item Threaded BST
\end{itemize}

\subsubsection{AVL Trees}
An {\bf AVL tree} is a BST with an additional structural property: the heights of the left and right subtree must differ by no more than one (where an empty tree is defined to have a height of $-1$).

At each non-empty node, we store $\text{height}{R} - \text{height}(L) \in \{-1, 0, 1\}$: $-1$ means the tree is left-heavy, $1$ means the tree is right-heavy, and $0$ means the tree is balanced. We could store the actual height, but storing the balances is simpler and more convenient.

\begin{figure}[ht]
\Tree
[.z
    [.y
        [.x
            [.A ] [.B ]
        ]
        [.C ]
    ]
    [.D ]
]
\caption{\label{fig:avlTreeUnbalanced} An inbalanced AVL tree.}
\end{figure}

An AVL tree can perform {\bf rotations} to rebalance itself.

\begin{figure}[ht]
\Tree
[.y
    [.x
        [.A ] [.B ]
    ]
    [.z
        [.C ] [.D ]
    ]
]
\caption{\label{fig:avlTreeBalanced} A balanced AVL tree (after rotating right).}
\end{figure}

This function can be represented as
\begin{verbatim}
rotate_right(T):
    newroot = T.left
    T.left = newroot.right
    newroot.right = T
    return newroot

rotate_left(T):
    newroot = T.right
    T.right = newroot.left
    newroot.left = T
\end{verbatim}

We can also perform {\bf double rotations}, which may be either two of the same rotation operation or one of each.

\begin{figure}[ht]
\Tree
[.z
    [.y
        [.A ]
        [.x
            [.B ] [.C ]
        ]
    ]
    [.D ]
]
\caption{\label{fig:avlTreeUnbalancedTwo} An inbalanced AVL tree requiring two rotations (left then right).}
\end{figure}

The \code{search} operation is the same as a BST and is in $\Theta(h)$ time. The \code{insert} uses rotations and will thus be $\Theta(h)$. Finally, the \code{delete} operation swaps then applies fixes (rotations) and is thus $\Theta(h)$.

We define $N(h)$ to be the least number of nodes in an AVL with height $h$. One subtree must have height of at least $h-1$, the other at least $h-2$. Then we have \[ N(h) =
\begin{cases}
1 + N(h-1) + N(h-2) & h \geq 1\\
1 & h = 0\\
0 & h = -1
\end{cases}\]

Red-black trees are another binary tree implementation which is somewhat similar to AVL trees. They have a minsize 1 and a maxsize 3, but represent 2- or 3-nodes as two or three distinct binary nodes with a color (i.e.\ ``red'' or ``black'').

\subsection{B Trees}
A {\bf B tree} is a classification of trees which have a more generalized structure than BSTs. Instead of each node having a single value, B tree nodes have multiple values. All B tree leaves must be at the same level.

A {\bf B tree of minsize $d$} is a B tree with each node containing at most $2d$ values and each non-root node containing at least $d$ values. A B tree of minsize $d$ is typically also refered to as a B tree of order $(2d+1)$, or (even more specifically) a $(d+1)$-$(2d+1)$ tree. For example, a 2-3 tree (introduced below) has $d = 1$.

\subsubsection{2-3 Trees}
A {\bf 2-3 tree} is a BST with additional structural properties:
\begin{itemize}
\item every node either contains one KVP and two children or two KVPS and three children.
\item all the leaves (nodes without children) are at the same level
\end{itemize}

Searching through a 1-node is the same as a BST, but we must examine both keys in a 2-node to determine the most accurate path.

\begin{figure}[ht]
\Tree
[.a
    [.b|c
        [.A ] [.B ] [.C ]
    ]
    [.d|e
        [.D ] [.E ] [.F ]
    ]
]
\caption{\label{fig:23TreeBalanced} A balanced 2-3 tree.}
\end{figure}

To insert into a 2-3 tree, we first search to find the leaf where the key belongs. If the leaf has a single KVP, we simply add the new key and make a 3-node. Otherwise, we sort the keys, split the left-most and right-most into two 1-nodes, and recursively insert the middle key into the parent (along with the link).

To delete, we first swap the KVP with its successor so we are always deleting from a leaf. If the target node is a 2-node, we simply delete the key. Otherwise, if the target node has an immediate 2-node sibling we perform a {\bf transfer} (put the intermediate KVP in the parent between the two nodes into the target node and replace it with the adjacent KVP in the sibling node). Otherwise, we {\bf merge} the target node and a 1-node sibling by removing the target node and recursively deleting the intermediate KVP in the parent and adding it the the sibling node.

Note that the 2-3 tree \code{search}, \code{insert}, and \code{delete} implementations defined above are the same for all B tree implementations.

Each implementation of a B tree (including those not mentioned) has the following variations
\begin{itemize}
\item B Trees: standard implementation
\item B+ Trees: all data stored in leaves (nodes are only used for determining which path to follow)
\item B\# Trees: all data stored in leaves, rotation operation is only defined for siblings
\item B* Trees: all data stored in leaves, nodes are kept $\frac{2}{3}$ full by redistributing to three binary children leaves (two with data, one without)
\end{itemize}

\subsection{Range-Finding Trees}
\subsubsection{Quadtrees}
{\bf Quadtrees} can be used to improve the efficiency of our range searches. If we have $n$ points in $P = \{(x_0,y_0), (x_1,y_1), \dots, (x_{n-1},y_{n-1})\}$, we can build a quadtree as follows:
\begin{enumerate}
\item Find a square $R$ which contains all the points in $P$ (using minimum and maximum $x$ and $y$ values)
\item Let the root of the quadtree correspond to $R$
\item We partition (split) $R$ into four equal subsquares (quadrants), each of which will correspond to a child of $R$
\item Recursively repeat the previous step for each node containing more than one point
\item Delete all leaves which do not contain a point
\end{enumerate}

We can search a quadtree in the same way as a BST.\@ When we insert, we simply place the tem in the correct node and split that node if it contains more than one point. To delete, we search for the point, delete it, and walk back up the tree to delete unnecessary splits.

The complexity of a quadtree range search is $\Theta(n + h)$, no matter the answer (e.g.\ including $\varnothing$). We also define the {\bf spread factor} of points $P$: $\beta(P) = \frac{d_{\max}}{d_{\min}}$, where $d_{\max}$ and $d_{\min}$ are the maximum or minimum distance between two points in $P$.

Note the height of a quadtree is $h \in \Theta(\log_2 \frac{d_{\max}}{d_{\min}})$.

The initial build time complexity of a quadtree is $\Theta(nh)$.

Quadtrees are easy to compute and deal with, but can waste a large amount of space and can have large heights (depending on distribution of points). Furthermore, they are simple to generalize to higher dimensions (e.g.\ octrees).

\subsubsection{KD Trees}
The idea behind {\bf KD trees} is to split the points into two roughly equal sides and thus create a balanced binary tree. We do this by alternating between vertical and horizontal ``splitting lines'' until each section has only one point. This gives us a time complexity of $\Theta(n\log n)$ and the height of the tree $h\in \Theta(\log n)$.

The range search algorithm
\begin{verbatim}
def rangeSearch(T,R):
  if T.empty:
    return
  if T.point in R:
    report T.point
  for each child C in T:
    if C.region intersect R is null:
      rangeSearch(C, R)
\end{verbatim}
has a complexity of $\Theta(k + U)$, where $k$ is the number of keys reported and $U$ is the number of regions we unsuccessfully visit.

We define $Q(n)0$ as the maximum number of regions which intersect a splitting line. We have $Q(n) = 2Q(\quarter n) + O(1)$ which solves to \[ Q(n) = Q(\sqrt(n)) \] and we have time complexity $O(k + \sqrt(n))$.

The query time of a KD tree is $O(2^{1-1d} + n)$. % CHECK THIS

\subsubsection{Range Trees}
A {\bf range tree} is a tree of trees (a multi-level data structure).

% MISSING



\end{document}
