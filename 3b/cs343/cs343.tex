\documentclass[12pt]{article}
\usepackage{amsmath,amssymb,bookmark,parskip,custom}
\usepackage[margin=.8in]{geometry}
\allowdisplaybreaks
\hypersetup{colorlinks,
    citecolor=black,
    filecolor=black,
    linkcolor=black,
    urlcolor=black
}
\setcounter{secnumdepth}{5}

\begin{document}

\title{CS 343 --- Concurrent and Parallel Programming}
\author{Kevin James}
\date{\vspace{-2ex}Fall 2015}
\maketitle\HRule

\tableofcontents
\newpage

\section{Exceptions}
Any contiguous code block can be factored into a helper routing and called from anywhere in the program. This process is called {\bf modularization}. Modularization can fail when the factored block exits (eg. with multi-level labelled exits) because labels are only scoped at function-level. To get around this, we can use global labels, ie:

\begin{verbatim}
label L;
void f() {
    // ...
    goto L;
}

void main() {
    L = L1;
    // ...
    f();
    // not exectuted
  L1:
    L = L2;
    // ...
    f();
    // not executed
  L2:
    return;
}
\end{verbatim}

Fundamentally, we can have routines with two types of returns:
\begin{description}
\item[normal returns] which skip to the statement after the call, and
\item[exceptional returns] which skip to statements \textit{not} after the call
\end{description}

Exceptional returns, though, can lead to the \code{goto} problem, ie. that you can jump anywhere and create spaghetti code.

\subsection{Traditional Approaches}
There are several more traditional approaches to this problem:
\begin{description}
\item[return codes] have return values which indicate normal or exceptional execution. These codes must be checked every time the function is called and mix normal return values with exceptional ones.
\item[status flags] set global variables indicating normal or exceptional execution. These values will be over-written by subsequent function calls and must be checked before this happens.
\item[fix-up routines] are passed into other routines; if a problem is detected, the fix-up routine returns a different result in place of the original function.
\end{description}

Note that we often combine these techniques.

\subsection{Exception Handling}
Compelx control-flow among routines is often called {\bf exception handling}. An exceptional event is one that is (usually) known to exists, but which is ancillary to the algorithm (eg. occurs with low frequency). An {\bf exception handling mechanism} (EHM) provides some alternate type of control flow.

The {\bf execution environment} has a significant effect on an EHM: eg. object-oriented environments require much more complex EHMs. Example: objects have destructores which must be executed no matter how the object ends, even if an exception is thrown.

Control structures with \code{finally} clauses must always be executed. This complicates the EHM, since now both destructors and \code{finally}s must be called. Given multiple execution stacks, an EHM must be even more sophisticated (eg. propogate the exception to another stack if no handler is found in the current one).

\subsubsection{Terms}
\begin{description}
\item[execution] is the language unit in which an exception can be raised, usually any entity with its own runtime stack.
\item[exception type] is a type name representing an exceptional event.
\item[exception] is an instance of an exception type, generated by executing an operation indicating an ancillary (exceptional) situation in execution.
\item[raise (throw)] is the special operation that creates an exception.
\item[source execution] is the execution raising an exception.
\item[faulting execution] is the execution changing control flow due to a raised exception.
\item[local exception] is when an exception is raised and handled by the same execution: source = faulting.
\item[non-local exception] is when an exception is raised by a source execution but delivered to a different faulting execution: source $\neq$ faulting.
\item[concurrent exception] is a non-local exception, where the source and faulting executions are executing concurrently.
\item[propagation] directs control from a raise in the source execution to a handler in the faulting execution.
\item[propagation mechanism] is the rules used to locate a handler. The most common propagation-mechanisms give precedence to handlers higher in the lexical/call stack.
\begin{itemize}
\item specificity versus generality
\item efficient linear search during propagation
\end{itemize}
\item[handler] is inline (nested) routine responsible for handling raised exception.
\begin{enumerate}
\item handler catches exception by matching with one or more exception types.
\item after catching, a handler executes like a normal subroutine
\item handler can return, reraise the current exception, or raise a new exception
\item re-raise terminate current handling and continuing propagation of caught exception.
\begin{itemize}
\item useful if a handler cannot deal with an exception but needs to propagate same exception to handler further down the stack.
\item provided by a raise statement without an exception type (\code{throw;}) where a raise must be in progress.
\end{itemize}
\item an exception is handled only if the handler returns rather than reraises
\end{enumerate}
\item[guarded block] is a language block with associated handlers, e.g., try-block in C++/Java.
\item[unguarded block] is a block with no handlers.
\item[termination] means control cannot return to the raise point. All blocks on the faulting stack from the raise block to the guarded block handling the exception are terminated, called stack unwinding.
\item[resumption] means control returns to the raise point: no stack unwinding.
\item[EHM] = Exception Type + Raise (exception) + Propagation + Handlers
\end{description}

\subsubsection{Static and Dynamic Returns}
All routines and exceptional control flows can be characterized by the following properties:
\begin{description}
\item[static/dynamic call] routine/exectpion name at the call/raise is looked up statically (compile-time) or dynamically (run-time)
\item[static/dynamic return] after routine handler completes, it returns to its static (definition) or dynamic (call) context.
\end{description}

\begin{table}[ht]
\centering
\begin{tabular}{r||l|l}
  return/handled & static  & dynamic \\
  \hline \hline
  static         & sequel  & termination exception \\ \hline
  dynamic        & routine & routine pointer, virtual routine, resumption \\
  \end{tabular}
\end{table}

\subsubsection{Static Propogation}
A {\bf sequel} is a routine with no return value where the sequel name is looked up lexically at the call site and control returns to the end of the block in which the sequel is declared. This is called {\bf static propogation}.

\begin{verbatim}
for(;;) {
    sequel f() { ... }
    // ...
    f();
    // not executed
}
// executed immediately after f();
\end{verbatim}

These are implemented in \code{try-catch}s. You can always determine statically what line a \code{catch} clause will execute when it returns: \code{catch}s are sequels. Note that sequels are not commonly used as-defined since they only work for monolithic programs as they must be statically nested at the point of use.

\subsubsection{Dynamic Propogation}
Termination and resumption both have dynamic raises with static and dynamic returns, respectively. This method of propogation works for seperately-compiled programs, but does not allow for statically knowing the handler.

For {\bf termination}, control transfers from the start of propogation to a handler (dynamically) then performs a static return (on handler return) which unwinds the stack. There exist three basic termination forms from a \textit{non-recoverable} operation: \code{non-local}, \code{terminate}, and \code{retry}. Non-local is general but has the goto problem, terminate is more limited, and \code{retry} is a combination of termination with special handler semantics (ie. restart the guarded block handling the exception [Eiffel], which pretends end-of-file is an exception of type EOF).

Since we can simulate \code{retry}, it is rarely supported directly.

In c++, we can toggle throwing exceptions (over returning status codes) in IO code with:
\begin{verbatim}
ifstream infile;
infile.exceptions(ios_base::failbit);

// ...
// do things
// ...

try {
  getline(infile, 42);
} catch (ios_base::failure) { }
\end{verbatim}

Note that $\mu$c++ does this by default.

A destructor can raise an exception, but this will cause errors if the exception is raised \textit{during propogation}:
\begin{verbatim}
struct E {};
struct C {
    ~C() { throw E(); }
}

try {
    C x;
    throw E();
} catch (E) {
    // ...
}
\end{verbatim}
this would not have caused issues without the \code{throw()} within the \code{try-catch}.

{\bf Resumption} provides a limited mechanism to generate new blocks on the call stack: control transfeers from the start of propogation to handler dynamically then returns dynamically on handler return -- note that this does not unwind the stack. A resumption handler is corrective action so a computation can continue: eg. the \code{new\_handler} fixup method in c++. If corrective action is impossible, the resumption handler should throw and exception instead of stepping into the enclosing block so the stack will unwind.

So long as a resumption handler remains on the stack, it may not be reused until it has caught and completed handling an exception. This precludes the handler from re-raising and re-catching the same exception. For this reason, propogation ignores unfinished resumption handlers when searching for an exception handler.

\subsubsection{Implementation}
To implement any dynamic propogation system, the \code{raise} must know the last guarded block with a handler for the raised exception type.

One approach is to do a lot of fuckery with labels, setting and resetting them upon entering or leaving a block. For termination, though, a direct transfer is often impossible anyway since activations on the stack may contain objects with destructors or finalizers. Resetting these labels are exepnsive, so the labels are often stored in the \code{catch}/destructor data within each block and the handler is found by linear search during a tack walk (with no direct transfer taking place). These are often implemented using that expensive approach on raise and zero cost on guarded-block entry, to improve overall efficiency.

% TODO: missing

\section{Coroutines}
% TODO: missing

\subsection{Semi-Coroutines}
% TODO: missing

\subsection{Full Coroutines}
{\bf Full Coroutines} have resume cycles. A full corotuine is allowed to perform semi-coroutine operations because it subsumes the notion of a semi-routine. We can think of standard routines as a linear path through our stack and semi-cortouines as a branching structure. A full coroutine, then, is a branch structure with occasional loopbacks.

\subsection{Raising}
We can either \code{throw} or \code{resume} when we want to raise an exception. Either acts as a rethrow or reresume (respectively) when no exception type is specified. Optionally, we can throw to other stacks, eg. with $\mu$cpp's \code{\_At} clause.

\subsubsection{Nonlocal Exceptions}
Local exceptions within a coroutine are the same as for exceptions within a routine or class, with one nonlocal difference: an unhandled exception raised by a coroutine raises a nonlocal exception of type \code{BaseCoroutine::UnhandledException} at the coroutine's last resumer and then terminates the coroutine. Note that nonlocal exception delivery is initially disabled for a coroutine -- it must be explicitely enabled at compile-time with the \code{\_Enable} block.

\section{Concurrency}
A {\bf thread} is an independant sequential execution path through a program. Each thread is scheduled and executed independantly from other threads.

A {\bf process} is a program componenbt such as a routine which has its own thread and the same state information as a coroutine.

A {\bf task} is similar to a process except that it is reduced along some dimension -- it is often the case that a process has its own memor, while tasks share common memory. We sometimes refer to tasks as {\bf Light-Weight Processes} (LWPs).

{\bf Parallel execution} is when two or more operations occur simultaneously, which can only occur when multiple processors are present.

{\bf Concurrent execution} is any situation in which execution of multiple threads \textit{appears} to be performed in parallel. It is the threads of control associated with processes and tasks that results in concurrent execution.

\subsection{Concurrent Hardware}
Concurrent thread execution possible with a uniprocessor using {\bf multitasking} and {\bf multiprocessing}. Parallelism is imulated through context switching. Unlike coroutines, task switching may occur at non-deterministic program locations, includeing between any two machine instructions. This introduces virtually all the difificulties in concurrent programs: concurrent programs must be written to work properly regardless of non-deterministic ordering of program execution.

\subsection{Concurrent Systems}
There are three major types of concurrent systems: systems with {\bf explicit} constructs, {\bf implicit} constructs, or those which attempt to discover concurrency in otherwise sequential programs. Note that the latter two types are built upon the first.

% MISSING

\subsection{Mutual Exclusion}
Is it possible to write code guaranteeing a statement is always serially executed by 2 threads? Yes, Dekker could do it!

\subsubsection{Dekker's Algorithm}
Dekker's algorithm is not read-write safe, though an RW-safe version exists. The algorithm also has unbounded overtaking (not starvation) because the race losed retracts intent (this is allowed - we do not prevent entry to the critical section by the delayed thread). A thread exiting the critical section does not exclude itself for reentry.

\subsubsection{Peterson}
The Peterson algorithm is much shorter; simply start a race condition then immediately block on it. This algorithm does not have unbounded overtaking and technically cheats according to the criteria for a mutually exclusive algorithm. It is also RW unsafe.

We now attempt to generalize these algorithms to $n > 2$ threads. We note that we should only need $n$ bits to solve the problem... that said, there is no known solution for all rules in only $n$ bits. There is a RW-unsafe algorithm in $3n$ bits and a RW-safe algorithm in $4n$ bits.

\subsubsection{Lamport}
Lamport's algorithm was the first successful solution to the general case (also: RW-safe) in $nm$ bits, where $m$ is the size of the key used. Hehner/Shyamasundar ``simplified'' it to a RW-unsafe case.

% TOURNAMENT (Tuabenfield/Buhr)

\subsection{Arbiter}
An arbiter is a full-time task which controls entry to critical sections. This task must be constantly executing and ends up converting mutual exclusion problems to synchronization problems.

\subsection{Hardware Solutions}
There also exist hardware solutions to this problem. These solutions cheat by making assumptions about execution (eg. y controlling order and speed of execution). This allows the elimination of much of the shared information and the checking required in the software solution. Generally, this is done by providing special atomic operations. This is sufficient for multitasking on a single CPU.

\end{document}
