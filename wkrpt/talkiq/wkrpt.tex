\documentclass[12pt]{article}
\usepackage{wkrpt}
\begin{document}


\title{An Analysis of the Development of Operational Systems for Software Engineering Workflows}
{
    TalkIQ\\
    1 Letterman Drive, San Francisco, California.
}
{
    Kevin Carruthers\\
    3B Software Engineering\\
    20463098\\
    kcarruth\\
    September 21st, 2015
}


\letter{An Analysis of the Development of Operational Systems for Software Engineering Workflows}{3B}{TalkIQ}{Technical Development}
{
    \noindent
    Kevin Carruthers\\
    271 Westcourt Pl, unit 111\\
    Waterloo, Ontario. N2L 2R8
}
{
    My position involved working on all aspects of TalkIQ's stack, as well as the operational aspects of those systems. I touched on Salesforce integration, integrated dialer design, telecommunications routing, data analysis, database design, speech transcription, and event logging.
}
{
    This report will focus on the development of a large-scale operational environment, specifically commenting on the various problems such a system attempts to solve and the subsystems required to do so. As an afterword, I will generalize this solution towards all software engineering workflows from the more specific solution provided herein.
}
{
    I wish to thank P. Kang for the idea of creating the \LaTeX\ template with which I wrote this report.
}
{
    \includegraphics[scale=0.5]{signature.jpg}\\
    \line(1,0){150}\\
    Kevin Carruthers, 20463098
}


\tocsection{Executive Summary}
TalkIQ, like many (if not all) software development companies~\cite{component}, has several different layers of products and services, both internal and external. Upon development of new code, the relevant systems must be deployed, tested, and verified to function as expected, along with re-deployments of any co-dependent services and management of other dependencies, such as database schema migrations.

There are many treatises on proper development standards and practices~\cite{practices} as well as the Development Operational (DevOps) systems related to them. Though arguments abound for which of these standards is ``best'' as well as which variation of a class of DevOps system is ``best'', these can mostly be split into two categories (assuming a somewhat agile development methodology is followed): completely automated DevOps or more manual operations.

Ultimately, this report shows that a completely automated system fits all criteria for a good quality DevOps system and argues that virtually all software development workflows should implement the system described herein.
\newpage


\toc
% \lof
\lot


\pagenumbering{arabic}
\section{Introduction}
TalkIQ, like many (if not all) software development companies, has several different layers of products and services, both internal and external. Upon development of new code, the relevant systems must be deployed, tested, and verified to function as expected, along with re-deployments of any co-dependent services and management of other dependencies, such as database schema migrations.

In the past, these deployments were managed by ``sysadmins'' and developers didn't care (nor were they meant to) about this process. More recently, the trend has been to slowly migrate towards automated deployment systems; reducing the workload on sysadmins and forcing the developers to learn about the process and think about the effects their changes will cause.

This manual, sysadmin-led process is inherently flawed for a multitude of reasons: any variations in procedure can cause difficult-to-solve issues, deployment strategies need to be considered on a per-deployment basis, release schedules must be slowed down to accommodate the slow process, staff needs to be trained solely to interact with the company's deployment infrastructure, and so on.

\subsection{Problem Requirements}
Any system which replaces this manual process must support all aspects of that process as well as solving the above issues. Our requirements, then, are as follows:
\begin{itemize}
\item create deployment plans by ensuring all systems are deployed or re-deployed as necessary, and
\item test the new deployment for pre-existing or new issues and roll-back as necessary, and
\item do the above in negligible time without staff intervention, and
\item ensure the process is carried out identically for each deployment
\end{itemize}

\subsection{Problem Statement}
This report will discuss possible solutions to these issues, the pros and cons of those solutions, and the determination of the correct set of solutions. It will then argue that these solutions should be applied to more generalized workflows in addition to the TalkIQ-specific workflow from which it was determined.

\subsection{Report Structure}
This report will discuss each aspect of a DevOps workflow in depth, discussing the manual approach as well as a couple approaches to an automated replacement of that approach. These alternative implementations will be subjected to MCDM (Multi-Criteria Decision-Making)~\cite{mcda} criterion evaluation and those implementations that come up as best under this system will be carried forward to further aspects of the system.

The completed system in all its aspects will then be described once each individual aspect has been determined.

This report is intended to be read by those interested in the development of DevOps workflows both at TalkIQ and elsewhere. A background in software development workflows and a general understanding of standard practices is beneficial, but not required.
\newpage


\section{System Development}
We will develop a DevOps system which can perform in some respect the following tasks:
\begin{enumerate}
\item code quality verification
\item release deployment planning
\item deployment testing and verification
\end{enumerate}

Though the final task described above (deployment testing and verification) is important to any DevOps workflow, this report will not apply MCDM evaluation in the interest of staying within the maximum page count for this report. Instead, the ``release deployment planning'' section will touch on this topic. Further aspects of this task which must be considered in designing a system to perform this task will be outlined, but an analysis of the correct implementation will be left as an exercise to the reader.

Initially, these systems are applied to the environment defined by TalkIQ's development practices and procedures, described below:
\begin{itemize}
\item TalkIQ maintains a large number of small projects spread across many repositories so as to maintain modularity.
\item Many of these projects act in a ``pipeline'' fashion, that is, each project relies on the output of the previous project in this pipeline.
\item Projects which are not members of this pipeline are supplementary projects: database schemas and migrations, recurring jobs, user-facing interfaces to various other projects, queue management software, caching software, and data analysis tools.
\item Many of the projects interface with a centralized database.
\item Some projects have their own caches, queues, etc, and some of these queues are shared between two or more modules.
\item Both the entry point and the final step of this pipeline interface with Salesforce~\cite{talkiq}.
\item Some of the projects are deployed to a single server, some are deployed to a small number of servers, and some are deployed to many servers. There is no requirement that each server have only one project deployed on it.
\item Some projects need to automatically scale as load is increased (eg. more users, peak hours, etc)
\item These projects are deployed to various different services such as Heroku or AWS, as well as services managed by other companies, such as OpenRedis.
\end{itemize}

\subsection{Code Quality Verification}
We will describe first the standard manual approach to this problem. Developers simply look over each other's work (hereafter referred to as a ``pull request'' for simplicity), scanning for potential bugs including but not limited to logic errors, styling issues, system stability problems, best practices, and potentially ``too limiting'' designs. Upon some set number of developers deciding the pull request is ``good enough'', that code will be merged into the codebase and further work will be built upon it.

Another possibility is a completely automated approach wherein a pull request is automatically submit to some gatekeeper service. This service should perform the above analysis: checking for logic errors, styling issues, etc in an automated way by running tests, performing static analysis, etc. This service should then approve or reject the pull request and perform the appropriate action: merging it into the codebase if that pull request was accepted or alerting the developer if it was not.

Finally, it is possible to describe a hybrid approach wherein the pull request is analyzed by this gatekeeper and approved or rejected based on the above requirements. Upon approval, though, the pull request should not be automatically merged. Instead, other developers are notified of a new pull request to look over and perform some of the manual approach. They need only perform a small portion of the manual approach since many of the aspects they originally had to search for (logic errors, styling issues, etc) have been verified by the gatekeeper system.

We define the criteria for determining which of these systems should be implemented as follows:
\begin{description}
\item[Code quality] should be maximized. This should be the most important criteria.
\item[Developer time] should be minimized as much as possible, though not at the expense of quality.
\item[Implementation time] should be minimized, but since this cost is only paid once this report will consider it minimally important.
\item[Maintenance cost] of the in-place system should be minimized, though again not at the expense of quality.
\item[Time until merge] should be minimized, but is not overly important.
\end{description}

For mathematical ease, each of these criteria will be represented on a scale from one to five such that a five is the best possible score and a one is the worst. We determine the weighting of these systems as follows and while keeping in mind the above descriptions of our criteria. Note that larger weightings should be applied to more important criteria:
\begin{itemize}
\item ``Implementation Time'' and ``Time Until Merge'' are not overly important and so are assigned weightings of {\bf 1}.
\item ``Developer Time'' and ``Maintenance Cost'' both deal with recurring costs which should be minimized but should not be the limiting factor in our decision. As such, these will be declared to be twice as important as the previously described criteria and assigned weightings of {\bf 2}.
\item Finally, ``Code Quality'' is our most important metric. Due to its importance, and since ``Developer Time'' and ``Maintenance Cost'' should be sacrificed somewhat for this metric, it is assigned a weighting equal to the sum of those two metrics, ie. a weighting of {\bf 4}.
\end{itemize}

\begin{table}[ht]
\caption{Criteria Weighting for Code Quality Verification}
\label{tbl:weighting-cqv}
\centering
\begin{tabular}{|l|r|r|}
    \hline
    Criterion & Relative Weighting & Percentage Weighting \\
    \hline
    \hline
    Code Quality        &  4 &  40\% \\
    Developer Time      &  2 &  20\% \\
    Implementation Time &  1 &  10\% \\
    Maintenance Cost    &  2 &  20\% \\
    Time Until Merge    &  1 &  10\% \\
    \hline
    \hline
    Total               & 10 & 100\% \\
    \hline
\end{tabular}
\end{table}

We apply these criteria in table~\ref{tbl:mcdm-cqv} to determine which approach is best. The scores for each approach were calculated as follows:
\begin{description}
\item[Code quality] is applied to the manual, automated, and hybrid systems by determining the subjective quality of the resulting code. The manual approach catches some set of errors which is assigned a score of {\bf 3}. The fully automated approach would likely catch many of the classes of errors the manual approach could catch, but not all (eg. future architectural instabilities), but will also likely catch a few more issues within those classes than humans could: so thus this report assigns this a score of {\bf 2}. Finally, the hybrid approach should catch the union of these sets of issues. Since they are not mutually exclusive sets, the hybrid system is given a score of {\bf 4}.
\item[Developer time] is determined as follows: the manual approach requires the most developer time and thus is given a score of {\bf 1}. The automated approach involves no developer time and so is given a score of {\bf 5}. Finally, the hybrid approach allows developers to put in less time while still requiring they put in some, and is thus given a score of {\bf 3}.
\item[Implementation Time] for the manual approach is virtually zero: commonly-used services such as Github have the capability for a manual approach built-in (thus this report assigns a score of {\bf 5}). The automated approach needs to be designed (mostly) from scratch and is thus assigned a score of {\bf 2}. Similarly, the hybrid approach must be designed from scratch, but does not involve quite as many aspects as the fully-automated approach and would thus require a bit less time; this report assigns it a score of {\bf 3}.
\item[Maintenance Cost] for the manual approach is virtually zero since there is nothing to maintain other than internal standard guides (score: {\bf 5}). The automated approach, as a semi-complex system, must be maintained over time, especially as new subsystems are added to the project workflow and as standards change, thus a value of {\bf 2} is assigned. Similarly, the hybrid approach must be maintained but should be slightly easier as it involves fewer ``moving parts''; this report thus assigns it a value of {\bf 3}.
\item[Time Until Merge] is determined as follows: the manual approach takes some amount of time which is assigned to a score of {\bf 3} as a baseline. The automated approach takes ``no'' time and is thus assigned a score of {\bf 5}. The hybrid approach, being somewhere between the two, is assigned a value of {\bf 4}.
\end{description}

\begin{table}[ht]
\caption{MCDM Results for Code Quality Verification}
\label{tbl:mcdm-cqv}
\centering
\begin{tabular}{|p{4.0cm}|p{0.65cm}|p{0.5cm}|p{0.5cm}|p{0.8cm}|p{0.8cm}|p{0.45cm}|p{0.45cm}|}
    \hline
    {\bf Criterion} & {\bf Wt.} & \multicolumn{2}{|p{1cm}|}{{\bf Manual}} & \multicolumn{2}{|p{1.6cm}|}{{\bf \mbox{Automated}}} & \multicolumn{2}{|p{0.9cm}|}{{\bf Hybrid}} \\
    \hline
    \hline
    Code Quality        &  4 & 3 & 12 & 2 &  8 & 4 & 16 \\
    Developer Time      &  2 & 1 &  2 & 5 & 10 & 3 &  6 \\
    Implementation Time &  1 & 5 &  5 & 2 &  2 & 3 &  3 \\
    Maintenance Cost    &  2 & 5 & 10 & 2 &  4 & 3 &  6 \\
    Time Until Merge    &  1 & 3 &  3 & 5 &  5 & 4 &  4 \\
    \hline
    \hline
    {\bf Total}         &    & \multicolumn{2}{|r|}{{\bf 32}} & \multicolumn{2}{|r|}{{\bf 29}} & \multicolumn{2}{|r|}{{\bf 35}} \\
    \hline
\end{tabular}
\end{table}

As indicated by having the largest ``score'', the hybrid approach is best. We thus begin our DevOps system by performing automated code verification with human gatekeepers to that code entering the codebase.

Each project at TalkIQ is connected to several web-based services:
\begin{itemize}
\item CircleCI, which automatically runs tests on each commit and shows the results of those tests on each pull request~\cite{circleci}.
\item Codacy, which automatically runs static analysis on each commit, finds potential code issues, and shows these results on each pull request~\cite{codacy}.
\item Coveralls, which automatically determines test coverage from after a CircleCI run, records this data, and shows the output on each pull request~\cite{coveralls}.
\item Requires.io, which watches the requirements of each project for changes, breakages, etc, and creates a pull request for fixing these issues as they occur~\cite{requires}.
\end{itemize}

Additionally, these services are tied together with an in-house testing and analysis suite specific to the platforms and technologies used by each project and a set of git hooks which prevent developers from acting without fulfilling standards based on each of the above services.

The workflow for Code Quality Verification at TalkIQ, then, is as follows:
\begin{enumerate}
\item A feature or bugfix is written by a developer. This developer should ensure that their changes do not break tests, disagree with standards, are covered by new tests, etc.
\item The developer submits this code as a pull request.
\item CircleCI runs tests on the pull request; if any of these tests fail or take much longer than usual, the developer is notified and returns to step 1.
\item Coveralls receives test coverage results from CircleCI; if the test coverage is lower across the codebase with this pull request than without it, the developer is notified and returns to step 1.
\item Codacy runs static analysis on the pull request; if any issues are created by the pull request, the developer is notified and returns to step 1. Additionally, the presence of any of a subset of legacy issues (which had been introduced prior to implementing this process) within any file modified by the developer causes a notification and a return to step 1.
\item Requires.io runs on any new requirements; if any of these are out-of date or have any relevant issues, the developer is notified. If the issue is deemed important in terms of severity and problem area, the developer returns to step 1. Note that Requires.io performs additional tasks independently of these pull requests, so only critical issues need to be fixed here as all other issues will be dealt with independently.
\item Finally, the pull request is marked as ``potentially good'' and other developers with experience in this aspect of the system are notified that they should look over the pull request and merge it if they deem it good. If not, they comment on the issues they discover and the pull request author is notified and returns to step 1.
\end{enumerate}

Clearly, this process implements the hybrid procedure described above and will result in a high-quality codebase.

\subsection{Release Deployment Planning}
In order to create and deploy a new release, it is important to consider all systems which must be updated, their dependencies, and their deployment processes. We define two possible methods of accomplishing this:

This entire process may be done manually. Clearly, this requires a staff member or team of members whom understand the entire codebase across all relevant projects as well as all dependencies (both internal and external), understand the changes which are to be released, and work through the requirements for deploying each affected project and the aforementioned dependencies. This process is long and arduous, must be entirely repeated for each new deployment, and is very prone to error.

Alternatively, it is possible to automate this procedure. This would require listing the requirements for deploying each system (eg. deploying system A requires system B be deployed first and database C be updated to the latest version) and, upon a new release being requested, having these requirements recursively generated into a list of tasks.

For either of these options, the deployment plan must then be run. For the manual approach, this would involve having staff members reading through the list of tasks and performing those actions. For the automated approach, this would involve having the machines for which those systems are deployed on execute the tasks sequentially.

We define the criteria for determining which of these systems should be implemented as follows:
\begin{description}
\item[Catastrophe recovery] includes chance of recovery, success of recovery, and effort involved in recovery. These three aspects have been combined since they will likely overlap in many key areas. This is among the most important aspects of the system, along with deployment success rate.
\item[Deployment success rate] should be maximized. This is among the most important aspects of the system.
\item[Deployment time] should be minimized, so as to promote deploying code as soon as it is ready instead of leaving features or bugfixes needlessly stuck in the ``ready'' queue.
\item[Implementation time] should be minimized, but since this cost is only paid once it can be considered minimally important.
\item[Maintenance cost] of the in-place system should be minimized, though again not at the expense of success rate.
\item[Staff time] should be minimized as much as possible, though not at the expense of success rate.
\end{description}

For mathematical ease, each of these criteria will be represented on a scale from one to five such that a five is the best possible score and a one is the worst. We determine the weighting of these systems as follows and while keeping in mind the above descriptions of our criteria. Note that larger weightings should be applied to more important criteria:
\begin{itemize}
\item ``Implementation Time'' is the least important feature and so is assigned a weighting of {\bf 1}.
\item ``Maintenance cost'' and ``Staff time'' both deal with expended effort which should be minimized but should not be the limiting factor in our decision, nor should system quality be sacrificed to reduce the amount of effort by any small margin. We assign these aspects a value of {\bf 2}.
\item ``Deployment time'' is slightly more important as it is generally agreed upon that completed development cycles should be delivered as soon as possible after completion. This aspect is thusly assigned a value of {\bf 3}.
\item ``Deployment success rate'' and ``Catastrophe recovery'' are by far the most important aspects and are thus given a weighting of {\bf 5}.
\end{itemize}

\begin{table}[ht]
\caption{Criteria Weighting for Release Deployment Planning}
\label{tbl:weighting-rdp}
\centering
\begin{tabular}{|l|r|r|}
    \hline
    Criterion & Relative Weighting & Percentage Weighting \\
    \hline
    \hline
    Catastrophe Recovery    &  5 &  27.78\% \\
    Deployment Success Rate &  5 &  27.78\% \\
    Deployment Time         &  3 &  16.67\% \\
    Implementation Time     &  1 &   5.55\% \\
    Maintenance Cost        &  2 &  11.11\% \\
    Staff Time              &  2 &  11.11\% \\
    \hline
    \hline
    Total                   & 18 & 100.00\% \\
    \hline
\end{tabular}
\end{table}

We apply these criteria in table~\ref{tbl:mcdm-rdp} to determine which approach is best. The scores for each approach were calculated as follows:
\begin{description}
\item[Catastrophe Recovery] is applied to the two potential systems by determining the subjective chance of recovery from the theoretically ``worst-case deployment''. We assume the manual approach -- which would involve noticing the issue, determining a rollback plan, implementing that plan, determining whether any other problems occurred and hotfixing those, then analyzing the cause of the issue to prevent future issues -- is given a baseline score of {\bf 3}. Then the automated approach -- which would automatically create the rollback plan before deployment, discover the problem instantly after deployment, implement the rollback virtually instantly upon said discovery, not need to worry about issues not covered by the rollback as it should be a theoretically perfect ``undo'' of the deploy, and then notify developers of the issue -- is clearly much better as virtually all steps are automated, theoretically ``perfect'', and performed instantly. We assign the automated system, then, a score of {\bf 5}.
\item[Deployment Success Rate] is determined as follows: the manual approach requires staff analyze each aspect of the system and determine the best approach for that system, then integrate each of these plans between all systems that must be deployed as well as all dependencies, etc. Though staff eventually become extremely proficient at this process, there is always the chance of human error; as such, the manual approach is given a value of {\bf 4}. The automated approach, on the other hand, performs the same tasks with no chance of human error. It is thusly assigned a value of {\bf 5}.
\item[Deployment Time] is determined simply: the manual approach requires an extreme amount of developer time plus whatever deployment time the plan requires, and is thus assigned a value of {\bf 2}. The automated system only requires only the latter (plus a negligible amount of machine-time to generate the instruction set) and is thus assigned a value of {\bf 4}.
\item[Implementation Time] for the automated approach is somewhat long as the system must be designed and developed per the project specifications, systems, and machines; this is assigned a value of {\bf 3}. Similarly, the manual approach requires training many staff members to complete this process and is also assigned a value of {\bf 3}.
\item[Maintenance Cost] for the manual approach is mostly skill maintenance of staff members: training conferences, continuous learning, etc. We assign this a value of {\bf 3}. Similarly, the automated approach must be updated for changes to the deployment process, which this report determines should take approximately the same amount of time -- this is also assigned a value of {\bf 3}.
\item[Staff Time] is determined as follows: the manual approach takes some amount of time which is assigned to a score of {\bf 3} as a baseline. The automated approach takes ``no'' time and is thus assigned a score of {\bf 5}.
\end{description}

\begin{table}[ht]
\caption{MCDM Results for Release Deployment Planning}
\label{tbl:mcdm-rdp}
\centering
\begin{tabular}{|p{5.0cm}|p{0.65cm}|p{0.5cm}|p{0.5cm}|p{0.8cm}|p{0.8cm}|}
    \hline
    {\bf Criterion} & {\bf Wt.} & \multicolumn{2}{|p{1cm}|}{{\bf Manual}} & \multicolumn{2}{|p{1.6cm}|}{{\bf \mbox{Automated}}} \\
    \hline
    \hline
    Catastrophe Recovery    &  5 & 3 & 15 & 5 & 25 \\
    Deployment Success Rate &  5 & 4 & 20 & 5 & 25 \\
    Deployment Time         &  3 & 2 &  6 & 4 & 12 \\
    Implementation Time     &  1 & 3 &  3 & 3 &  3 \\
    Maintenance Cost        &  2 & 3 &  6 & 3 &  6 \\
    Staff Time              &  2 & 3 &  6 & 5 & 10 \\
    \hline
    \hline
    {\bf Total}             &    & \multicolumn{2}{|r|}{{\bf 56}} & \multicolumn{2}{|r|}{{\bf 81}} \\
    \hline
\end{tabular}
\end{table}

As indicated by having the larger ``score'', the automated approach is best. We thus form the middle step of our DevOps system by performing automated code deployments upon success of the previous (code quality verification) step with automated rollbacks and developer notification upon deployment issues.

TalkIQ uses the configuration management system ``Ansible'' for much of the process described below. For ease of understanding, the instruction set for Ansible which defines tasks required to install, update, re-deploy, rollback, or uninstall any given project is referred to as a ``playbook''~\cite{playbooks}.

Each project at TalkIQ has a playbook which implements each of these four targets. These playbooks are managed by a central server which can run these playbooks on any machine used in deployment.

Upon being notified of a pull request being merged into our central codebase, this central server begins to execute its tasks:
\begin{enumerate}
\item Determine which pull request caused the deployment and if they are any linked pull requests (ie. a pull request in repository X which mentions the requirement that pull request Y in repository Z links those two pull requests).
\item Recursively analyze the ``update'' targets of each relevant playbook to determine the full list of playbook targets which must be executed.
\item If any of these projects are involved in the pipeline, determine (by analyzing the playbook of the next project in series) if any projects ``ahead'' in the pipeline must be re-deployed. If so, for each of these projects, add their ``re-deploy'' task to the list of required playbook tasks to-be-executed.
\item If any of these projects are linked to multiple other projects (eg. a database used by multiple projects), determine whether this change will break any related project. If so, check for any pending deployment of that project. If that deployment fixes the issue, merge the list of tasks generated from analyzing that project's deployment with the current list. If not, or if it has no pending deployments, terminate this deployment and notify the developers of which projects must be fixed to un-block this deployment.
\item For each of these playbook tasks, determine which tasks must be executed and in which order.
\item Match these tasks to the machines which the central server records as having the role associated with each playbook. This is not necessarily a one-to-one mapping.
\item For each of these servers, ensure that it is possible to communicate with it and run commands by examining the central server for login credentials and machine locations. For servers not managed by us, ensure that it is possible to instruct the service provider to execute those tasks.
\item For each of these servers, perform the tasks in order. In any cases where there are dependencies, complete the process on machines without the dependency first, then move on to the others. For any machines which are meant to deploy the same update (eg. two machines which both run the same project), deploy these at non-simultaneous times to ensure there always exists some server running the system (to prevent outages).
\item Upon completion of the deployment, read the logs across each machine (including machines not involved in the deployment) for error messages. Upon discovery of any error messages, perform the above steps once more with the ``rollback'' tasks of each playbook instead of the ``update'' tasks. The process is otherwise identical. Then, notify the developers with the error message which were generated by the deployment.
\end{enumerate}

Clearly, this process implements the automated procedure described above and will result in an always-stable and always-updated production environment resilient to failure.

\subsection{Deployment Testing and Verification}
As this section has been somewhat included in the above section ``Release Deployment Planning'' (see the criterion for a rollback outlined at the section end), this report will simply provide an overview of the further aspects which must be considered here and omit the analysis of competing alternatives.

This section of the DevOps system deals with any aspect of our system which is analyzed post-deployment. This may include the rollback aspects of the above section. Other aspects include the following:
\begin{itemize}
\item Perform speed tests, load tests, stability tests, etc on the live deployment (or a mirror of that deployment) of the entire system to determine areas which need improvement, have been recently made worse, etc.
\item Perform usability tests and other test which involve examining user behavior to examine interface and user experience changes which should be implemented.
\item Ensure the system is always operational by performing simple tasks in a recurring manner.
\item Ensure the above is visible to all staff at all times, as well as easy to consume and determine which changes should be made.
\end{itemize}

TalkIQ determined that the best approach would be an automated system developed mostly in-house.

Each project is given a set of requirements for which is must always fulfill (eg. be operational, perform its role, fit within standard limits for process execution time and machine cycles, etc). These are managed by ``Consul''~\cite{consul} across all deployment machines and are validated every few seconds. Consul runs on a central server which will be uses for all tasks in this section.

Upon each deployment as well as once per day, a set of speed, load, and stability tests are executed. Results of these tests, as well as trends, are stored in a central database on the Consul server. Similarly, usability tests are executed at the same frequency and sent to the same database. User interactions are collected as users interact with our software and are periodically sent to this database.

All results of the above systems are collected and presented on an internal dashboard which is displayed at all times on the wall in our office. As such, developers can immediately see issues as they occur and determine what, if anything, they should do to improve these results.
\newpage


\section{Conclusion}
The results of these evaluations conclude that automated approaches to the three analyzed systems are clearly the best options. As such, this solution should be (and was) implemented at TalkIQ.

Ultimately, quality of various aspects of the selected system was the largest factor in determining the correct solution. This is a good approach since it is important to place quality over developer time in non-extreme cases.

It is also worth noting that in some cases, eg. the MCDM analysis of Release Deployment Planning (see table~\ref{tbl:mcdm-rdp}), the automated approach was at least as good if not better than the competing option. This is interesting to note, though not a particularly useful result on its own.
\newpage


\section{Generalization}
This report recommends that TalkIQ implements the above defined DevOps workflow to enhance its software engineering practices. Furthermore, it is easy to see that the above approaches easily generalizes to any company with a similar set of requirements (see the requirements outlined in the ``System Development'' section).

This report would argue that this DevOps workflow can and should be applied to a wider range of companies, that is, any with any sort of software development workflows. This argument can be made due to the overwhelmingly ``better'' effects such a system causes for any company which implements it:
\begin{itemize}
\item Always-up-to-date systems
\item Always-function systems
\item Instant notification of errors
\item Reduced developer training and lack of deployment staff
\item Reduced possibility and impact of human error
\item Verification that all projects fulfill defined requirements
\end{itemize}

Clearly, the introduction of these qualities to any company's software development workflow would be beneficial. As such, they should implement the above defined system, modified slightly for their slightly differing needs.
\newpage


\addcontentsline{toc}{section}{\refname}
\bibliography{wkrpt}
\newpage

\end{document}
