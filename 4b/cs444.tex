\documentclass[12pt]{article}
\usepackage{amsmath,amssymb,bookmark,parskip,custom}
\usepackage[margin=.8in]{geometry}
\allowdisplaybreaks
\hypersetup{colorlinks,
    citecolor=black,
    filecolor=black,
    linkcolor=black,
    urlcolor=black
}
\setcounter{secnumdepth}{5}

\begin{document}

\title{CS 444 --- Compiler Construction}
\author{Kevin James}
\date{\vspace{-2ex}Winter 2017}
\maketitle\HRule

\tableofcontents
\newpage

\section{Summary}

\section{Analysis}
The {\bf Analysis} step within a compiler requires we check to ensure validity of the program and then determine its meaning. When possible, we prefer using formal languages as the specificiation for validity checking.

\begin{quote}
A {\bf formal language} ($L$) has some alphabet ($\Sigma$) with a finite set of symbols, arranged into some words ($w$) which are finite sequences of those symbols.
\end{quote}

In the first step of the Analysis phase, we split the sequence of input characters into tokens or lexemes. This is refered to as {\bf Scanning} or {\bf Lexical Analysis}. There are several possible approaches to this:
\begin{itemize}
    \item simply coding each case
    \item maximal munch alogrithm, using a DFA
    \begin{itemize}
        \item simply coding the entire DFA
        \item Regex $\to$ \code{lex} $\to$ DFA
    \end{itemize}
\end{itemize}

When building a DFA, the $\epsilon$-closure($S$) of a set $S \in Q$ of states is
\begin{itemize}
    \item the set of states reachable from $S$ by $\epsilon$-transitions
    \item the smallest set $S^\prime$ such that $S \subseteq S^\prime$ and $\{q \suchthat q^\prime \in S^\prime, q \in \delta(q^\prime, \epsilon)\} \subseteq S^\prime$
\end{itemize}

The following psuedo-code defines a method for computing these closures.

\begin{verbatim}
def compute_closure(S):
    worklist = S
    Sprime = S
    while worklist:
        qprime = worklist.pop()
        for q in delta(qprime, epsilon):
            if q not in Sprime:
                Sprime.append(q)
                worklist.append(q)
    return Sprime
\end{verbatim}

We may also consider creating a DFA by converting from an NFA ($\{\Sigma, Q, \dots\} \to \{\Sigma^\prime, Q^\prime, \dots\}$).

A state of $q^\prime \in Q^\prime$ corresponds to a set of states from $Q$, ie. $Q^\prime = z^Q$. More specifically, we have
\begin{align*}
q_0^\prime &= \epsilon\text{-closure}(\{q_0\}) \\
\delta^\prime (q^\prime, a) &= \epsilon\text{-closure}(\displaystyle\bigcup_{q \in q^\prime} \delta(q, a))
\end{align*}
and $Q^\prime$ is the smallest set of subsets of $Q$ such that $\{q_0^\prime\} \subseteq Q^\prime$ (where $q_0^\prime \in Q^\prime$) and $\forall a \in \Sigma \cdot Q^\prime, \{\delta^\prime(q^\prime, q) \suchthat q^\prime \in Q^\prime\} \subseteq Q^\prime$. We also have \[ A^\prime = \bigl\{q^\prime \in Q^\prime \suchthat q^\prime \cap A \neq \{\}\bigl\} \]

\begin{quote}
We can build a recognizer for a DFA with
\begin{verbatim}
def is_word_in_language(q0):
    q = q0
    for i in range(1, n):
        q = delta(q, w[i])
    return q in A
\end{verbatim}
\end{quote}

\subsection{Scanning}
Given some input DFA specifying valid tokens and $w \in \Sigma^*$, output a sequence of tokens whose concatenation is $w$ and each token $t \in L$.

In psuedocode:
\begin{verbatim}
while unprocessed input {
    find a prefix of unprocessed input that is in L
}
\end{verbatim}

When using the {\bf Maximal Munch Algorithm} (which is used for virtually all compiler scanners), we instead find the \textit{longest} prefix.
\begin{verbatim}
while unprocessed input {
    run DFA on unprocessed input until it gets stuck
    backtrack DFA and input to last-seen accepting state or fail
    output current prefix as token
    reset DFA to start state
}
\end{verbatim}

\subsection{Parsing}
After the Scanner turns a sequence of characters into a sequence of tokens, those tokens are sent into a Parser which outputs a {\bf parse tree}.

Context-free grammars have concatention, alternation (I) like regular expressions but replace repetition ($*$) with recursion.

\begin{quote}
A Context-Free Grammar is a 4-tuple $G = (N, T, R, S)$ -- Non-terminals and Terminals (symbols $V = T \cup N$), Rules ($R \subseteq N \times V^*$), and Start ($S \in N$).

We use the symbols $\Rightarrow$ and $\Rightarrow^*$ to imply ``direct derivation'' and ``derivation''.

$\beta A \gamma \Rightarrow \beta\alpha\gamma$ if $A \to \alpha \in R$. Therefore, $\alpha \Rightarrow^* \beta$ if $\alpha = \beta$ or $\alpha \Rightarrow \gamma$ and $\gamma \Rightarrow^* \beta$. $\alpha$ is a sentential form if $S \Rightarrow \alpha$ (where $\alpha$ is the start symbol).

The language of the grammar $L(G) = \{ x\ in T^* \suchthat S \Rightarrow^* x \}$ which is the sentential forms $S(G) \cap T^*$.
\end{quote}

Therefore, if some $x$ is in the grammar, there must exist some derivation for $x$. Parsing, then, involves finding this derivation. A context-free grammar is unambiguous if there exists only a single parse tree, left derivation, and right derivation for any given word.

We can parse either forwards (top-down, eg. LL1) or backwards (bottom-up, eg. LR1), ie. beginning or ending with the starting token. In this course, we will focus on LR1 parsing.

\begin{verbatim}
def top_down_parsing():
    alpha = S
    while alpha != x
        replace some A in alpha with B (where A->B is a rule)

def bottom_up_parsing():
    alpha = x
    while alpha != S:
        replace some B in alpha with A (where A->B is a rule)
\end{verbatim}

In either of the above algorithms, the token being replaced is called the {\bf handle}.

The variation in the above algorithms that defines LL1, LR1, RR1, etc occurs in the loop body. For and LL1 parser, for example:
\begin{verbatim}
def ll1_parse():
    alpha = S
    while a != x:
        A = first non-term in alpha (alpha = y A gamma)
        a = first terminal in x after y (x = y a z)
        let A->B = predict(A, a)
        replace A in alpha with B

def predict(A, a):
    # TODO
\end{verbatim}

\end{document}
