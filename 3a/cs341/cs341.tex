\documentclass[12pt]{article}
\usepackage{amsmath,amssymb,bookmark,mathtools,parskip,custom}
\usepackage[margin=.8in]{geometry}
\allowdisplaybreaks
\hypersetup{colorlinks,
    citecolor=black,
    filecolor=black,
    linkcolor=black,
    urlcolor=black
}
\setcounter{secnumdepth}{5}

\begin{document}

\title{CS 341 --- Algorithms}
\author{Kevin James}
\date{\vspace{-2ex}Winter 2015}
\maketitle\HRule

\tableofcontents
\newpage

\section{Solving Recurrences}
\begin{example}
Determine the runtime of mergesort.
\[ T(n) =
\begin{dcases*}
2T\bigl(\frac{n}{2}\bigl) + \Theta(n) & if $n > 1$ \\
\Theta(1) & if $n = 1$
\end{dcases*}\]
which we can solve to find \[ T(n) \in \Theta(n\log n) \]
\end{example}

\begin{definition}
Stoogesort:
\begin{verbatim}
def stoogesort(A[1..n]):
  if n <= 1:
    return
  if A[1] > A[2]:
    swap(A[1], A[2])
  stoogesort(A[1..\frac{2n}{3}])
  stoogesort(A[\frac{n}{3}+1..n])
  stoogesort(A[1..\frac{2n}{3}])
\end{verbatim}
\end{definition}

\begin{example}
Let $T(n)$ be the runtime of stoogesort on $n$ elements. Then \[ T(n) =
\begin{dcases*}
3T\bigl(\frac{2n}{3}\bigl) + \Theta(1) & if $n > 1$ \\
\Theta(1) & if $n = 1$
\end{dcases*}\]
\end{example}

There are three methods for solving this recurrence:
\begin{enumerate}
\item Recursion Tree method: Expand for $k$ iterations to get a tree of terms, set $k$ to reach the base case, sum across rows, then over all levels.
\item Master method: Look up the answer. The Master Theorem: Let $T(n) = aT(\frac{n}{b}) + f(n)$ if $n > n+0$ and $c$ if $n = n_0$. Set $d = \log_b a$i and pick a small constant $\varepsilon > 0$. Case 1: $f(n) = O(n^{d-\varepsilon}) \implies T(n) = \Theta(n^d)$. Case 2: $f(n) = \Theta(n^d) \implies T(n) = \Theta(n^d \log n)$. Case 3: $\lim_{n\to\infty} f(n) / n^{d+\varepsilon} = \infty \implies T(n) = \Theta(f(n))$.
\item Substitution method: Guess the form of the solution $T(n) \leq x)$, then verify your guess by induction and fill in any constants.
\end{enumerate}

\begin{example}
\begin{align*}
T(n) &= 2T\bigg(\frac{n}{2}\bigg) + n^2, 7 \\
T(n) &\leq cn^2 \\
n = 1: T(n) &= 7 \leq cn^2, c \geq 7 \\
T\bigg(\frac{n}{2}\bigg) &\leq c\bigg(\frac{n}{2}\bigg)^2 \\
T(n) &= 2T\bigg(\frac{n}{2}\bigg) + n^2 \\
     &\leq 2c\bigg(\frac{n}{2}\bigg) + n^2 \\
     &= \frac{2cn^2}{4} + n^2 \\
     &= \bigg(\frac{c}{2} + 1\bigg)n^2 \\
     &\leq cn^2, \frac{c}{2} + 1 \leq c, c \geq 2
\end{align*}
We can then pick $c = 7$ and solve as \[ T(n) \leq 7n^2 \implies T(n) \in O(n^2) \] We can also note that $T(n) \geq n^2$, so $T(n) \in \Theta(n^2)$.
\end{example}

\begin{example}
\begin{align*}
T(n) &= 3T\bigg(\bigg\lfloor \frac{n}{2} \bigg\rfloor\bigg) + 4T\bigg(\bigg\lfloor \frac{n}{4} \bigg\rfloor\bigg) + 1, 1 \\
T(n) &\leq cn^2 \\
n = 1: T(n) &= 1 \leq cn^2, c \geq 1 \\
T\bigg(\bigg\lfloor \frac{n}{2} \bigg\rfloor\bigg) &\leq c\bigg\lfloor \frac{n}{2} \bigg\rfloor^2 \\
T(n) &= 3T\bigg(\bigg\lfloor \frac{n}{2} \bigg\rfloor\bigg) + 4T\bigg(\bigg\lfloor \frac{n}{4} \bigg\rfloor\bigg) + 1 \\
     &\leq 3c \bigg\lfloor \frac{n}{2} \bigg\rfloor^2 + 4c\bigg\lfloor \frac{n}{4} \bigg\rfloor + 1 \\
     &\leq 3c \bigg(\frac{n}{2}\bigg)^2 + 4c \bigg(\frac{n}{2}\bigg)^2 + 1 \\
     &= \bigg(\frac{3}{4} + \frac{4}{16}\bigg)cn^2 + 1 \\
     &= cn^2 + 1
\end{align*}
but since we could not get rid of the constant, we try
\begin{align*}
T(n) &\leq cn^2 - c_0 \\
n = 1: T(n) &= 1 \leq cn^2 - c_0, c \geq 1 + c_0 \\
T\bigg(\bigg\lfloor \frac{n}{2} \bigg\rfloor\bigg) &\leq c\bigg\lfloor \frac{n}{2} \bigg\rfloor^2 - c_0 \\
T(n) &= 3T\bigg(\bigg\lfloor \frac{n}{2} \bigg\rfloor\bigg) + 4T\bigg(\bigg\lfloor \frac{n}{4} \bigg\rfloor\bigg) + 1 \\
     &\leq 3\bigg(c \bigg\lfloor \frac{n}{2} \bigg\rfloor^2 - c_0\bigg) + 4\bigg(c\bigg\lfloor \frac{n}{4} \bigg\rfloor - c_0\bigg) + 1 \\
     &\leq 3c \bigg(\frac{n}{2}\bigg)^2 - 3c_0 + 4c \bigg(\frac{n}{2}\bigg)^2 - 4c_0 + 1 \\
     &= \bigg(\frac{3}{4} + \frac{4}{16}\bigg)cn^2 - 7c_0 + 1 \\
     &= cn^2 - c_0, -7c_0 + 1 \leq -c_0, c_0 \geq \frac{1}{6}
\end{align*}
Pick $c_0 = \frac{1}{6}, c = \frac{7}{6}$ and we see that \[ T(n) \leq \frac{7}{6}n^2 - \frac{1}{6} \implies T(n) \in O(n^2) \]
\end{example}

\section{Algorithm Design Techniques}
\subsection{Divide and Conquer}
Divide your problem into subproblems of the same type, then use recursion to solve each problem and combine the results.

\begin{example}
{\bf (Maxima)}: Given a set $P$ of $n$ points in 2D, we say point $p$ dominates point $q$ if and only if $p$ has both a greater $x$ and $y$ value than $q$. We say point $q$ is maximal if and only if $q \in P$ and no point in $P$ dominates $q$. Find all maximal points.

{\bf Solutions}:
\begin{itemize}
\item Brute Force: for each $q \in P$, check if no points dominat $q$. Total time: $\Theta(n^2)$
\item Divide and Conquer: divide into two subarrays of size $\frac{n}{2}$.
\item Divide by Medians: instead of dividing by size, divide by a median vertical line.
\end{itemize}

\begin{verbatim}
maxima(sorted[p_1..p_n]):
  1. if n == 1: return p_1
  2. [q_1..q_l] = maxima([p_1..p_{n/2}])
  3. [s_1..s_m] = maxima([p_{n/2}..p_n])
  4. i = 1
  5. while q_i.y > s_1.y
  6.   i += 1
  7. return [q_1..q_l, s_1..s_m]
\end{verbatim}
which is an $O(n\log n)$ algorithm.
\end{example}

\begin{example}
{\bf (Closest Pair)}: Given set $P$ of $n$ points in 2D, find a pair $p, q \in P$ such that these points have a smaller distance between them than any other pair of points in $P$, ie. $d(p,q) = \sqrt{(p.x - q.x)^2 + (p.y - q.y)^2}$.

{\bf Solutions}:
\begin{itemize}
\item Brute Force Algorithm: $\Theta(n)$
\item Shamos' Algortithm: $\Theta(n\log^2 n)$. Note that if we refine the Shamos algorithm by pre-sorting $P$ also by y-coordinate at the beginning, we find that our complexity becomes $O(n\log n)$.
\end{itemize}

\begin{verbatim}
def bruteForce(P):
  distance = infinity
  for each p in P:
    for each q in P (q neq P):
      distance = min(distance, d(p,q))
  return distance

def shamos(P):
  if n <= 10:
    return bruteForce(P)
  x_m = median x_coordinate
  P_L = { p in P : p.x < x_m }
  P_R = { p in P : p.x > x_m }
  d_L = shamos(P_L)
  d_R = shamos(P_R)
  d = min(d_L, d_R)
  <p_1..p_m> = sorted_by(points in { p in P : x_m - d <= p.x <= x_m + d }, y)
  for i = 1 to m do
    j = i + 1
    while p_j.y <= p_i.y + d:
      d = min(d, d(p_i, p_j))
      j++
  return d
\end{verbatim}
\end{example}

\subsubsection{Multiplying Large Numbers}
\begin{example}
Given two $n$-bit numbers $A = a_{n-1}, a_{n-2}, \dots a_0$, $b = b_{n-1}, b_{n-2}, \dots b_0$ in binary, compute $AB = c_{n-1}, c_{n-2}, \dots c_0$.

{\bf Solution}: The ``Elementary School'' algorithm for this involves doing
\begin{verbatim}
    1011
  x 1101
--------
    1011
  1011
 1011
--------
10001111
\end{verbatim}
which is $O(n)$ shifts and $O(n)$ additions, thus making it an $O(n^2)$ algorithm.

The ``Karatsuba and Ofman'' algorithm involves a different process:
\begin{verbatim}
A' = [a_{n-1}..a_{n/2}]
A" = [a_{n/2 - 1}..a_0]
A = [A'..A"]

B' = [b_{n-1}..b_{n/2}]
B" = [b_{n/2 - 1}..b_0]
B = [B'..B"]

AB = A'B'2^n + (A'B" + A"B')2^(n/2) + A"B"
\end{verbatim}
which gives us a complexity of $O(n^2)$.
\end{example}

% MISSING

\section{Selection}
\begin{example}
Given $n$ numbers $a_1..a_n$ and $k$, find the $k^{th}$ smallest number.

{\bf Solutions}:
\begin{itemize}
\item Method 0: sort and look up index $k$. $O(n\log n)$ time.
\item Method 1 (selection sort variation): find minimum, remove, and repeat $k$ times. $O(nk)$ time.
\item Method 2 (heapsort variation): find minimum, remove, build heap, and repeat $k$ times. $O(n + k\log n$) time.
\item Method 3 (quicksort variation [``quickselect''])
\end{itemize}

\begin{verbatim}
def quickselect([a_1..a_n], k):
  if n = 1:
    return a_1
  pick pivot x
  L = { a_i : a_i <= x }, l = sizeof(L)
  R = { a_i : a_i > x }
  if k <= l:
    return quickselect(L, k)
  else:
    return quickselect(R, k-l)
\end{verbatim}

Analysis of quickselect: let $T(n)$ be the runtime on $n$ elements. Then \[ T(n) \leq T\bigl(\max\{l, n-l\}\bigl) + O(n) \] In the ideal case, $l = \frac{n}{2}$. In this case, we have (using the master theorem) $T(n) \in O(n)$. In the worst case, $l = 1$ or $l = n - 1$, which gives us $T(n) \in O(n^2)$. The challenge of quickselect, then, is determining a pivot selection strategy which brings us closer to the ideal case.
\end{example}

\subsubsection{Pivot Selection}
Selecting a pivot at random gives us an expected case near $O(n)$.

Randomized Analysis: $l$ is equally likely to be $1, 2,\dots n$. Thus $\text{prob}\bigl[\frac{n}{4} < l \leq \frac{3n}{4}\bigl] = \half$. Expected number of iterations to get this case is $2$. So then \[ T(n) \leq T\bigg(\frac{3n}{4}\bigg) + 2 * O(n) \] which is an (expected) $O(n)$ runtime.

Unfortunately, this algorithm gives us only an expected runtime. We do not have a worst case upper bound specifying $O(n)$.

The Blum, Floyd, Prattt, Rivest, and Tarjan algorithm tries to deterministically find a good pivot. They find an approximate median by recursion: this is the ``median of medians of five'' algorithm.
\begin{verbatim}
def quickselect([a_1..a_n], k):
  if n = 1:
    return a_1

  // pivot selection in n time
  split a_1..a_n into groups G_1..G_{n/5} of five elements each
  for i = 1 to n/5:
    x_i = median of G_i
  x = quickselect([x_i..x_{n/5}], n/10)
  // end pivot selection

  L = { a_i : a_i <= x }, l = sizeof(L)
  R = { a_i : a_i > x }
  if k <= l:
    return quickselect(L, k)
  else:
    return quickselect(R, k-l)
\end{verbatim}

\begin{lemma}
\[ \frac{3n}{10} \lesssim l \lesssim \frac{7n}{10} \]
\end{lemma}

\begin{proof}
How many groups $G_i$ with $x_i \leq x$? $\frac{n}{10}$. For each such group, how many elements are less than or equal to $x_i$? $3$. Thus we have the number of elements less than or equal to $x$ is greater than or equal to $\frac{3n}{10}$. Similarly, the number of elements greater than $x$ is greater than or equivalent to $\frac{3n}{10}$.
\end{proof}

By this lemma, we see that the final recursion lines in this algorithm are in $\leq T(\frac{7n}{10})$. We have the overall complexity of this algorithm as \[ T(n) \leq T(\frac{n}{5}) + T(\frac{7n}{10}) + O(n) \] which solves to $T(n) \in O(n)$.

\subsection{Greedy Algorithms}
{\bf Greedy Algorithms} are usually used for optimization problems, where we find the solution by minimizing or maximizing some objective subject to some restraints. These algorithms incrememntally build the solution by choosing what seems best at each step.
\begin{itemize}
\item Advantages: simple, fast
\item Disadvantages: local maxima over global maxima
\end{itemize}

% EXAMPLES MISSING

\subsubsection{Interval Coloring}
\begin{example}
Given $n$ intervals $[a_1, b_1], \dots, [a_n, b_n]$, assign each interval a color such that no two intervals of the same color intersect while minimizing the number of colors used.

Greedy Algorithm Idea: scan intervals from left to right and use a new color only when needed.

{\bf Algorithm}:
\begin{verbatim}
k = 0
for each interval [a_1, b_1] in increasing order of a:
    pick any color c in {1, ..., k} not within any intervals intersecting [a_i, b_1]
    if c exists:
        assign [a_i, b_i] color c
    else:
        assign [a_i, b_i] color k + 1
        k++
\end{verbatim}
\end{example}

\subsection{Pairing Algorithms}
% PAIRING ALGORITHMS

\subsubsection{Stable Marriage}
% ALGORITHM

The {\bf stable marriage} algorithm is stable if for all matched pairs $(C_1, E_1), (C_2, E_2)$ we don't have both $C_1$ prefers $E_2$ over $E_1$ and $C_2$ prefers $C_1$ over $C_2$.

\subsubsection{Gale and Shapley's Algorithm}
\begin{verbatim}
while exist unmatched employer E
    pick C = next best candidate in E's preference list
    // E makes offer to C
    if C is unmatched or C prefers E over C's current employer E_0
        unmatch(C, E_0)
        match(C, E)
\end{verbatim}
We can see that this algorithm is bounded by $O(n^2)$.

\section{Dynamic Programming}
{\bf Dynamic Programming} can be used for optimization problems. We simply define a class of subproblems, derive a recursive formula to explore all choices at each step, and then evaluate that formula bottom-up using a table. This is successful when the number of total subproblems is not overly large.

% COIN CHANGING

% LOTS OF STUFF

\section{Graph Problems}
% INTRO

\subsection{Shortest Paths}
Given a directed graph with weights along each path, find the cheapest route between two points.

Special Cases:
\begin{itemize}
\item Unweighted (or, all are equal to 1): simply find the path which uses the fewest amount of edges (with BFS). Analysis: $O(n + m)$.
\item DAG (directional acyclic graph): use dynamic programming. Subproblem: $\forall v \in V, \delta[v] =$ the weight of the shortest path from $s$ to $v$. Our answer, then, is $\delta[t]$. Base Case: $\delta[s] = 0$. Recursive Formula: $\delta[v] = \min\bigl(\delta[u] + w(u, v), \{u : (u,v) \in E\}\bigl)$. Note the evaluation order for the recursive formula is in the same order as a topological sort over the edges. Analysis: $O\bigl(n + \displaystyle\sum_{v\in V} \text{in-deg}(v)\bigl) = O(n + m)$.
\end{itemize}

General Case: we could consider taking the smallest weighted edge at every step, but this would not necessarily give us the correct answer. Instead, we use {\bf Dijkstra's Algorithm}:
\begin{verbatim}
// assume positive weights
// solve single-source (all destination) problem
// compute d[v]: shortest path weight from s to v for all v in V.
S= {s}, d[s] = 0
while S != V:
    pick edge (u, v) with u in S, v in V - S minimizing d[u] + w(u, v)
    insert v into S
    d[v] = d[u] + w(u, v)
\end{verbatim}

The detailed implementation is:
\begin{verbatim}
// TODO: Prim's, with changes to lines 7-8
if y in emptyset and key[v] + w(u, v) < key[y]:
  key[y] = key[v] + w(u, v), pi[y] = v
\end{verbatim}

Analysis: no data structure: $O(n^2)$, heap: $O\bigl((n+m)\log n\bigl)$, Fibonacci heap: $O(m + n\log n)$.

\subsubsection{Correctness Proof}
Claim: if $(u, v)$ is the edge from $S$ to $V \setminus S$ minimizing $\delta[u] + w(u, v)$, then $\delta[v] = \delta[u] + w(u, v)$.

Proof: There is a path from $s$ to $v$ with weight $\delta[u] + w(u, v)$. Thus $\delta[v] \leq \delta[u] + w(u, v)$. Now, take any arbitrary path $p$ from $s$ to $v$. $p$ must contain an edge $(u^\prime, v^\prime)$ with $u^\prime \in S$ and $v^\prime \in V \setminus S$. $w(p) \geq \delta[u^\prime] + w(u^\prime, v^\prime) + 0 \implies \delta[v] \geq \delta[u] + w(u, v)$. Thus, we have shown $\delta[v] = \delta[u] + w(u, v)$.

Note that this is only correct if our graph contains no negative weights.

\subsubsection{All-Pairs Shortest Paths}
Given a weighted directed or undirected graph $G = (V, E)$, find the shortest path between all pairs of vertices.

\begin{itemize}
\item Method 0: Run Dijkstra's algorithm on each vertex, ie. run it $n$ times. This method assumes there are no negative weights. Complexity: $O\bigl(n(m + n\log n)\bigl)$.
\item Method 1:
\end{itemize}

\end{document}
