\documentclass[12pt]{article}
\usepackage{amsmath,amssymb,bookmark,parskip,custom}
\usepackage[margin=.8in]{geometry}
\allowdisplaybreaks
\hypersetup{colorlinks,
    citecolor=black,
    filecolor=black,
    linkcolor=black,
    urlcolor=black
}
\setcounter{secnumdepth}{5}

\begin{document}

\title{CS 343 --- Concurrent and Parallel Programming}
\author{Kevin James}
\date{\vspace{-2ex}Fall 2015}
\maketitle\HRule

\tableofcontents
\newpage

\section{Exceptions}
Any contiguous code block can be factored into a helper routing and called from anywhere in the program. This process is called {\bf modularization}. Modularization can fail when the factored block exits (eg. with multi-level labelled exits) because labels are only scoped at function-level. To get around this, we can use global labels, ie:

\begin{verbatim}
label L;
void f() {
    // ...
    goto L;
}

void main() {
    L = L1;
    // ...
    f();
    // not exectuted
  L1:
    L = L2;
    // ...
    f();
    // not executed
  L2:
    return;
}
\end{verbatim}

Fundamentally, we can have routines with two types of returns:
\begin{description}
\item[normal returns] which skip to the statement after the call, and
\item[exceptional returns] which skip to statements \textit{not} after the call
\end{description}

Exceptional returns, though, can lead to the \code{goto} problem, ie. that you can jump anywhere and create spaghetti code.

\subsection{Traditional Approaches}
There are several more traditional approaches to this problem:
\begin{description}
\item[return codes] have return values which indicate normal or exceptional execution. These codes must be checked every time the function is called and mix normal return values with exceptional ones.
\item[status flags] set global variables indicating normal or exceptional execution. These values will be over-written by subsequent function calls and must be checked before this happens.
\item[fix-up routines] are passed into other routines; if a problem is detected, the fix-up routine returns a different result in place of the original function.
\end{description}

Note that we often combine these techniques.

\subsection{Exception Handling}
Compelx control-flow among routines is often called {\bf exception handling}. An exceptional event is one that is (usually) known to exists, but which is ancillary to the algorithm (eg. occurs with low frequency). An {\bf exception handling mechanism} (EHM) provides some alternate type of control flow.

The {\bf execution environment} has a significant effect on an EHM: eg. object-oriented environments require much more complex EHMs. Example: objects have destructores which must be executed no matter how the object ends, even if an exception is thrown.

Control structures with \code{finally} clauses must always be executed. This complicates the EHM, since now both destructors and \code{finally}s must be called. Given multiple execution stacks, an EHM must be even more sophisticated (eg. propogate the exception to another stack if no handler is found in the current one).

\subsubsection{Terms}
\begin{description}
\item[execution] is the language unit in which an exception can be raised, usually any entity with its own runtime stack.
\item[exception type] is a type name representing an exceptional event.
\item[exception] is an instance of an exception type, generated by executing an operation indicating an ancillary (exceptional) situation in execution.
\item[raise (throw)] is the special operation that creates an exception.
\item[source execution] is the execution raising an exception.
\item[faulting execution] is the execution changing control flow due to a raised exception.
\item[local exception] is when an exception is raised and handled by the same execution: source = faulting.
\item[non-local exception] is when an exception is raised by a source execution but delivered to a different faulting execution: source $\neq$ faulting.
\item[concurrent exception] is a non-local exception, where the source and faulting executions are executing concurrently.
\item[propagation] directs control from a raise in the source execution to a handler in the faulting execution.
\item[propagation mechanism] is the rules used to locate a handler. The most common propagation-mechanisms give precedence to handlers higher in the lexical/call stack.
\begin{itemize}
\item specificity versus generality
\item efficient linear search during propagation
\end{itemize}
\item[handler] is inline (nested) routine responsible for handling raised exception.
\begin{enumerate}
\item handler catches exception by matching with one or more exception types.
\item after catching, a handler executes like a normal subroutine
\item handler can return, reraise the current exception, or raise a new exception
\item re-raise terminate current handling and continuing propagation of caught exception.
\begin{itemize}
\item useful if a handler cannot deal with an exception but needs to propagate same exception to handler further down the stack.
\item provided by a raise statement without an exception type (\code{throw;}) where a raise must be in progress.
\end{itemize}
\item an exception is handled only if the handler returns rather than reraises
\end{enumerate}
\item[guarded block] is a language block with associated handlers, e.g., try-block in C++/Java.
\item[unguarded block] is a block with no handlers.
\item[termination] means control cannot return to the raise point. All blocks on the faulting stack from the raise block to the guarded block handling the exception are terminated, called stack unwinding.
\item[resumption] means control returns to the raise point: no stack unwinding.
\item[EHM] = Exception Type + Raise (exception) + Propagation + Handlers
\end{description}

\subsubsection{Static and Dynamic Returns}
All routines and exceptional control flows can be characterized by the following properties:
\begin{description}
\item[static/dynamic call] routine/exectpion name at the call/raise is looked up statically (compile-time) or dynamically (run-time)
\item[static/dynamic return] after routine handler completes, it returns to its static (definition) or dynamic (call) context.
\end{description}

\begin{table}[ht]
\centering
\begin{tabular}{r||l|l}
  return/handled & static  & dynamic \\
  \hline \hline
  static         & sequel  & termination exception \\ \hline
  dynamic        & routine & routine pointer, virtual routine, resumption \\
  \end{tabular}
\end{table}

\subsubsection{Static Propogation}
A {\bf sequel} is a routine with no return value where the sequel name is looked up lexically at the call site and control returns to the end of the block in which the sequel is declared. This is called {\bf static propogation}.

\begin{verbatim}
for(;;) {
    sequel f() { ... }
    // ...
    f();
    // not executed
}
// executed immediately after f();
\end{verbatim}

These are implemented in \code{try-catch}s. You can always determine statically what line a \code{catch} clause will execute when it returns: \code{catch}s are sequels. Note that sequels are not commonly used as-defined since they only work for monolithic programs as they must be statically nested at the point of use.

\subsubsection{Dynamic Propogation}
Termination and resumption both have dynamic raises with static and dynamic returns, respectively. This method of propogation works for seperately-compiled programs, but does not allow for statically knowing the handler.

For {\bf termination}, control transfers from the start of propogation to a handler (dynamically) then performs a static return (on handler return) which unwinds the stack. There exist three basic termination forms from a \textit{non-recoverable} operation: \code{non-local}, \code{terminate}, and \code{retry}. Non-local is general but has the goto problem, terminate is more limited, and \code{retry} is a combination of termination with special handler semantics (ie. restart the guarded block handling the exception [Eiffel], which pretends end-of-file is an exception of type EOF).

Since we can simulate \code{retry}, it is rarely supported directly.

In c++, we can toggle throwing exceptions (over returning status codes) in IO code with:
\begin{verbatim}
ifstream infile;
infile.exceptions(ios_base::failbit);

// ...
// do things
// ...

try {
  getline(infile, 42);
} catch (ios_base::failure) { }
\end{verbatim}

Note that $\mu$c++ does this by default.

A destructor can raise an exception, but this will cause errors if the exception is raised \textit{during propogation}:
\begin{verbatim}
struct E {};
struct C {
    ~C() { throw E(); }
}

try {
    C x;
    throw E();
} catch (E) {
    // ...
}
\end{verbatim}
this would not have caused issues without the \code{throw()} within the \code{try-catch}.

{\bf Resumption} provides a limited mechanism to generate new blocks on the call stack: control transfeers from the start of propogation to handler dynamically then returns dynamically on handler return -- note that this does not unwind the stack. A resumption handler is corrective action so a computation can continue: eg. the \code{new\_handler} fixup method in c++. If corrective action is impossible, the resumption handler should throw and exception instead of stepping into the enclosing block so the stack will unwind.

So long as a resumption handler remains on the stack, it may not be reused until it has caught and completed handling an exception. This precludes the handler from re-raising and re-catching the same exception. For this reason, propogation ignores unfinished resumption handlers when searching for an exception handler.

\subsubsection{Implementation}
To implement any dynamic propogation system, the \code{raise} must know the last guarded block with a handler for the raised exception type.

One approach is to do a lot of fuckery with labels, setting and resetting them upon entering or leaving a block. For termination, though, a direct transfer is often impossible anyway since activations on the stack may contain objects with destructors or finalizers. Resetting these labels are exepnsive, so the labels are often stored in the \code{catch}/destructor data within each block and the handler is found by linear search during a tack walk (with no direct transfer taking place). These are often implemented using that expensive approach on raise and zero cost on guarded-block entry, to improve overall efficiency.

% TODO: missing

\section{Coroutines}
% TODO: missing

\subsection{Semi-Coroutines}
% TODO: missing

\subsection{Full Coroutines}
{\bf Full Coroutines} have resume cycles. A full corotuine is allowed to perform semi-coroutine operations because it subsumes the notion of a semi-routine. We can think of standard routines as a linear path through our stack and semi-cortouines as a branching structure. A full coroutine, then, is a branch structure with occasional loopbacks.

\subsection{Raising}
We can either \code{throw} or \code{resume} when we want to raise an exception. Either acts as a rethrow or reresume (respectively) when no exception type is specified. Optionally, we can throw to other stacks, eg. with $\mu$cpp's \code{\_At} clause.

\subsubsection{Nonlocal Exceptions}
Local exceptions within a coroutine are the same as for exceptions within a routine or class, with one nonlocal difference: an unhandled exception raised by a coroutine raises a nonlocal exception of type \code{BaseCoroutine::UnhandledException} at the coroutine's last resumer and then terminates the coroutine. Note that nonlocal exception delivery is initially disabled for a coroutine -- it must be explicitely enabled at compile-time with the \code{\_Enable} block.

\section{Concurrency}
A {\bf thread} is an independant sequential execution path through a program. Each thread is scheduled and executed independantly from other threads.

A {\bf process} is a program componenbt such as a routine which has its own thread and the same state information as a coroutine.

A {\bf task} is similar to a process except that it is reduced along some dimension -- it is often the case that a process has its own memor, while tasks share common memory. We sometimes refer to tasks as {\bf Light-Weight Processes} (LWPs).

{\bf Parallel execution} is when two or more operations occur simultaneously, which can only occur when multiple processors are present.

{\bf Concurrent execution} is any situation in which execution of multiple threads \textit{appears} to be performed in parallel. It is the threads of control associated with processes and tasks that results in concurrent execution.

\subsection{Concurrent Hardware}
Concurrent thread execution possible with a uniprocessor using {\bf multitasking} and {\bf multiprocessing}. Parallelism is imulated through context switching. Unlike coroutines, task switching may occur at non-deterministic program locations, includeing between any two machine instructions. This introduces virtually all the difificulties in concurrent programs: concurrent programs must be written to work properly regardless of non-deterministic ordering of program execution.

\subsection{Concurrent Systems}
There are three major types of concurrent systems: systems with {\bf explicit} constructs, {\bf implicit} constructs, or those which attempt to discover concurrency in otherwise sequential programs. Note that the latter two types are built upon the first.

\subsection{Mutual Exclusion}
Is it possible to write code guaranteeing a statement is always serially executed by 2 threads? Yes, Dekker could do it!

To design a successful mutual exclusion section, we must ensure
\begin{enumerate}
\item Only one thread can be in a critical section at a time with respect to a particular object (safety).
\item Threads may run at arbitrary speed and in arbitrary order, while the underlying system guarantees a thread makes progress (i.e., threads get some CPU time).
\item If a thread is not in the entry or exit code controlling access to the critical section, it may not prevent other threads from entering the critical section.
\item In selecting a thread for entry to a critical section, a selection cannot be postponed indefinitely (liveness). Not satisfying this rule is called indefinite postponement or livelock.
\item After a thread starts entry to the critical section, it must eventually enter. Not satisfying this rule is called starvation.
\end{enumerate}

\subsubsection{Dekker's Algorithm}
Dekker's algorithm is not read-write safe, though an RW-safe version exists. The algorithm also has unbounded overtaking (not starvation) because the race losed retracts intent (this is allowed - we do not prevent entry to the critical section by the delayed thread). A thread exiting the critical section does not exclude itself for reentry.

\subsubsection{Peterson}
The Peterson algorithm is much shorter; simply start a race condition then immediately block on it. This algorithm does not have unbounded overtaking and technically cheats according to the criteria for a mutually exclusive algorithm. It is also RW unsafe.

We now attempt to generalize these algorithms to $n > 2$ threads. We note that we should only need $n$ bits to solve the problem... that said, there is no known solution for all rules in only $n$ bits. There is a RW-unsafe algorithm in $3n$ bits and a RW-safe algorithm in $4n$ bits.

\subsubsection{Lamport}
Lamport's algorithm was the first successful solution to the general case (also: RW-safe) in $nm$ bits, where $m$ is the size of the key used. Hehner/Shyamasundar ``simplified'' it to a RW-unsafe case.

% TOURNAMENT (Tuabenfield/Buhr)

\subsection{Arbiter}
An arbiter is a full-time task which controls entry to critical sections. This task must be constantly executing and ends up converting mutual exclusion problems to synchronization problems.

\subsection{Hardware Solutions}
There also exist hardware solutions to this problem. These solutions cheat by making assumptions about execution (eg. y controlling order and speed of execution). This allows the elimination of much of the shared information and the checking required in the software solution. Generally, this is done by providing special atomic operations. This is sufficient for multitasking on a single CPU.

\section{Locking Mechanisms}
Locks may either yield or not, and may either spin or not (block). Any type of lock can be used for either synchronization or mutual exclusion.

\subsection{Spin Locks}
A spin lock must watch the lock to see when it is opened, within the process that is blocked.

Some spin-locks are adaptive in spin time, though most spin locks allow for possible task starvation. A spin lock may be most appropriate when there is no other work to do.

\subsection{Blocking Locks}
An acquiring task makes only one check for open lock and blocks. The \textit{releasing} task has the sole responsibility for detecting blocked acquirers and transferring the lock (or simply releasing it if there are no blocked processes).

Blocking locks reduce busy-waiting through cooperation. This improves speed, but leaves tasks in a zombie state if they are not woken up by the foreign process.

Note that blocking locaks require logic for selecting which task to wake up.

\subsection{Mutex Locks}
A mutex lock is used only for mutual exclusion. By restricting its usage, we can optimize it to the task;

A {\bf single acquisition} mutex lock may only be acquired once. A {\bf multiple acquisition} mutex lock can be acquired multiple times (from the same owner); we also call these owner locks. Multiple acquisition locks are very useful for recursive calls.

A specializied multiple acquisition lock for IO is called a {\bf stream lock}.

\subsection{Synchronization Locks}
These are the weakest form of blocking locks since they are stateless (other than the list of blocked tasks). Acquiring this lock always blocks and the release is lost when no other threads are waiting.

\section{Buffering}
Tasks communicate unidirectionally through a queue; the producer adds items, the consumer removes them.

An {\bf unbounded buffer} allow the producer to produce indefinitely, as the queue never fills up. A counting semaphore can control access to this shared queue.

\section{Barrier}
A {\bf barrier} coordinates a group of tasks performing a concurrent operation surrounded by sequential operations. It specifies the start and (optionally) end times of all tasks, ie.
\begin{verbatim}
Barrier start(n), end(n);

start.block(); // wait for threads to start
// work
end.block(); // wait for threads to end
\end{verbatim}

A barrier can be used only for synchronization, not mutual exclusion. Unlike previous synch locks, a barrier retains state information as well as the number of tasks blocked on it. The below case ensures that \code{S1}, \code{S2}, and \code{S3} occur before \code{S5}, \code{S6}, and/or \code{S7}.
\begin{verbatim}
Barrier b(3);

T1::main() {
  S1;
  b.block();
  S5;
}

T2::main() {
  S2;
  b.block();
  S6;
}

T3::main() {
  S3;
  b.block();
  S7;
}
\end{verbatim}

\subsection{Semaphores}
\subsubsection{Binary}
A {\bf binary semaphore} is a blocking equivalent to a yielding spin-lock. It can provide both synchronization and mutual exclusion. This more powerful than either a synch lock or a mutex lock. Uses \code{lock.P()} to acquire and \code{lock.V()} to release. \code{lock.acquire()} waits until the semaphore is zero, then decrements it.

A {\bf split binary semaphore} is a collection of semaphores within which at most one in the collection has the value 1. This can be used when different kinds of tasks have to block seperately. This gives us the ability to unblock a specific kind of task and to collectively manage multiply semaphores in a more simple way.

\subsubsection{Counting}
A {\bf counting semaphore} allows multi-values semaphores to have arbitrary states (besides open and closed). This allows for $n$ simultaneous tasks. We can augments \code{V} to allow increasing the counter an arbitrary amount.

\section{Freshness}
In addition to the five rules, we must ensure {\bf freshness} / {\bf staleness}. This involves ensuring no reader (in a reader-writer problem) reads data written after it arrived (too fresh) or reads data from the past (too stale). This is generally a problem when we have split waiting queues; the solution is generally to combine those queues into a single priority queue, use cooperation to wake groups of two tasks at a time, then have the second wait in a higher-priority single-node queue if the second and first are not of the same type.

\section{Concurrent Errors}
\subsection{Race Conditions}
A {\bf race condition} occurs when we miss some instances of either mutual exclusion or synchronizatoin; two more tasks race along assuming synchronization or mutual exclusion is guaranteed -- but it is not.

\subsection{No Progress}
\subsubsection{Live Lock}
If we have a {\bf live lock}, there is indefinite postponement. This is caused by poor scheduling in the entry protocol. There always exists some mechanism to break the tie on simultaneous arrival that deals effectively with live-lock.

\subsubsection{Starvation}
A selection algorithm ignores one or more tasks so they are never executed, ie. a lack of long-term fairness. Infinite starvation is extremely rare, but short-term starvation can occur and may be enough of a problem. Like live-lock, starving taks might be ready at any time, switching among action and blocked.

\subsubsection{Deadlock}
A {\bf deadlock} is the state when one or more prcesses are waiting for an event that will not (and \textit{can not} occur).

There are five conditions which must occur to cause a deadlock:
\begin{enumerate}
\item there exists more than one shared resource requiring mutual exclusion
\item a process holds a resource while waiting for access to a resource held by another process (hold and wait)
\item once a process has gained access to a resource, the runtime system cannot get it back (no preemption)
\item there exists a circulay wait of processes on resources
\item these conditions must occur simultaneously
\end{enumerate}

A {\bf synchronization deadlock} is a failure in cooperation, so a blocked task is never unblocked (stuck waiting forever). For example
\begin{verbatim}
void uMain::main() {
  uSemaphore s(0);
  s.P();
}
\end{verbatim}

A {\bf mutual exclusion deadlock} is the failure to acquire a resource protected by mutual exclusion. This is a deadlock unless one (or more) of the tasks backs away from the lock.

\subsection{Deadlock Prevention}
Eliminate one or more conditions to ensure a deadlock is impossible.

\subsubsection{Synchronization Prevention}
Eliminate all synhronization (communication). Tasks must then be completely independant and generate results only through side-effects.

\subsubsection{Mutual Exclusion Prevention}
Deadlock can be prevented by eliminating any one of these conditions:
\begin{itemize}
\item No mutual exclusion.This is often impossible.
\item No hold and wait: Do not give any resource unless all resources can be given. This often leads to poor resource utilization and starvation.
\item
\end{itemize}

% TODO: missing

\section{Indirect Communication}
\subsection{Critical Regions}
% TODO: missing

\subsubsection{Conditional Crititcal Sections}
% TODO: missing

\subsection{Monitors}
A {\bf monitor} is an abstract data type that combines shared data with serialization of its modification. A {\bf mutex member} is one that does not begin execution if there is another active mutex member. A call to a mutex member may become blocked waiting entry and queues of waiting tasks may form. Public member routines of a monitor are implicitely mutex and other kinds of members can be made explicitely mutex (\code{\_Mutex}). You can consider a monitor as implicitly having a lock acquire at entry to any member and a release on exit.

This gives us mutual exclusion across members, eg for
\begin{verbatim}
_Monitor Counter {
    int counter
public:
    Counter() : counter(0) { }
    int inc() { return ++counter; }
    int dec() { return --counter; }
}
\end{verbatim}
we have mutual exclusion across both \code{inc()}, \code{dec()}, and both.

\subsection{Scheduling}
A monitor may want to schedule tasks in an order different from the order in which they arrive (eg. reader/writers with freshness/staleness issues). There are two techniques: external and internal scheduling;
% TODO: missing

External scheduling is generally easier to use, but sometimes is impossible; it can not be used when scheduling depends on parameters or requires being blocked within a member.

\subsubsection{External Scheduling}
The accept statement conrtols which mutex members can accept calls. By preventing certain members from accepting calls at certain times, we can control the scheduling of tasks.

\subsubsection{Internal Scheduling}
Internal scheduling involves scheduling among the tasks inside the monitor. A {\bf condition} is a queue of waiting tasks. A tasks waits by placing itself on a condition (\textit{atomically} place self on back of queue, sleep, and release monitor lock). A task on a condition queue is made ready by signalling the condition; this removes the blocked task on the front of the queue and makes it ready.

\section{Increasing Concurrency}
Given two tasks involved in direct communication (a client and a server), it is possible to increase the cocurrency on both sides.

\subsection{Server}
The server is responsible for managing resources and should increase concurrency by introducing administration tasks to the entrane/exit of its concurrent functions instead of executing them within them (ie. to decrease the time in the critical section).

Alternatively, all server ``blocking'' can be managed by the client.

\subsection{Client}
The client should have the shortest delay as possible, though sometimes it may need to wait for a server request to process. This may be accomplished with asynchronous wait routines.

Asynchronous calls are cimple when the client does not need the result. If it does, we must split this into two calls (\code{callee.start(args)} and \code{callee.finish()}). In the case where the server is not yet done by the second call, we may place that call in eg. a polling loop.

In this case, the server needs a protocol to check which result is being finished.

One way to do this is with {\bf tickets}; the server immediately returns a ticket upon the first call then the client uses that ticket to request the results. Note that in the case of using this ticket approach, the system is somewhat error-prone: the client may not obey the protocol by never retreiving a result, using the same ticket twice, forging a ticket, etc.

Another method is through the use of {\bf callback routines}. The client may provide the server with a function to be run after the server completes the method call. The function is provided with the return value of the the original server method and may operate on it in whatever fashion. Additionally, the callback routine may set some indicator bit which the client may poll to determine when the call has completed.

A {\bf future} provides the same asynchrony as the above without any explicit protocol. The protocol becomes implicit between the future and the task generating the result. Further, it removes the difficult problem of when the caller should try to retreive the result. The future is a container object which is returned immediately as an empty object and updated to have the new data as soon as that data is ready. If the client attempts to access the object before it is ready, the client will simply block until the data exists.

\section{Optimization}
There exist several general forms of optimization, generally restricted by the kind of execution environment. The general forms of optimizations are
\begin{description}
\item[reordering] data and code are re-ordered to increase performance in certain contexts.
\item[eliding] removal of unnecessary data, data accesses, and computation.
\item[replication] processors, memory, data, code are duplicated because of limitation and communication speed (speed of light).
\end{description}

The optimized program must be isomorphic to the original.

\subsection{Sequential Execution}
{\bf Sequential} execution presents simple optimization semantics because operations occur in {\bf program order}. We find {\bf data dependencies} and {\bf control dependencies}; the former is often reorderable but the latter tends not to be.

Thus we can reorder disjoint operations (where variables have different addresses), elide unnecessary operations (dead code or unnecessary lines), and execute in parallel if there exist multiple functional-units.

\subsection{Memory Hierarchy}
Optimizing data flow through memory defines a program's speed. Hardware aggressively optimizes data flow for sequential execution; having an understanding of the cache is essential to understanding performance of both sequential and concurrent programs.

\subsection{Concurrent Execution}
We can only optimize concurrent sections of our code. When we attempt to parallelize sequential code, we must be very careful; often, parallelizing sequential code is either impossible or not worth the additional overhead.

\end{document}
