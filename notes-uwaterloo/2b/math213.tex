\documentclass[12pt]{article}
\usepackage{amsmath,amssymb,hyperref,mathtools,parskip,custom}
\usepackage[margin=.8in]{geometry}
\allowdisplaybreaks
\hypersetup{colorlinks,
    citecolor=black,
    filecolor=black,
    linkcolor=black,
    urlcolor=black
}

\newcommand{\laplace}[1]{\ensuremath{\mathcal{L} \{#1\}}}

\begin{document}

\title{MATH 213 --- Advanced Mathematics for Software Engineers}
\author{Kevin James}
\date{\vspace{-2ex}Spring 2014}
\maketitle\HRule

\tableofcontents
\newpage

\section{Differential Equations}
{\bf Differential equations} are equations involving derivatives with respect to some independant variable. For example, Newton's Law states \[ M \ddot x = F \] or \[ M \dderiv{x}{t} = F \]

In the {\bf classical approach}, we suppose $f$ is given as a function of time, and we solve for the dependant variable with respect to the independant one. For example, \[ F(x) \function x(t) \]

The {\bf systems approach} has less of an emphasis on the response to a specific input and deals more with the overall relationships between the function and between the individual dependant variables.

\subsection{Examples}
The population of an organism (given abundant resources) or the growth of an economy (if the economy were to grow at a constant percentage rate) can be modelled as:
\begin{align*}
\dot x &= ax\\
\frac{\dd x}{x} &= a \dd t\\
\int \frac{\dd x}{x} &= \int a \dd t\\
\ln x + C_1 &= at + C_2\\
\ln x &= at + C_3\\
x(t) &= e^{at + C_3}\\
x(t) &= e^{at} \times e^{C_3}\\
x(t) &= C_4 \times e^{at}\\
x(t) &= x(0) \times e^{at}
\end{align*}
where the value of $x(0)$ is called the {\bf initial condition}.

Given that this function assumes that the population growth is not limited by resources, etc., it is not very useful in the real world. More likely, we would find for large populations a limit of some sort must be included. For example, the {\bf logistic equation} is modelled as:
\begin{align*}
\dot x &= ax - bx^2\\
\frac{\dd x}{ax - bx^2} &= \dd t\\
\int \frac{\dd x}{ax - bx^2} &= \int \dd t\\
\int \frac{\dd x}{x(a - bx)} &= \int \dd t\\
\int \dd x (\frac{A}{x} + \frac{B}{a - bx}) &= \int \dd t\\
\int \dd x (\frac{1}{ax} + \frac{B}{a - bx}) &= \int \dd t\\
\int \dd x (\frac{1}{ax} + \frac{b}{a(a - bx)}) &= \int \dd t\\
\int \frac{\dd x}{ax} + \int \frac{b \dd x}{a(a - bx)} &= \int \dd t\\
\frac{\ln x}{a} + \frac{b}{a}\frac{-1}{b}\ln(a - bx) &= t + C_0\\
\frac{1}{a} \bigg(\ln\frac{x}{a - bx}\bigg) &= t + C_0\\
\frac{x}{a - bx} &= e^{at + aC_0}\\
x &= C_1 e^{at} (a - bx)\\
x &= \frac{aC_1 e^{at}}{1 + bC_1e^{at}}\\
x &= \frac{a}{b} \bigg(\frac{1}{1 + C_2 e^{-at}}\bigg)
\end{align*}
Where $C_2 = \frac{1}{bC_1}$. In this case, the population will ``level out'' at $ax = bx^2$ (i.e.\ have an asymptote). The solution to this specific {\bf DE (differential equation)} is called the {\bf logistic curve}.

\subsection{Partial Differential Equations}
{\bf PDEs (partial differential equations)} arise when there is more than one independant variable.

If we were to model the vibration of a string, we would use a PDE.\@ Assuming there is no length-wise vibration (i.e.\ horizontal motion), that the string has constant tension and mass per unit length, and that we are only considering small transverse displacements, we could write Newton's equation $F = ma$ as \[ F = (\rho\Delta x) \times \dderiv{y}{t} \]

Since all forces on this string are tension, we have \[ F = T\sin\theta_1 - T\sin\theta_2 \]

Given that we have small displacements (which leads to small angles $\theta_1$ and $\theta_2$), we can replace all instances of $\sin$ with $\tan$. Finally, this gives us \[ F \approx T\deriv{y}{x} \bigg|_{x+\frac{\Delta x}{2}} - T\deriv{y}{x} \bigg|_{x-\frac{\Delta x}{2}} \]

so therefore \[ \rho\dderiv{y}{t} = \frac{T}{\Delta x} \bigg(\deriv{y}{x} \bigg|_{x+\frac{\Delta x}{2}} - \deriv{y}{x} \bigg|_{x-\frac{\Delta x}{2}}\bigg) \]

As $\Delta x$ approaches zero, we see that \[ \dderiv{y}{t} = \frac{T}{\rho} \dderiv{y}{X} \]

We will not be solving PDEs in this course, but this equation is solveable to give us \[ y = A\sin k\bigg(x - t\sqrt{\frac{T}{\rho}}\bigg) \]

\subsubsection{Boundary Conditions}
This equation should additionally have several {\bf boundary conditions} which represent the ends of the string being fixed in place.

\begin{align*}
y(0, t) &= 0\\
y(L, t) &= 0
\end{align*}

We use these equations as well as the equation for a standing wave to get \[ y = A_+ \sin k \bigg( x - t\sqrt{\frac{T}{p}} \bigg) + A_- \sin k \bigg( L - t\sqrt{\frac{T}{\rho}} \bigg) \]

Since $y(0, t) = 0$,
\begin{align*}
0 &= A_+ \sin k \bigg( 0 - t\sqrt{\frac{T}{p}} \bigg) + A_- \sin k \bigg( 0 - t\sqrt{\frac{T}{\rho}} \bigg)\\
A_+ &= A_- = A
\end{align*}

and since $y(L, t) = 0$,
\begin{align*}
y &= A_+ \sin k \bigg( L - t\sqrt{\frac{T}{p}} \bigg) + A_- \sin k \bigg( L - t\sqrt{\frac{T}{\rho}} \bigg)\\
KL &= \pm n\pi
\end{align*}

Because waves can only have certain frequences we have
\begin{align*}
K \sqrt\frac{T}{\rho} &= \pm \frac{n\pi}{L} \sqrt\frac{T}{\rho}\\
f &= \pm \frac{n}{2L} \sqrt\frac{T}{\rho}
\end{align*}
where $n = 1$ implies a fundamental frequency and $n \geq 2$ implies a harmonic frequency.

\section{Linear Differential Equations}
A {\bf linear differential equation} has the form: \[ a_n(t)\nderiv{y(t)}{t}{n} + a_{n-1}(t)\nderiv{y(t)}{t}{n-1} + \cdots + a_0(t) y(t) = b_n(t)\nderiv{f(t)}{t}{n} + b_{n-1}(t)\nderiv{f(t)}{t}{n-1} + \cdots + b_0(t) f(t) \]

In this equation, we have two dependant variables: $f$ and $y$. The `classical' approach is to assume $f(t)$ is given, then solve for $y(t)$.

This equation is linear in the sense that it obeys the principle of superposition: if $y_1(t), y_2(t)$ are solutions corresponding to $f_1(t), f_2(t)$, then if $f(t) = k_1f_1(t) + k_2f_2(t)$, then $y(t) = k_1y_1(t) + k_2y_2(t)$.

Given constant coefficients, we would write this equation as \[ a_n\nderiv{y(t)}{t}{n} + a_{n-1}\nderiv{y(t)}{t}{n-1} + \cdots + a_0y(t) = b_n\nderiv{f(t)}{t}{n} + b_{n-1}\nderiv{f(t)}{t}{n-1} + \cdots + b_0f(t) \]

In shorthand, given
\begin{align*}
Dy &= \deriv{y}{t}\\
D^2 y &= \dderiv{y}{t}\\
D^n y &= \nderiv{y}{t}{n}
\end{align*}
we can write this as \[ Q(D)y = P(D)y \] where $Q(x)$ and $P(x)$ are polynomials.

In general, we can assume $P(D) = 1$.

\begin{proof}
Suppose $\tilde y(t)$ is a solution of $Q(D)y = f$ (e.g.\ $Q(D) \tilde y = f$). Now let $y = P(D) \tilde y$. Then
\begin{align*}
Q(D)y &= Q(D)P(D) \tilde y\\
&= P(D)Q(D)\tilde y\\
&= P(D) f
\end{align*}
So $y = Q(D)\tilde y$ solves the original equation.
\end{proof}

\begin{example}
Prove $Dy = (D+1)f$
\end{example}

\begin{proof}
Suppose $Dy = (D + 1)f$ and $f(t) = t, \forall t$. Let's find $\tilde y$ that solves
\begin{align*}
Q(D)\tilde y &= f\\
D\tilde y &= f\\
D\tilde y &= t
\end{align*}
So $\tilde y = \half t^2 + C$. Let
\begin{align*}
y &= P(D)\tilde y\\
&= (D+1)\bigl(\half t^2 + C\bigl)\\
&= t + \half t^2 + C \half
\end{align*}
Then
\begin{align*}
D\bigl(t + \half t^2 + C\bigl) &= 1 + t\\
&= (D + 1)t
\end{align*}
\end{proof}

\begin{definition}
If $f(t)$ is continuous on an interval $a \leq t \leq b$, then there exists a solution $y(t)$ satisfying the above differential equation and also the ``intial conditions'' for $a \leq t_0 \leq b$
\begin{align*}
y(t_0) &= P_0\\
\deriv{y(t_0)}{t} &= P_1\\
\nderiv{y(t_0)}{t}{n-1} &= P_{n-1}
\end{align*}
Moreover, this solution is unique.
\end{definition}

\begin{example}
For a falling block, given the mass and gravitational force, determine the distance fallen over a given time $t$.
\end{example}

\begin{proof}
\begin{align*}
m \ddot y &= mg\\
\ddot y &= g\\
\dot y &= gt + C_1\\
y &= \half gt^2 + C_1t + C_2
\end{align*}
For an $n$-th degree differential equation, we will end up with $n$ unknowns. Thus, we will need $n$ sets of initial conditions to solve for a general solution. Let's assume $y(0) = 0$. This gives us
\begin{align*}
y &= \half gt^2 + C_1t + C_2\\
0 &= C_2\\
\end{align*}
So we now have $y = \half gt^2 + C_1t$. Given a second initial condition $\dot y(0) = 0$, we have
\begin{align*}
\dot y &= gt + C_1\\
0 &= C_1
\end{align*}
Thus \[ y = \half gt^2 \]
\end{proof}

The {\bf general solution} of the equation is an expression for $y$ that solves the equation and contains $n$ arbitrary constants.

Given a differential equation of order $n$ and $n$ initial conditions, we have an {\bf initial value problem}.

To solve such problems, we find the general solution of the differential equation and plug in the initial conditions to evaluate the arbitrary constants.

\begin{example}
We'll first find the general solution of the auxillary equation $q(D)y = 0$ (a ``homogeneous equation'' because the right-hand side is zero). Note: this contains $n$ arbitrary constants and is called the complimentary solution $y_c$.

Then, we'll find any solution of the original equation \[ Q(D)y = f \] This is called a particular solution $y_p$.

To see why this works, let \[ y = y_c + y_p \] Then
\begin{align*}
Q(D)y &= Q(D)(y_c + y_p)\\
&= Q(D)y_c + Q(D)y_p\\
&= 0 + f\\
&= f
\end{align*}
\end{example}

\begin{example}
Prove $(D^2 - 6D + 9) = 0$.

Then
\begin{align*}
Q(m) &= 0\\
m^2 -6m + 9 &= m\\
{(m - 3)}^2 &= 0\\
m &= 3
\end{align*}
where $m$ has multiplicity $2$.

If $y = y_1$ is a solution of $Q(D)y(t) = f(t)$, then substituting $y = y_1v$ yields an equation of order $n=1$ in $\dot v$. Thus we have
\begin{align*}
y_1 &= e^{3t}\\
y &= e^{3t}v\\
Dy &= 3e^{3t}v + e^{3t}\dot v\\
D^2y &= 3(3e^{3t}v + e^{3t}\dot v) + 3e^{3t}\dot v + e^{3t} \ddot v\\
(D^2 - 6D +9)y &= 0\\
9e^{3t} + 6e{3t} \dot v + e^{3t} \ddot v -6 (3e^{3t}v + e^{3t}\dot v) + 9e^{3t} v &= 0\\
e^{3t} \ddot v &= 0\\
\ddot v &= 0\\
\dot v &= c_0\\
v &= c_1t + c_2
\end{align*}
So we have another solution to our original equation \[ y_c = y_1v = e^{3t} (c_1 t + c_2) \]
\end{example}

Generally, if the repeated root $\lambda$ is of multiplicity $k$, then the complimentary solution would be \[ (c_1 + c_2t + \cdots + c_k t^{k-1})e^{\lambda t} \] Of course this implies that with multiplicity $1$ we have \[ ce^{\lambda t} \] The general solution, then, is the given by the sum of all such terms for every $\lambda$ of the characteristic equation.

\begin{example}
For a block of mass $M$ attached to a spring with constant $k$, we have
\begin{align*}
M\ddot y &= ky\\
\ddot y &= \frac{k}{M}y = 0\\
\bigg(D^2 + \frac{k}{M}\bigg)y &= 0\\
Q(m) &= 0\\
M^2 + \frac{k}{M} &= 0\\
m &= \pm i\sqrt\frac{k^2}{M}
\end{align*}

By our method we have
\begin{align*}
y &= c_1 e^{i\sqrt\frac{k}{M} t} + c_2 e^{-i\sqrt\frac{k}{M} t}\\
&= c_1\bigg( \cos\bigl(\sqrt\frac{k}{M} t\bigl) + i\sin\bigl(\sqrt\frac{k}{M} t\bigl)\bigg) + c_2 \bigg(\cos\bigl(-\sqrt\frac{k}{M} t\bigl) + i\sin\bigl(-\sqrt\frac{k}{M} t\bigl)\bigg)
\end{align*}

Consider the potential initial conditions. We would have
\begin{align*}
y(t_0) &= c_1 e^{i\sqrt\frac{k}{M} t_0} + c_2 e^{-i\sqrt\frac{k}{M} t_0}\\
\dot y(t_0) &= i\sqrt\frac{k}{M} \bigg(c_1 e^{i\sqrt\frac{k}{M} t_0} + c_2 e^{-i\sqrt\frac{k}{M} t_0}\bigg)
\end{align*}

So we can find
\begin{align*}
c_1 &= \frac{j\sqrt\frac{k}{M} y(t_0) + \dot y(t_0)}{2i\sqrt\frac{k}{M} e^{i\sqrt\frac{k}{M} t_0}}\\
c_2 &= \frac{i\sqrt\frac{k}{M} y(t_0) - \dot y(t_0)}{2i\sqrt\frac{k}{M} e^{-i\sqrt\frac{k}{M} t_0}}
\end{align*}
or in other words, if $y(t_0)$ and $\dot y(t_0)$ are real-valued, $c_2$ is the complex conjugate of $c_1$ ($c_1 = c_2^*$).

It thus follows that \[ y = 2 |c_1| \cos\bigg(\sqrt\frac{k}{M} t_0 + \angle c_1\bigg) \]
\end{example}

\subsection{Finding Particular Solutions}
The {\bf method of undetermined coefficients} works for functions $f(t)$ of the form of a polynomial, exponential, or a sum of the them. The main idea is to ``guess'' the form of a particular solution. Generally, this solution is of the same form as $f(t)$.

\begin{example}
% CIRCUIT QUESTION EXAMPLE
Suppose $f(t) = t \cdot \forall t$. Then we may guess that we may have $y_p (t) = k t$, which gives us $Dy_p = k$. To satisfy our differential equation, we need
\begin{align*}
(D + 50)y_p &= f = t\\
k + 50kt &= t
\end{align*}
since there is no constant solution for $k$, this can not be true. So we can guess that we have a more complicated equation $y_p (t) = k_0 + k_1 t$. This gives us $Dy_p = k_1$ and we have
\begin{align*}
(D + 50)y_p (t) &= f(t) = t\\
k_1 + 50(k_0 + k_1 t) &= t\\
t &= \frac{1}{50} + 50\bigg(-{\bigg(\frac{1}{50}\bigg)}^2 + \frac{1}{50}t\bigg)
\end{align*}
which gives us $k_0 = -0.0004$ and $k_1 = 0.02$.

So the solution of the equation is
\begin{align*}
y &= y_c + y_p\\
&= ce^{-50t} - 0.0004 + 0.02t
\end{align*}

Suppose an initial condition is given: $y(0) = 0$. So we have
\begin{align*}
y(t) &= ce^{-50t} - 0.0004 + 0.02t\\
0 &= ce^0 - 0.0004 + 0.02(0)\\
&= c - 0.0004\\
c &= 0.0004
\end{align*}
\end{example}

The general method is as follows:
\begin{itemize}
\item for a polynomial function $f(t)$ of degree $d$, assume that the particular solution is of degree $d$ with \emph{undetermined coefficients}: \[ y_p(t) = k_0 + k_1t + \cdots + k_d t^d \]
\item for an exponential function $f(t) = e^{\lambda t}$, assume that a particular solution is \[ ke^{\lambda t} \]
\item for sums of polynomials and exponentials, look for particular solutions which are sums of the above two forms
\item if any of the terms in the particular solution presented by the above rules occur in the complementary solution, multiply that term by the smallest power of $t$ which will give us a value not present in the complementary solution
\end{itemize}

\begin{example}
Given the response of an RC circuit to exponentials \[ (D + 50)y(t) = f(t) = e^{\lambda t} \] we can, by our second rule, try
\begin{align*}
y_p(t) &= ke^{\lambda t}\\
Dy_p &= k\lambda e^{\lambda t}
\end{align*}
and through substitution \[ k\lambda e^{\lambda t} + 50ke^{\lambda t} = x^{\lambda t} \]

So we have
\begin{align*}
k\lambda + 50k &= 1\\
k &= \frac{1}{50+\lambda}
\end{align*}
if and only if $\lambda \neq -50$.  In this case, $y_p(t)$ solves the auxiliary equation $Q(D)y = 0$.

If $\lambda = -100$, $y_p(t) = -\frac{1}{50}e^{\lambda t}$ is a particular solution. But if $\lambda = -50$, then we apply our fourth rule
\begin{align*}
y_p(t) &= kte^{\lambda t}\\
Dy_p(t) &= ke^{\lambda t} + \lambda kte^{\lambda t}
\end{align*}

Substituting, we have
\begin{align*}
Dy_p + 50 y_p(t) &= e^{\lambda t}\\
ke^{\lambda t} - \lambda kte^{\lambda t} + 50kte^{\lambda t} &= e^{\lambda t}\\
\lambda k &= -50\\
k &= 1
\end{align*}

This gives us the general solution \[ y(t) = ce^{-50t} + te^{-50t} \]
\end{example}

Note that our method allows for sinusoidal functions $f(t)$. We have \[ \cos\omega t = \frac{e^{j\omega t} + e^{-j\omega t}}{2} \] and \[ \sin\omega t = \frac{e^{j\omega t} - e^{-j\omega t}}{2j} \]

\begin{example}
If $f(t) = e^{j\omega t}$ (for the previous example), we can apply the second method to get the particular solution \[ y_p(t) = \frac{50}{50 + j\omega} e^{j\omega t} \]

If we instead have $f(t) = e^{-j\omega t}$, we set \[ y_p(t) = \frac{1}{50-j\omega}e^{-j\omega t} \]

So if $f(t) = \cos\omega t$, then, by linearity, we can take \[ y_p(t) = \half \bigg(\frac{1}{50+j\omega}e^{j\omega t} + \frac{1}{50-j\omega} e^{-j\omega t}\bigg) \] If we let $k = \frac{1}{50 + j\omega}$, then we can write
\begin{align*}
y_p(t) &= \half \bigg(ke^{j\omega t} + k^* e^{-j\omega t}\bigg)\\
&= \half \bigg( |k|e^{j\angle k}e^{j\omega t} + |k|e^{-j\angle k}e^{-j\omega t}\bigg)\\
&= |k|\half \bigg(e^{j(\omega t+ \angle k)} + e^{-j(\omega t + \angle k)}\bigg)\\
&= |k| \cos(\omega t + \angle k)
\end{align*}

Note: because $|k| \to 0$ as $\omega \to \infty$, $|k| = \frac{1}{\sqrt{50^2 + \omega^2}} \to $ ``low-pass filter''
\end{example}

\begin{example}
Suppose $f(t) = \sin 2t = \frac{e^{j2t} - e^{-j2t}}{2j}$. What is a particular solution?

\begin{proof}
% MISSING y_c
It suffices to find a particular solution for $f(t) = e^{\pm j2t}$ and then apply superposition.

We have
\begin{align*}
f(t) &= e^{j2t}\\
y_p(t) &= ke^{j2t}\\
Dy_p(t) &= wkje^{j2t}\\
D^2y_p(t) &= -4ke^{j2t}
\end{align*}

We substitute this into
\begin{align*}
\bigg(D^2 + \frac{3}{2}D + \quarter\bigg) y = f &= e^{j2t}\\
\bigg(-4 + 3j + \quarter\bigg) ke^{j2t} &= e^{j2t}\\
\bigg(\frac{-15}{4} + 3j\bigg) ke^{j2t} &= e^{j2t}
\end{align*}
so $k$ is the inverse
\begin{align*}
k &= \frac{1}{(\frac{-15}{4}) + 3j} \frac{(\frac{-15}{4}) + 3j}{(\frac{-15}{4}) + 3j}\\
&= \frac{(\frac{-15}{4}) + 3j}{{(\frac{15}{4})}^2 + 3^2}\\
&= \frac{-20 -16j}{123}
\end{align*}
so we have a particular solution \[ y_p(t) = \frac{-20-16j}{123}e^{j2t} \]

We can also write
\begin{align*}
k &= \frac{-20 - 16j}{123}\\
&= \frac{\sqrt{656}}{123} e^{-j2.47}\\
&= \frac{4}{3\sqrt{41}} e^{-j2.47}\\
&= |k|e^{j\angle k}
\end{align*}
so
\begin{align*}
y_p(t) &= \frac{4}{3\sqrt{41}} e^{-j2.47}e^{j2t}\\
&= \frac{4}{3\sqrt{41}} e^{j(2t-2.47)}
\end{align*}

Then if $f(t) = e^{-j2t}$ we get a particular solution
\begin{align*}
y_p(t) &= k^* e^{-j2t}\\
&= |k^*| e^{-j(2t-\angle k^*)}\\
&= |k| e^{-j(2t+\angle k)}
\end{align*}
i.e.\ \[ y_p(t) = \frac{4}{3\sqrt{41} e^{-j(2t - 2.47)}} \]

So if $f(t) = \sin 2t = \frac{e^{j2t} - e^{-j2t}}{2j}$, then a particular solution is \[ y_p(t) = \frac{|k|e^{j(2t+\angle k)} - |k|e^{-j(2t+\angle k)}}{2j} \] i.e.\
\begin{align*}
y_p(t) &= \frac{4}{3\sqrt{41}} \frac{e^{j(2t-2.47)} - e^{-j(2t-2.47)}}{2j}\\
&= \frac{4}{3\sqrt{41}} \sin(2t-2.47)\\
&= |k|\sin (2t + \angle k)
\end{align*}

So the general solution of $Q(D) \tilde y = f$ is \[ \tilde y = c_1 e^{(-\frac{3}{4} + \frac{\sqrt{5}}{4})t} + c_2 e^{(-\frac{3}{4} - \frac{\sqrt{5}}{4})t} + \frac{4}{3\sqrt{41}} \sin(2t-2.47) \]

So then the general solution of $Q(D)y = P(D)f$ is
\begin{align*}
y &= P(D)\tilde y\\
&= (D+1)\tilde y\\
&= \bigg(\quarter + \frac{\sqrt{5}}{4}\bigg) c_1 e^{(-\frac{3}{4} + \frac{\sqrt{5}}{4})t} + \bigg(\quarter - \frac{\sqrt{5}}{4}\bigg) c_2 e^{(-\frac{3}{4} - \frac{\sqrt{5}}{4})t} + \frac{4}{3}\sqrt{\frac{5}{41}} \bigg(\frac{1}{\sqrt{5}} \sin(2t - 2.47) + \frac{2}{\sqrt{5}} \cos(2t-2.47) \bigg)
\end{align*}

So since
\begin{align*}
&= \frac{1}{\sqrt{5}} \sin(2t-2.47) + \frac{2}{\sqrt{5}} \cos(2t-2.47)\\
&= \sin 0.47\sin(2t-2.47) + \cos 0.47\cos(2t-2.47)\\
&= \sin(2t-2.47+0.47)\\
&= \sin(2t-2)
\end{align*}
we have \[ y = c_3 e^{(\frac{-3}{4} + \frac{\sqrt{5}}{4})t} + c_4 e^{(\frac{-3}{4} - \frac{\sqrt{5}}{4})t} + \frac{4}{3} \sqrt{\frac{5}{41}} \sin(2t-2) \]
\end{proof}

If we were given initial conditions (e.g.\ $y(0)$ and $\dot y(0)$), we could evaluate $c_3$ and $c_4$.
\end{example}

\subsection{Laplace Transforms}
A {\bf Laplace transform} simplifies the process of solving a linear differential equation by ``converting'' it to a standard linear algebra problem.

For example, if all of our functions were exponentials $y = e^{st}$, we would have $Dy = sy$.

Since not all of our functions are exponentials, we follow the process of decomposing our functions into \emph{weighted sums} of exponential equations, then applying standard algebraic practices to these sums. By recomposing these equations afterward, we can find solutions to differential equations with a minimal of effort.

More formally, a Laplace transform is denoted by \[ F(s) := \laplace{f(t)} = \int_{-\infty}^\infty f(t) e^{-st} \dd t \] where $s$ is complex-valued. To ensure convergence, suppose that for some real $\alpha$, the integral \[ \int_{-\infty}^{\infty} |f(t)| e^{-\alpha t} \dd t \] converges for some $s$.

The transform can be inverted by means of the following {\bf inversion integral}: \[ f(t) = \frac{1}{2\pi j} \int_{\sigma-j\infty}^{\sigma+j\infty} F(s) e^{st} \dd s \] This is the {\bf contour integral} which is carried out within the area of the complex plain in which the Laplace transform converges (the ``region of convergence'' of $F(s)$).

\begin{example}
Let $f(t) = 1, \forall t$. Then we can suppose $s = \alpha + j\beta$ for some $\alpha, \beta \in \mathbb{R}$. Thus \[ \int_{-\infty}^\infty f(t)e^{-st} \dd t = \int_{-\infty}^\infty e^{-\alpha t} e^{-j\beta t} \dd t \] does not exist for any value of $\alpha$.
\end{example}

\begin{example}
Let $f(t) = u_{-1}(t)$, where $u$ denotes the unit step function $u(t) =
\begin{dcases*}
1 & if $t \geq 0$\\
0 & otherwise
\end{dcases*}$. Then \[ \int_{-\infty}^\infty f(t)e^{-(\alpha + j\beta)t} \dd t = \int_{-\infty}^\infty e^{-\alpha t} e^{-j\beta t} \dd t \] converges for any $\alpha > 0$.

So we have
\begin{align*}
F(s) &= \int_{-\infty}^\infty f(t) e^{-st} \dd t\\
&= \int_0^\infty e^{-st} \dd t\\
&= \bigg(\frac{-1}{s}\bigg)e^{-st} \limit{0}{\infty}\\
&= 0 - \bigg(\frac{-1}{s}\bigg)
\end{align*}
so \[ F(s) = \frac{1}{s} \text{, given }\Re(s) > 0 \]
\end{example}

\begin{example}
Let
\begin{align*}
f(t) &=
\begin{dcases*}
e^{\alpha t} & if $t \geq 0$\\
0 & otherwise
\end{dcases*}\\
&= e^{\alpha t}u_{-1}(t)
\end{align*}

Then we have
\begin{align*}
F(s) &= \int_0^\infty e^{\alpha t} e^{-st} \dd t\\
&= \int_0^\infty e^{-(s-\alpha)t} \dd t
\end{align*}

So \[ F(s) = \frac{1}{s-\alpha} \text{, given }\Re(s-\alpha) > 0 \iff \Re(s) > \Re(\alpha) \]
\end{example}

\begin{example}
Suppose $f(t) = t u_{-1}(t)$. Then
\begin{align*}
F(s) &= \int_0^\infty t e^{-st} \dd t\\
&= -\frac{1}{s}t e^{-st} \limit{0}{\infty} - \int_0^\infty \bigg(\frac{-1}{s}\bigg) e^{-st} \dd t\\
&= 0 - 0 + \frac{1}{s} \int_0^\infty e^{-st} \dd t\\
&= \frac{1}{s^2} \text{, given }\Re(s) > 0
\end{align*}
\end{example}

We'll deal mainly with ``one-sided'' functions $F(t)$ for which $F(t) = 0, \forall t < 0$. We'll therefore often use the ``one-sided'' Laplace transform \[ F(s) = \int_{0^-}^\infty f(t) e^{-st} \dd t = \lim_{m\uparrow 0} \int_m^\infty f(t) e^{-st} \dd t \]

\begin{example}
Suppose $f(t) = \sin(\omega t) e_{-1}(t), \omega \in \mathbb{R}$. Then $\sin{\omega t} = \frac{e^{j\omega t} - e^{-j\omega t}}{2j}$ and so
\begin{align*}
F(s) &= \frac{1}{2j} \bigg[\frac{1}{s-j\omega} - \frac{1}{s+j\omega}\bigg]\\
&= \frac{1}{2j} \bigg[\frac{(s+j\omega) - (s - j\omega)}{(s-j\omega)(s + j\omega)}\bigg]\\
&= \frac{\omega}{s^2 + \omega^2}\text{, given }\Re(s) > 0
\end{align*}
\end{example}

The Laplace transformation obeys the following key properties:
\begin{itemize}
\item Linearity: $\laplace{ \alpha f(t) + \beta g(t)} = \alpha F(s) + \beta G(s)$
\item Time-Scaling: $\laplace{f(ct)} = \frac{1}{c}F(\frac{s}{c})$
\item Exponential Modulation: $\laplace{e^{\alpha t} f(t)} = F(s-\alpha)$
\item Time Shifting: Suppose $F(s) = \laplace{f(t) u_{-1}(t)}$ and let $g(t) = f(t - T)u_{-1}(t - T)$. Then $G(S) = \laplace{g(t)} = e^{-sT} F(s)$
\item Multiplication by $t$: $\laplace{tf(t)} = -\frac{\dd}{\dd s} F(s)$
\item Differentiation/Integration: Suppose that there exists a real $\alpha$ such that $\displaystyle\int_{0^-}^\infty |f(t)| e^{-\alpha t} \dd t$ converges. Suppose also that there exists a function $f^\prime(t)$ such that $f(t) = f(0^-) + \displaystyle\int_{0^-}^\infty f^\prime(\tau) \dd \tau, \forall t \geq 0$ and that $\displaystyle\int_{0^-}^\infty |f^\prime(t)| e^{-\beta t} \dd t$ converges for some real $\beta$. Then both $f$ and $F^\prime$ have one-sided Laplace transforms and we have $F(s) = \frac{1}{s}f(0^-) + \frac{1}{s} \laplace{f^\prime(t)}$ or $\laplace{f^\prime(t)} = sF(s) - f(0^-)$
\end{itemize}

\subsubsection{Solving Differential Equations with Laplace Transforms}
To solve a differential equation, we may simply take the Laplace transform of each side of the equation, solve for the value of $F(s)$, and then perform the inverse Laplace transformation on each side.

\begin{example}
Assume we have $\dot y + y = t + e^t$ with the initial condition $y(0^-) = 1$.

If we take the Laplace transform of each side of this equation, we have \[ sY(s) - y(0^-) + Y(s) = \frac{1}{s^2} + \frac{1}{s-1} \]

This is a much simpler equation to solve, and can in fact be solved using only standard algebra techniques.

We have
\begin{align*}
sY(s) - y(0^-) + Y(s) &= \frac{1}{s^2} + \frac{1}{s-1}\\
sY(s) - 1 + Y(s) &= \frac{1}{s^2} + \frac{1}{s-1}\\
(s+1)Y(S) &= 1 + \frac{1}{s^2} + \frac{1}{s-1}\\
Y(s) &= \frac{1}{s+1} + \frac{1}{s^2(s+1)} + \frac{1}{(s-1)(s+1)}\\
&= \frac{\frac{3}{2}}{s+1} + \frac{1}{s^2} - \frac{1}{s} + \frac{\frac{1}{2}}{s-1}\\
y(t) &= \frac{3}{2}e^{-t} + t - 1 + \half e^t \text{, given }t \geq 0
\end{align*}
\end{example}

\subsubsection{Inversion Integral}
We know that $\mathcal{L}^{-1} \{ F(s) + G(s) \} = f(t) + g(t)$. Thus we have \[ \mathcal{L}^{-1} \{ F(s)G(s) \} = (f * g)(t) = \int_{-\infty}^\infty f(\tau)g(t-\tau) \dd\tau \] which is the {\bf convolution} of $f$ and $g$. Given $u = t - \tau$, we can see that this operation is commutative (e.g.\ the convolution of $f$ and $g$ is equivalent to the convolution of $g$ and $f$).

Consider two ``discrete-time'' functions $f,g : \mathbb{Z} \to \mathbb{C}$ that are nonzero only for a finite number of nonnegative integers. We can think of $f$ and $g$ as representations of two polynomials \[ p(x) = \sum_{n=-\infty}^\infty f(n)x^n \] and \[ q(x) = \sum_{n=-\infty}^\infty g(n) x^n \]

Then the convolution $f * g$ represents the product of $p$ and $q$.

Suppose that, for some real $\alpha, \beta$, the integrals \[ \int_{-\infty}^\infty |f(t)| e^{-\alpha t} \dd t \] and \[ \int_{-\infty}^\infty |g(t)| e^{-\beta t} \dd t \] converge. We'll show that this means that $(f * g)(t)$ has a transform \[ \laplace{(f*g)(t)} = F(s)G(s) \]

Suppose that $\gamma \geq \alpha, \beta$ and consider the product of the two integrals
\begin{align*}
&\int_{-\infty}^\infty |f(t)| e^{-\gamma t} \dd t \int_{-\infty}^\infty |g(\tau)| e^{-\gamma \tau} \dd \tau\\
&= \int_{-\infty}^\infty \int_{-\infty}^\infty |f(t)|e^{-\gamma t} |g(\tau)|e^{-\gamma \tau} \dd t \dd \tau\\
&= \int_{-\infty}^\infty\int_{-\infty}^\infty |f(u-\tau)|e^{-\gamma(u-\tau)} |g(\tau)|e^{-\gamma \tau} \dd u \dd \tau\\
&= \int_{-\infty}^\infty \int_{-\infty}^\infty |f(u-\tau)|e^{-\gamma(u-\tau)} |g(\tau)| e^{-\gamma \tau} \dd \tau \dd u\\
&= \int_{-\infty}^\infty \int_{-\infty}^\infty |f(u-\tau)| |g(\tau)| \dd \tau e^{-\gamma t} \dd u \\
&= \int_{-\infty}^\infty \int_{-\infty}^\infty |f(u-\tau) g(\tau)| \dd \tau e^{-\gamma u} \dd u\\
&\geq \int_{-\infty}^\infty \bigg| \int_{-\infty}^\infty f(u-\tau)g(\tau) \dd \tau \bigg| e^{-\gamma u} \dd u
\end{align*}
so the last integral converges and $(f * g)$ has a Laplace transform.

\subsubsection{Initial-Value Theorem}

\begin{definition}
A function $f(t)$ is piecewise-continuous on an interval $[a,b] (\{t: a \leq t \leq b \})$ if $f$ is continuous and bounded everywhere in this interval, except possibly at some finite number of points.

Moreover, at any discontinuity $t_0$, the limits \[ f(t_0^-) = \lim_{t\uparrow t_0} f(t) \] and \[ f(t_0^+) = \lim_{t\downarrow t_0} f(t) \] must exist.

If $f$ is piecewise-continuous in all such intervals, then $f$ is piecewise-continuous.
\end{definition}

\begin{theorem}[The Initial-Value Theorem]
If $f$ is piecewise-continuous and there exists a real $\alpha$ such that \[ \int_{-\infty}^\infty |f(t)| e^{-\alpha t} \dd t\] converges, then \[ f(0^+) = \lim_{s\to\infty} sF(s) \]

This gives a means of finding $f(0^+)$ from $F(s)$ withouts inverting the transform.
\end{theorem}

This limit shows us that the real part of $s$ trends to $+\infty$.

\begin{proof}
\begin{align*}
\lim_{s\to\infty} sF(s) &= \lim_{s\to\infty} s\int_{0^-}^\infty f(t) e^{-st} \dd t\\
&= \lim_{s\to\infty} s\int_{0^-}^\epsilon f(t) e^{-st} \dd t + \lim_{s\to\infty} s\int_\epsilon^\infty f(t) e^{-st} \dd t\\
&= \lim_{s\to\infty} s\int_{0^-}^\epsilon f(t) e^{-st} \dd t + \lim_{s\to\infty} \int_\epsilon^\infty f(t) se^{-st} \dd t\\
&= \lim_{s\to\infty} s\int_{0^-}^\epsilon f(t) e^{-st} \dd t + \int_\epsilon^\infty \lim_{s\to\infty} f(t) se^{-st} \dd t\\
&= \lim_{s\to\infty} s\int_{0^-}^\epsilon f(t) e^{-st} \dd t
\end{align*}
As $\epsilon$ approaches zero, this approaches
\begin{align*}
&= \lim_{s\to\infty} s\int_{0^-}^\epsilon f(0^+) e^{-st} \dd t\\
&= \lim_{s\to\infty} s f(0^+) \bigg[ \frac{1-e^{-s\epsilon}}{s} \bigg]\\
&= \lim_{s\to\infty} f(0^+) (1-e^{-s\epsilon})\\
&= f(0^+)
\end{align*}
\end{proof}

\subsubsection{Rational Functions}
Many of the Laplace transforms we've seen are rational functions (functions represented as ratios of polynomials). As with rational numbers, we generally cancel out common factors in the numerator and denominator.

Moreover, just as the rational numbers extend the integers to a field, so too do the rational functions extend the polynomials to a field.

The roots of the numerator are called the (finite) zeroes of the function; those of the denominator are the (finite) poles.

A function which has no finite zeroes such as $\frac{1}{s^2}$ is said to have zeroes at infinity, since in the theory of functions of a complex variable they tend to zero as $s\to\infty$. The reciprocal of this type of function is said to have poles at infinity.

If we don't specify finite or infinite, we tend to be refering to finite functions.

A rational function is {\bf proper} if the degree of the numerator is no greater than that of the denominator. If the degree of the numerator is strictly less than that of the denominator, the funtion is strictly proper.

\subsubsection{Final-Value Theorem}
Let $F(s)$ be a proper rational function, all of whose finite poles have real parts that are strictly negative, with the possible expection of a single pole at $s = 0$. Alternatively, $F(s)$ may consist of such a function multiplied by a complex exponential. Then \[ \lim_{s\to\infty} f(t) = \lim_{s\to\infty} sF(s) \] Moreover, if the poles of $F(s)$ do not satisfy the above assumption, then the limit on the left side does not exist.

\begin{proof}
Think of inverting $F(s)$ by means of partial-fractions decomposition. Then
\begin{align*}
F(s) &= \frac{\text{polynomial}}{s{(s-p_1)}^m{(s-p_2)}^k\dots}\\
&= \frac{A}{s} + \frac{B_{11}}{(s-p_1)} + \frac{B_{12}}{{(s-p_1)}^2} + \cdots + \frac{B_{1m}}{{(s-p_1)}^m} + \frac{B_{21}}{(s-p_2)} + \cdots\\
\end{align*}
so \[ f(t) = Au_{-1}(t) + B_{11}e^{p_1 t} u_{-1}t + \dots \] and since $\Re(p_j) < 0, \forall j$ we have \[ \lim_{t\to\infty} f(t) = A \]

By heavyside coverup we have $A = \lim_{s\to 0} sF(s)$, so $\lim_{t\to\infty} f(t) = \lim_{s\to 0} sF(s)$.

If $G(s) = e^{sT}F(s)$ then $g(t) = f(t-T)$ so we have $\lim_{t\to\infty} g(t) = \lim_{t\to\infty} f(t)$ and $\lim_{s\to 0} sG(s) = \lim_{s\to 0} se^{sT} F(s) = \lim_{s\to 0} sF(s)$.

Suppose that $F(s)$ has a pole $p$ with $\Re(p) \geq 0$ and $p \neq 0$. Then the decomposition of $F(s)$ has a term $\frac{B}{s-p}$ whose inverse transform is $Be^{pt} u_{-1}(t)$ and $\lim_{t\to\infty} f(t)$ does not exit.

Finally, if $F(s)$ has $k > 1$ poles at $s = 0$, then the decomposition of $F(s)$ has a term \[ \frac{A_k}{s^k} \] where the inverse transform is proportional to $t^{k-1} u_{-1}(t)$ (where $k > 1$), so $\lim_{t\to\infty} f(t)$ does not exist.
\end{proof}

\section{Signals and Systems}
A {\bf signal} is a real- or complex-valued function of a variable $t$. If $t$ ranges over the reals, the signal is continuous-time (CT). If $t$ ranges over a discrete set (e.g.\ $\mathbb{Z}$), the signal is discrete-time (DT).

A {\bf system} is is a process that generates outputs in response to inputs. Mathemematically, this is a functions from a set $f$ of input signals to a set $y$ of output signals. $y$ is called the {\bf response} to input $f$.

A system is CT, DT, or hybrid if its inputs are CT, DT, or both, respectively. CT systems include most classical physical systems, DT systems are mostly digital signal processors and financial systems. Hybrid systems tend to be analog-digital converters.

A CT system might be modelled by means of a differential equation(s). A DT system is more likely to be modelled by {\bf difference equations}: $\nabla y[k] = y[k] - y[k-1], \nabla^2 y[k] = \nabla y[k] - \nabla y[k-1]$. Even more likely, we could model a CT system as a recurrence equation: $y[k] + a_1y[k-1] + a_2y[k-2] + \cdots + a_n y[k-n] = b_0f[k] + b_1f[k-1] + \cdots + b_m f[k-m]$. In place of initial conditions, values $y[0], y[1], \dots, y[n-1]$ might be given (though these tend to be refered to as initial conditions, regardless).

\subsection{System Properties}
Systems can be either {\bf memory-less} or {\bf dynamic}. In a memory-less system, the value of $y(t)$ depends only on $f(t)$ for any given $t\in\mathbb{R}$. In a dynamic system, the value of $y(t)$ is related to more than simply $f(t)$. For example, $m \ddot y = f \iff y(t) = \frac{1}{m} \int_{-\infty}^t \int_{-\infty}^\tau f(u) \dd u \dd \tau$.

A system $S$ is {\bf causal} if for a given $t$, $y(t) = (Sf)(t)$ depends only on present and past values of $f(t)$, i.e.\ only on $\{ f(\tau) : \tau \leq t \}$. Mathematically, if a system is causal then if $y_1 = Sf_1 \land y_2 = Sf_2$ and $f_1(\tau) = f_2(\tau), \forall \tau < t$ we must have $y_1(\tau) = y_2(\tau), \forall \tau < t$.

A system can also be either {\bf scalar} (one single input and one single output) or {\bf multivariable} (multiple-input and multiple-output). {\bf Linear} systems represent the principle of superposition: $S(c_1f_1 + c_2f_2) = c_1Sf_1 + c_2Sf_2$.

In a {\bf time-invariant} system, we have a response which does not change with time. Mathematically, if $f(t) \to y(t)$ we must have $f(t-T) \to y(t-T)$.

When dealing with linear, time-invariant systems, we can represent therm with linear ODEs with constant coefficients. In this case, it's convention to use the {\bf Dirac delta function} or the {\bf unit impulse function}.

The unit impulse function $\delta(t)$ satisfies \[ \int_{-\infty}^\infty \delta(t) \dd t = 1\] and $\delta(t) = 0, \forall t \neq 0$. This is an example of a generalized function or distribution. In the case of piecewise-continuous signals, this defining property of $\delta(t)$ becomes \[ f(\tau) = \int_{-\infty}^\infty f(t) \delta(t-\tau) \dd t \] This is the ``sifting property'' of $\delta(t)$.

We also have \[ f(t) = \int_{-\infty}^\infty f(\tau) \delta(t-\tau) \dd\tau \] which we can use to show that convolution is central to LTI systems. This also allows us to express $f$ as a superposition of unit impulse functions.

Suppose that \[ \delta(t) \xrightarrow{S} h(t) \] where $h(t)$ is the ``impulse response'' of the LTI system. Then $h(t)$ is the response of the system to the input $\delta(t)$ when all initial conditions are zero. Then we have \[ \delta(t-\tau) \xrightarrow{S} h(t-\tau) \] by time-invariance, \[ f(\tau) \delta(t-\tau) \xrightarrow{S} f(\tau)h(t-\tau) \] by linearity, and \[ f(t) = \int_{-\infty}^\infty f(\tau) \delta(t-\tau) \dd\tau \xrightarrow{S} \int_{-\infty}^\infty f(\tau) h(t-\tau) \dd\tau \] by convolution.

The response in an LTI system, then, is the convolution of its input with its impulse response.

So in the Laplace domain, we see that \[ Y(s) = \laplace{(h * f)(t)} = H(s)F(s) \] Where $H(s) = \laplace{h(t)}$ is called the {\bf transfer function} of the system.

\begin{example}
Suppose $S$ is an LTI system and consider an exponential input $e^{s_0 t}$ where $s_0 \in \mathbb{C}$. Let $e^{s_0 t} \xrightarrow{S} y(t)$. By time-invariance, we have \[ e^{s_0 (t-\tau)} \xrightarrow{S} y(t-\tau) \] But $e^{s_0(t-\tau)} = e^{-s_0 T} e^{s_0t}$, so by linearity \[ e^{s_0(t-\tau)} = e^{-s_0T} e^{s_0t} \xrightarrow{S} e^{-s_0T}y(t) \] So $y(t\tau) = e^{-s_0T} y(t)$. This holds for all $t$ and $T$. So, in particular, $y(0) = e^{-s_0t}y(t)$ for all $T$ which gives us \[ y(t) = y(0) e^{s_0t} \]

Thus we can comment that exponentials are {\bf eigenfunctions} of LTI systems.

By our convolution integral, we have
\begin{align*}
y(t) &= \int_{-\infty}^\infty f(t-\tau)h(t) \dd\tau\\
&= \int_{-\infty}^\infty e^{s_0(t-\tau)}h(t) \dd\tau\\
&= e^{s_0t} \int_{-\infty}^\infty h(\tau) e^{-s_0\tau} \dd\tau\\
&= e^{s_0t} H(s) \bigg|_{s=s_0}
\end{align*}
which is the transfer function evaluated at $s=s_0$.

If $s_0$ is imaginary ($s_0 = j\omega$), our input $e^{s_0 t} = e^{j\omega t} = \cos\omega t + j\sin\omega t$. Then $y(t) = H(j\omega)e^{j\omega t}$. In this case, $H(j\omega)$ is called the ``frequency response'' of $S$.

We also see that $H(j\omega) = |H(j\omega)| e^{j\angle H(j\omega)}$. Thus we have $y(t) = H(j\omega)e^{j\omega t} = |H(j\omega)| e^{j(\omega t + \angle H(j\omega))}$ which is the amplitude (given a phase shift of the angle of the frequency response) multiplied by $|H(j\omega)|$.
\end{example}

In systems and signals, we generally focus on the zero-state response (where we have $Y(s) = aB(s) + cy(0^-)$ is the sum of the ``zero-state'' and ``zero-input'' responses), so we still have the relationship \[ Y(s) = H(s)F(s) \]

In the time domain, we have the standard first order system \[ H(s) = \frac{K}{s\tau - 1} \] where $k, \tau \in \mathbb{R}$ and $k, \tau > 0$. For the impulse response, we know that $y(t) = \mathcal{L}^{-1} \{ H(s)F(s) \}$; if $f(t) = \delta(t)$, then \[ F(s) = \lim_{x\uparrow 0} \int_{-\infty}^\infty \delta(t) e^{-st} \dd t \] which gives us \[ y(t) = \frac{k}{\tau} e^{\frac{-t}{\tau}} e_{-1}(t) \]

\subsection{Stability}
If a system behaves like a standard first order system as the real pole ($\alpha$) approaches zero, we say that the real pole is {\bf dominant}. If the system behaves like a standard second order transfer function as the real pole approaches infinity, then we have a system where the complex poles are dominant.

The dominant pole is responsible for determining the response of the system when we are in the time domain. If no pole is dominant, the response is based on both.

We generally say that poles are dominant if all other poles are at least five times further from the imaginary axis than they are. This is more-or-less a rule of thumb.

A rational transfer function is {\bf stable} if all of its finite poles have real parts which are strictly negative. In this case, we sometimes say that all of the poles ``lie in the open left-half-plane''. All real responses, then, will be decaying exponentially (if all poles are strictly positive, we have an increasing exponential response and if they all lie exactly on the imaginary axis we will have a constant response).

A notion of stability of LTI systems is based on that of {\bf boundedness} of a signal. We say that a signal $x(t)$ is bounded if there exists some real number $M \in \mathbb{R}$ such that $|x(t)| \leq M, \forall t$. A single-input single-output (SISO) system is said to be ``bounded-input bounded-output stable'' (BIBO stable) if, whenever its input signal is bounded, its output signal is also bounded.

A rational function is {\bf proper} (respectively, {\bf strictly proper}) if the degree of its numerator is $\leq$ (respectively, $<$) that of its denominator.

\begin{theorem}
A SISO LTI system with a rational tranfer function is BIBO stable if and only if that transfer function is both stable and proper.
\end{theorem}

\begin{proof}[If]
Suppose the transfer function is stable and proper. Then it can be decomposed into the sum of a constant $a_0$ and various terms of the form $\frac{a_{jk}}{{(s-p_j)}^k}$ where the real part of $p_j$ is strictly negative. Taking the inverse transform, we have $\mathcal{L}^{-1} \{ a_0 \} = a_0 \delta(t)$ and $\mathcal{L}^{-1} \{ \frac{a_{jk}}{{(s-p_j)}^k} \} = a_{jk} e^{p_j t} e^{k-1} u_{-1}(t)$ where $\Re (p_j) < 0$.

Suppose that the inout $x(t)$ is bounded \[ |x(t)| \leq M, \forall t \] for some real $M$. So, the output is a sum of convolutions of the form \[ \int_{0^-}^\infty x(t-\tau) a_0 \delta(\tau) \dd\tau = a_0 x(t) \]

But $|a_0 x(t)| \leq a_0 M$ so this term is bounded and $\infty_{0^-}^\infty x(t-\tau) a_{jk} \tau^{k-1} e^{p_j \tau} \dd\tau \leq M \int_{0^-}^\infty a_{jk} \tau^{k-1} e^{p_j \tau} \dd\tau$ which will converge.

So $y(t)$ is a finite sum of bounded functions and therefore is itself bounded.
\end{proof}

\begin{proof}[Only If]
Suppose the transfer function is unstable. Then it has some pole $p$ whose real part is not strictly negative.

Suppose $p=0$. Then a partial-fractions decomposition of $H(s)$ contains a term proportional to $\frac{1}{s}$; i.e.\ the system is not BIBO stable.

If we suppose that $p = j\omega, \omega \neq 0$, we can find that the system must also not be BIBO stable. If we have $y(t) = \mathcal{L}^{-1} \{ \frac{1}{s-j\omega}\frac{1}{s-j\omega} \}$ which gives $y(t) = e^{j\omega t} tu_{-1}(t)$ which is unbounded.

Finally, if $\Re(p) > 0$, then the step response contains a term
\begin{align*}
y(t) &= \mathcal{L}^{-1} \{ \frac{1}{{(s-p)}^k} \frac{1}{s} \}\\
&= \int_{0^-}^t e^{pt} t^{k-1} u_{-1}(t) \dd t
\end{align*}
which is unbounded because $\Re(p) > 0$.

Therefore, the stability of the transfer function is necessary.

Suppose the transfer function is improper. Then \[ H(s) = q(s) + \frac{n(s)}{d(s)} \] where $q(s)$ is a polynomial of degree $\geq 1$ and $\frac{n(s)}{d(s)}$ is strictly proper.

So a unit step gives rise to a Dirac delta function in the expression for the output.
\end{proof}

\subsection{Frequency Response}
We represent the frequency response as a Bode plot: $|H(j\omega)|_{\text{dB}}$ vs $\log_{10} \omega$ (``magnitude'') and $\angle H(j\omega)$ vs $\log_{10} \omega$ (``phase''). Note that a Bel is the $\log_{10}$ of the ratio of the average power of two signals. This basically boils down to seeing that if \[ H(s) = G_1(s)G_2(s)\dots G_m(s) \] then we have \[ \angle H(j\omega) = \angle G_1(j\omega) \angle G_2(s) \dots \angle G_m(j\omega) \]

Because $|H(j\omega)|$ is the ratio of amplitudes of sinusoidal signals and $|H(j\omega)|^2$ is, therefore, the ratio of powers of signals, we see that \[ |H(j\omega)|_{dB} = 10 \log_{10} |H(j\omega)|^2 = 20 \log_{10} |H(j\omega)| \]

We know, then, that \[ |H(j\omega)|_{dB} = |G_1(j\omega)|_{dB} + |G_2(j\omega)|_{dB} + \cdots + |G_m(j\omega)|_{dB} \]

In a standard first order system, we have $H = \frac{K}{s\tau + 1}$. Then we have
\begin{align*}
H(j\omega) &= \frac{K}{1 + j\omega\tau}\\
|H(j\omega)| &= \frac{K}{\sqrt{1 + {(\omega\tau)}^2}}\\
20\log_{10}|H(j\omega)| &= 20\log_{10} K - 20\log_{10} \sqrt{1+{(\omega\tau)}^2}\\
20\log_{10}|H(j\omega)| &\approx 20\log_{10} K
\end{align*}

\section{Fourier Series}
The effect of sinusoidal signals $e^{j\omega t}$ on LTI systems is easy to analyze: we simply multiply our input by the frequency response $H(j\omega)$. We can extend this analysis via the {\bf Fourier Series} to other periodic signals.

We represent a periodic signal as an infinite sum of sinusoids.

A function $f(t)$ is {\bf periodic} with period $T$ if for all $t$, $f(t+T) = f(t)$. If this in the case, clearly $f$ is periodic with period $af, a \in \mathbb{Z}$. We continue to refer to a periodic function by its minimal period, i.e.\ $T$, not $3T$.

Given such a function, we'll try to represent it as a superposition of $e^{j\frac{2\pi}{T}nt}, n \in \mathbb{Z}$. A function $f$ is {\bf piecewise-smooth} on the interval $[-\half T, \half T]$ if there exists a finite set of points $-\half T = t_0 < t_1 < t_2 < \cdots < t_k = \half T$ such that $f$ and $\dot f$ are bounded and continuous on each interval.

The set of all piecewise-smooth functions on $[-\half T, \half T]$ forms a vector space (under multiplication by complex scalars). We can make the vector space into an {\bf inner product space} by defining \[ <f,g> = \frac{1}{T} \int_{-\half T}^{\half T} f(t) g^*(t) \dd t \]

The complex sinusoids $e^{j \frac{2\pi}{T} nt} (n \in \mathbb{Z})$ form an orthonormal basic of a subspace of the piecewise-smooth functions of $[-\half T, \half T]$.

Given $f = e^{j\frac{2\pi}{T}mt}$ and $g = e^{j\frac{2\pi}{T}{nt}}$, $<f,g> =
\begin{dcases*}
1 & if $m = n$\\
0 & otherwise
\end{dcases*}$.

We'll approximate a piecewise-smooth function $f(t)$ by its projection onto this subspace \[ f(t) \approx \sum_{n=-\infty}^\infty c_n e^{j\frac{2\pi}{T} nt} \] where $c_n = <f(t), e^{j\frac{2\pi}{T}nt}>$.

For every partial sum, we can define an error function \[ e_N(t) = f(t) - \sum_{n=-N}^N c_n e^{j\frac{2\pi}{T}nt} \] We define the ``size'' of the error as $<e_n, e_n>$ which is the ``average power'' of the error signal.

\subsection{Inner Products}
An {\bf inner product space} is a vector space $V$ equipped with an ``inner product'' operation $<f, g>$. An {\bf inner product operation} is an operation satisfying:
\begin{itemize}
\item $<au + bv, w> = a<u,w> + b<v,w>$ for all $u, v, w \in V, a, b \in \mathbb{R}$
\item $<u,w> = <w,u>^*$
\item $<u,u> \geq 0$
\item $<u,u> = 0$ if and only if $u=0$
\end{itemize}
Inner products also allow the definition of a normal vector $||v|| = \sqrt{<v,v>}$.

We defined the inner product of $<f,g>$ as \[ <f,g> = \frac{1}{T} \int_{-\half T}^{\half T} f(t)g^*(t) \dd t \] this does not satisfy the fourth requirement, but we can fix that by declaring out piecewise-smooth functions as ``equivalence classes''---that is, $f \equiv g \iff <f-g, f-g> = 0 \iff ||f-g|| = 0$.

\begin{theorem}[Projection Theorem]
Let $V$ be an inner product space and $W$ be a finite-dimensional subspace of $V$. Then for an arbitrary vector $v\in V$, there exists a vector $\hat w$, the unique best approximatino to $v$ in $W$.

That is \[ || v - \hat w|| \leq ||v - w|| \] for all $w\in W$.

This best approximatant $\hat w$ is the ``projection'' of $v$ onto $w$ and is characterized by the fact that the approximation error $e = v - \hat w$.
\end{theorem}

This best approximant $\hat w$ can be characterized in terms of an orthonormal basis for $W$ as \[ \hat w = \sum_{i=1}^M <v, u_i> u_i \] where the $u_i$ over $i = 1\dots M$ form an orthonormal basis for $W$.

\subsection{Dirichlet Convergence Theorem}
For any piecewise-smooth function $f(t)$ on $[-\half T, \half T]$, \[ \sum_{h=-\infty}^\infty c_n e^{j\frac{2\pi}{T} nt} =
\begin{dcases*}
f(t) & if $f$ is continuous at $t$\\
\frac{f(t^-) + f(t^+)}{2} & otherwise
\end{dcases*}\]

By the Dirilecht Theorem, we can derive \[ {||f||}^2 = \sum_{n=-\infty}^\infty |c_n|^2 \] which is ``Perseval's Theorem''. This theorem decomposes the power of $f$ into frequency components.

\section{Overview}
\subsection{Before Midterm}

\subsection{After Midterm}
\begin{itemize}
\item convolution property of Laplace transforms ($\mathcal{L} \{ f * g(t) \} = F(s)G(s)$)
\item initial- and final-value theorems (respectively, useful for transient and steady-state system performance analysis. From the transform of $f(s)$, these give us $f(0^+)$ and $f(\infty)$)
\item Laplace transforms allow us to solve initial-value problems involving linear ODEs with constant coefficients (in one step)
\item Laplace transforms are also fundamental to ``signals and systems''
\item defined key properties of signals and systems (causality, linearity, time-invariance). Given a linear time-invariant system, the output is the convolution of the input and the impulse response (the response to a unit impulse with zero initial conditions)
\item response to exponentials (an LTI system with transfer function $H(s)$ has $H(s) = \mathcal{L} \{ \text{impulse response} \}$). If the input is $e^{s_0 t}$ (a two-sided exponential, i.e.\ not multiplied by the unit step), the output is $H(s_0) e^{s_0 t} = |H(s_0)| e^{s_0 t + j\angle H(s_0)}$. If the input is one-sided and $H(s)$ is stable, then $H(s_0) e^{s_0 t}$ is the steady-state response
\begin{itemize}
\item particularly, if our input is $e^{j\omega t}$, our output is $H(j\omega)e^{j\omega t} = |H(j\omega) e^{j(\omega t + \angle H(j\omega))}$ where $H(j\omega)$ is the frequency response. Again, given a one-sided input and stable $H(s)$, this is the steady-state response
\end{itemize}
\item We looked at the responses of LTI systems in both time and frequency domains
\begin{itemize}
\item time domain: $y(t) = \mathcal{L}^{-1} \{ H(s) U(s) \}$
\item frequency domain: frequency response and its representation on the Bode plot
\item specifically, we looked at first- and second-order examples and saw that we could generalize the analysis to more complex systems (e.g.\ factor complex system s into simpler ones and plot their combination)
\end{itemize}
\item extended frequency-response results by showing how to approximate piecewise-smooth periodic signals using the Fourier series (``projecting'' such a function $f(t)$ onto a subspace spanned by an orthonormal basis consisting of complex sinusoids $\{ e^{j\frac{2\pi}{T}nt}, n\in\mathbb{Z}$. This gives us $f(t) = \displaystyle\sum_{n=-\infty}^\infty c_n e^{j\frac{2\pi}{T}nt}$, where $c_n = \frac{1}{T}\displaystyle\int_{-\half T}^{\half T} f(t) e^{-j\frac{2\pi}{T}nt} \dd t$)
\end{itemize}

\end{document}
