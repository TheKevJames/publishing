\documentclass[12pt]{article}
\usepackage{amsmath,amssymb,bookmark,mathtools,parskip,custom}
\usepackage[margin=.8in]{geometry}
\allowdisplaybreaks
\hypersetup{colorlinks,
    citecolor=black,
    filecolor=black,
    linkcolor=black,
    urlcolor=black
}
\setcounter{secnumdepth}{5}

\begin{document}

\title{CS 341 --- Algorithms}
\author{Kevin James}
\date{\vspace{-2ex}Winter 2015}
\maketitle\HRule

\tableofcontents
\newpage

\section{Solving Recurrences}
\begin{example}
Determine the runtime of mergesort.
\[ T(n) =
\begin{dcases*}
2T\bigl(\frac{n}{2}\bigl) + \Theta(n) & if $n > 1$ \\
\Theta(1) & if $n = 1$
\end{dcases*}\]
which we can solve to find \[ T(n) \in \Theta(n\log n) \]
\end{example}

\begin{definition}
Stoogesort:
\begin{verbatim}
def stoogesort(A[1..n]):
  if n <= 1:
    return
  if A[1] > A[2]:
    swap(A[1], A[2])
  stoogesort(A[1..\frac{2n}{3}])
  stoogesort(A[\frac{n}{3}+1..n])
  stoogesort(A[1..\frac{2n}{3}])
\end{verbatim}
\end{definition}

\begin{example}
Let $T(n)$ be the runtime of stoogesort on $n$ elements. Then \[ T(n) =
\begin{dcases*}
3T\bigl(\frac{2n}{3}\bigl) + \Theta(1) & if $n > 1$ \\
\Theta(1) & if $n = 1$
\end{dcases*}\]
\end{example}

There are three methods for solving this recurrence:
\begin{enumerate}
\item Recursion Tree method: Expand for $k$ iterations to get a tree of terms, set $k$ to reach the base case, sum across rows, then over all levels.
\item Master method: Look up the answer. The Master Theorem: Let $T(n) = aT(\frac{n}{b}) + f(n)$ if $n > n+0$ and $c$ if $n = n_0$. Set $d = \log_b a$i and pick a small constant $\varepsilon > 0$. Case 1: $f(n) = O(n^{d-\varepsilon}) \implies T(n) = \Theta(n^d)$. Case 2: $f(n) = \Theta(n^d) \implies T(n) = \Theta(n^d \log n)$. Case 3: $\lim_{n\to\infty} f(n) / n^{d+\varepsilon} = \infty \implies T(n) = \Theta(f(n))$.
\item Substitution method: Guess the form of the solution $T(n) \leq x)$, then verify your guess by induction and fill in any constants.
\end{enumerate}

\begin{example}
\begin{align*}
T(n) &= 2T\bigg(\frac{n}{2}\bigg) + n^2, 7 \\
T(n) &\leq cn^2 \\
n = 1: T(n) &= 7 \leq cn^2, c \geq 7 \\
T\bigg(\frac{n}{2}\bigg) &\leq c\bigg(\frac{n}{2}\bigg)^2 \\
T(n) &= 2T\bigg(\frac{n}{2}\bigg) + n^2 \\
     &\leq 2c\bigg(\frac{n}{2}\bigg) + n^2 \\
     &= \frac{2cn^2}{4} + n^2 \\
     &= \bigg(\frac{c}{2} + 1\bigg)n^2 \\
     &\leq cn^2, \frac{c}{2} + 1 \leq c, c \geq 2
\end{align*}
We can then pick $c = 7$ and solve as \[ T(n) \leq 7n^2 \implies T(n) \in O(n^2) \] We can also note that $T(n) \geq n^2$, so $T(n) \in \Theta(n^2)$.
\end{example}

\begin{example}
\begin{align*}
T(n) &= 3T\bigg(\bigg\lfloor \frac{n}{2} \bigg\rfloor\bigg) + 4T\bigg(\bigg\lfloor \frac{n}{4} \bigg\rfloor\bigg) + 1, 1 \\
T(n) &\leq cn^2 \\
n = 1: T(n) &= 1 \leq cn^2, c \geq 1 \\
T\bigg(\bigg\lfloor \frac{n}{2} \bigg\rfloor\bigg) &\leq c\bigg\lfloor \frac{n}{2} \bigg\rfloor^2 \\
T(n) &= 3T\bigg(\bigg\lfloor \frac{n}{2} \bigg\rfloor\bigg) + 4T\bigg(\bigg\lfloor \frac{n}{4} \bigg\rfloor\bigg) + 1 \\
     &\leq 3c \bigg\lfloor \frac{n}{2} \bigg\rfloor^2 + 4c\bigg\lfloor \frac{n}{4} \bigg\rfloor + 1 \\
     &\leq 3c \bigg(\frac{n}{2}\bigg)^2 + 4c \bigg(\frac{n}{2}\bigg)^2 + 1 \\
     &= \bigg(\frac{3}{4} + \frac{4}{16}\bigg)cn^2 + 1 \\
     &= cn^2 + 1
\end{align*}
but since we could not get rid of the constant, we try
\begin{align*}
T(n) &\leq cn^2 - c_0 \\
n = 1: T(n) &= 1 \leq cn^2 - c_0, c \geq 1 + c_0 \\
T\bigg(\bigg\lfloor \frac{n}{2} \bigg\rfloor\bigg) &\leq c\bigg\lfloor \frac{n}{2} \bigg\rfloor^2 - c_0 \\
T(n) &= 3T\bigg(\bigg\lfloor \frac{n}{2} \bigg\rfloor\bigg) + 4T\bigg(\bigg\lfloor \frac{n}{4} \bigg\rfloor\bigg) + 1 \\
     &\leq 3\bigg(c \bigg\lfloor \frac{n}{2} \bigg\rfloor^2 - c_0\bigg) + 4\bigg(c\bigg\lfloor \frac{n}{4} \bigg\rfloor - c_0\bigg) + 1 \\
     &\leq 3c \bigg(\frac{n}{2}\bigg)^2 - 3c_0 + 4c \bigg(\frac{n}{2}\bigg)^2 - 4c_0 + 1 \\
     &= \bigg(\frac{3}{4} + \frac{4}{16}\bigg)cn^2 - 7c_0 + 1 \\
     &= cn^2 - c_0, -7c_0 + 1 \leq -c_0, c_0 \geq \frac{1}{6}
\end{align*}
Pick $c_0 = \frac{1}{6}, c = \frac{7}{6}$ and we see that \[ T(n) \leq \frac{7}{6}n^2 - \frac{1}{6} \implies T(n) \in O(n^2) \]
\end{example}

\section{Algorithm Design Techniques}
\subsection{Divide and Conquer}
Divide your problem into subproblems of the same type, then use recursion to solve each problem and combine the results.

\begin{example}
{\bf (Maxima)}: Given a set $P$ of $n$ points in 2D, we say point $p$ dominates point $q$ if and only if $p$ has both a greater $x$ and $y$ value than $q$. We say point $q$ is maximal if and only if $q \in P$ and no point in $P$ dominates $q$. Find all maximal points.

{\bf Solutions}:
\begin{itemize}
\item Brute Force: for each $q \in P$, check if no points dominat $q$. Total time: $\Theta(n^2)$
\item Divide and Conquer: divide into two subarrays of size $\frac{n}{2}$.
\item Divide by Medians: instead of dividing by size, divide by a median vertical line.
\end{itemize}

\begin{verbatim}
maxima(sorted[p_1..p_n]):
  1. if n == 1: return p_1
  2. [q_1..q_l] = maxima([p_1..p_{n/2}])
  3. [s_1..s_m] = maxima([p_{n/2}..p_n])
  4. i = 1
  5. while q_i.y > s_1.y
  6.   i += 1
  7. return [q_1..q_l, s_1..s_m]
\end{verbatim}
which is an $O(n\log n)$ algorithm.
\end{example}

\begin{example}
{\bf (Closest Pair)}: Given set $P$ of $n$ points in 2D, find a pair $p, q \in P$ such that these points have a smaller distance between them than any other pair of points in $P$, ie. $d(p,q) = \sqrt{(p.x - q.x)^2 + (p.y - q.y)^2}$.

{\bf Solutions}:
\begin{itemize}
\item Brute Force Algorithm: $\Theta(n)$
\item Shamos' Algortithm: $\Theta(n\log^2 n)$. Note that if we refine the Shamos algorithm by pre-sorting $P$ also by y-coordinate at the beginning, we find that our complexity becomes $O(n\log n)$.
\end{itemize}

\begin{verbatim}
def bruteForce(P):
  distance = infinity
  for each p in P:
    for each q in P (q neq P):
      distance = min(distance, d(p,q))
  return distance

def shamos(P):
  if n <= 10:
    return bruteForce(P)
  x_m = median x_coordinate
  P_L = { p in P : p.x < x_m }
  P_R = { p in P : p.x > x_m }
  d_L = shamos(P_L)
  d_R = shamos(P_R)
  d = min(d_L, d_R)
  <p_1..p_m> = sorted_by(points in { p in P : x_m - d <= p.x <= x_m + d }, y)
  for i = 1 to m do
    j = i + 1
    while p_j.y <= p_i.y + d:
      d = min(d, d(p_i, p_j))
      j++
  return d
\end{verbatim}
\end{example}

\subsubsection{Multiplying Large Numbers}
\begin{example}
Given two $n$-bit numbers $A = a_{n-1}, a_{n-2}, \dots a_0$, $b = b_{n-1}, b_{n-2}, \dots b_0$ in binary, compute $AB = c_{n-1}, c_{n-2}, \dots c_0$.

{\bf Solution}: The ``Elementary School'' algorithm for this involves doing
\begin{verbatim}
    1011
  x 1101
--------
    1011
  1011
 1011
--------
10001111
\end{verbatim}
which is $O(n)$ shifts and $O(n)$ additions, thus making it an $O(n^2)$ algorithm.

The ``Karatsuba and Ofman'' algorithm involves a different process:
\begin{verbatim}
A' = [a_{n-1}..a_{n/2}]
A" = [a_{n/2 - 1}..a_0]
A = [A'..A"]

B' = [b_{n-1}..b_{n/2}]
B" = [b_{n/2 - 1}..b_0]
B = [B'..B"]

AB = A'B'2^n + (A'B" + A"B')2^(n/2) + A"B"
\end{verbatim}
which gives us a complexity of $O(n^2)$.
\end{example}

% MISSING

\section{Selection}
\begin{example}
Given $n$ numbers $a_1..a_n$ and $k$, find the $k^{th}$ smallest number.

{\bf Solutions}:
\begin{itemize}
\item Method 0: sort and look up index $k$. $O(n\log n)$ time.
\item Method 1 (selection sort variation): find minimum, remove, and repeat $k$ times. $O(nk)$ time.
\item Method 2 (heapsort variation): find minimum, remove, build heap, and repeat $k$ times. $O(n + k\log n$) time.
\item Method 3 (quicksort variation [``quickselect''])
\end{itemize}

\begin{verbatim}
def quickselect([a_1..a_n], k):
  if n = 1:
    return a_1
  pick pivot x
  L = { a_i : a_i <= x }, l = sizeof(L)
  R = { a_i : a_i > x }
  if k <= l:
    return quickselect(L, k)
  else:
    return quickselect(R, k-l)
\end{verbatim}

Analysis of quickselect: let $T(n)$ be the runtime on $n$ elements. Then \[ T(n) \leq T\bigl(\max\{l, n-l\}\bigl) + O(n) \] In the ideal case, $l = \frac{n}{2}$. In this case, we have (using the master theorem) $T(n) \in O(n)$. In the worst case, $l = 1$ or $l = n - 1$, which gives us $T(n) \in O(n^2)$. The challenge of quickselect, then, is determining a pivot selection strategy which brings us closer to the ideal case.
\end{example}

\subsubsection{Pivot Selection}
Selecting a pivot at random gives us an expected case near $O(n)$.

Randomized Analysis: $l$ is equally likely to be $1, 2,\dots n$. Thus $\text{prob}\bigl[\frac{n}{4} < l \leq \frac{3n}{4}\bigl] = \half$. Expected number of iterations to get this case is $2$. So then \[ T(n) \leq T\bigg(\frac{3n}{4}\bigg) + 2 * O(n) \] which is an (expected) $O(n)$ runtime.

Unfortunately, this algorithm gives us only an expected runtime. We do not have a worst case upper bound specifying $O(n)$.

The Blum, Floyd, Prattt, Rivest, and Tarjan algorithm tries to deterministically find a good pivot. They find an approximate median by recursion: this is the ``median of medians of five'' algorithm.
\begin{verbatim}
def quickselect([a_1..a_n], k):
  if n = 1:
    return a_1

  // pivot selection in n time
  split a_1..a_n into groups G_1..G_{n/5} of five elements each
  for i = 1 to n/5:
    x_i = median of G_i
  x = quickselect([x_i..x_{n/5}], n/10)
  // end pivot selection

  L = { a_i : a_i <= x }, l = sizeof(L)
  R = { a_i : a_i > x }
  if k <= l:
    return quickselect(L, k)
  else:
    return quickselect(R, k-l)
\end{verbatim}

\begin{lemma}
\[ \frac{3n}{10} \lesssim l \lesssim \frac{7n}{10} \]
\end{lemma}

\begin{proof}
How many groups $G_i$ with $x_i \leq x$? $\frac{n}{10}$. For each such group, how many elements are less than or equal to $x_i$? $3$. Thus we have the number of elements less than or equal to $x$ is greater than or equal to $\frac{3n}{10}$. Similarly, the number of elements greater than $x$ is greater than or equivalent to $\frac{3n}{10}$.
\end{proof}

By this lemma, we see that the final recursion lines in this algorithm are in $\leq T(\frac{7n}{10})$. We have the overall complexity of this algorithm as \[ T(n) \leq T(\frac{n}{5}) + T(\frac{7n}{10}) + O(n) \] which solves to $T(n) \in O(n)$.

\subsection{Greedy Algorithms}
{\bf Greedy Algorithms} are usually used for optimization problems, where we find the solution by minimizing or maximizing some objective subject to some restraints. These algorithms incrememntally build the solution by choosing what seems best at each step.
\begin{itemize}
\item Advantages: simple, fast
\item Disadvantages: local maxima over global maxima
\end{itemize}

% EXAMPLES MISSING

\subsubsection{Interval Coloring}
\begin{example}
Given $n$ intervals $[a_1, b_1], \dots, [a_n, b_n]$, assign each interval a color such that no two intervals of the same color intersect while minimizing the number of colors used.

Greedy Algorithm Idea: scan intervals from left to right and use a new color only when needed.

{\bf Algorithm}:
\begin{verbatim}
k = 0
for each interval [a_1, b_1] in increasing order of a:
    pick any color c in {1, ..., k} not within any intervals intersecting [a_i, b_1]
    if c exists:
        assign [a_i, b_i] color c
    else:
        assign [a_i, b_i] color k + 1
        k++
\end{verbatim}
\end{example}

\subsection{Pairing Algorithms}
% PAIRING ALGORITHMS

\subsubsection{Stable Marriage}
% ALGORITHM

The {\bf stable marriage} algorithm is stable if for all matched pairs $(C_1, E_1), (C_2, E_2)$ we don't have both $C_1$ prefers $E_2$ over $E_1$ and $C_2$ prefers $C_1$ over $C_2$.

\subsubsection{Gale and Shapley's Algorithm}
\begin{verbatim}
while exist unmatched employer E
    pick C = next best candidate in E's preference list
    // E makes offer to C
    if C is unmatched or C prefers E over C's current employer E_0
        unmatch(C, E_0)
        match(C, E)
\end{verbatim}
We can see that this algorithm is bounded by $O(n^2)$.

\section{Dynamic Programming}
{\bf Dynamic Programming} can be used for optimization problems. We simply define a class of subproblems, derive a recursive formula to explore all choices at each step, and then evaluate that formula bottom-up using a table. This is successful when the number of total subproblems is not overly large.

% COIN CHANGING

% LOTS OF STUFF

\section{Graph Problems}
% INTRO

\subsection{Shortest Paths}
Given a directed graph with weights along each path, find the cheapest route between two points.

Special Cases:
\begin{itemize}
\item Unweighted (or, all are equal to 1): simply find the path which uses the fewest amount of edges (with BFS). Analysis: $O(n + m)$.
\item DAG (directional acyclic graph): use dynamic programming. Subproblem: $\forall v \in V, \delta[v] =$ the weight of the shortest path from $s$ to $v$. Our answer, then, is $\delta[t]$. Base Case: $\delta[s] = 0$. Recursive Formula: $\delta[v] = \min\bigl(\delta[u] + w(u, v), \{u : (u,v) \in E\}\bigl)$. Note the evaluation order for the recursive formula is in the same order as a topological sort over the edges. Analysis: $O\bigl(n + \displaystyle\sum_{v\in V} \text{in-deg}(v)\bigl) = O(n + m)$.
\end{itemize}

General Case: we could consider taking the smallest weighted edge at every step, but this would not necessarily give us the correct answer. Instead, we use {\bf Dijkstra's Algorithm}:
\begin{verbatim}
// assume positive weights
// solve single-source (all destination) problem
// compute d[v]: shortest path weight from s to v for all v in V.
S= {s}, d[s] = 0
while S != V:
    pick edge (u, v) with u in S, v in V - S minimizing d[u] + w(u, v)
    insert v into S
    d[v] = d[u] + w(u, v)
\end{verbatim}

The detailed implementation is:
\begin{verbatim}
// TODO: Prim's, with changes to lines 7-8
if y in emptyset and key[v] + w(u, v) < key[y]:
  key[y] = key[v] + w(u, v), pi[y] = v
\end{verbatim}

Analysis: no data structure: $O(n^2)$, heap: $O\bigl((n+m)\log n\bigl)$, Fibonacci heap: $O(m + n\log n)$.

\subsubsection{Correctness Proof}
Claim: if $(u, v)$ is the edge from $S$ to $V \setminus S$ minimizing $\delta[u] + w(u, v)$, then $\delta[v] = \delta[u] + w(u, v)$.

Proof: There is a path from $s$ to $v$ with weight $\delta[u] + w(u, v)$. Thus $\delta[v] \leq \delta[u] + w(u, v)$. Now, take any arbitrary path $p$ from $s$ to $v$. $p$ must contain an edge $(u^\prime, v^\prime)$ with $u^\prime \in S$ and $v^\prime \in V \setminus S$. $w(p) \geq \delta[u^\prime] + w(u^\prime, v^\prime) + 0 \implies \delta[v] \geq \delta[u] + w(u, v)$. Thus, we have shown $\delta[v] = \delta[u] + w(u, v)$.

Note that this is only correct if our graph contains no negative weights.

\subsubsection{All-Pairs Shortest Paths}
Given a weighted directed or undirected graph $G = (V, E)$, find the shortest path between all pairs of vertices.

\begin{itemize}
\item Method 0: Run Dijkstra's algorithm on each vertex, ie. run it $n$ times. This method assumes there are no negative weights. Complexity: $O\bigl(n(m + n\log n)\bigl)$.
\item Method 1:
\end{itemize}

% TODO: missing

\subsection{P vs NP}
\subsubsection{P-Problems}
We define $P$ as the set of all decision problems solveable in ``polytime'' (worst-case polynomial time).

For example:
\begin{example}
We define TSP with inputs: graph $G = (V, E)$, $w : E \implies \mathbb{N}$ and output: cycle that visits every vertex exactly once minimizing total weight.

We define TSP-DECIS with inputs: graph $G = (V, E)$, $w : E \implies \mathbb{N}$, number $w$ and output: yes if and only if there exists a cycle that visits every vertex once and has total weight $\leq w$.

If TSP is solveable in polytime, then PSP-DECIS $\in P$.
\begin{proof}
Obviously.
\end{proof}

If TSP-DECIS $\in P$, then we can find the optimal weight in polytime.
\begin{proof}
By binary search. We need $O(\log\sum_{e\in E} w(e))$ calls to the decision algorithm, which is polynomial in the number of input bits.
\end{proof}

If TSP-DECIS $\in P$, then we can find the optimal cycle in polytime.
\begin{proof}
We first find the optimal weight $w^*$ in polytime. For each $e\in E$, we find the optimal weight $w^\prime$ for $G^\prime(V, E - \{e\})$ in polytime and if $w^\prime = w^*$ we delete $e$ from $E$. We do this recursively and return all edges remaining in $E$. We need $O(m)$ calls to this algorithm.
\end{proof}
\end{example}

\subsubsection{NP-Problems}
Our theory concentrates on one common type of ``search'' problem: ``does there exist\dots ?''. Thus we define $NP$ as the set of all decision problems that can be expressed in the form: input: Object $X$, output: yes if and only if there exists object $y$ such that property $R(X, y)$ holds where object $y$ has polysize (``certificate'') and property $R$ can be checked in polytime (``verification algorithm'').

For example:
\begin{example}
We define 3-COLORING with input: graph $G = (V, E)$ and output: yes if  and only if there exists a coloring of vertices such that no two adjacent vertices have the same color.

We define 3-COLORING $\in NP$ with certificate coloring: $O(n)$ size and verification no two adjacent vertices have the same color: $O(m)$ time.
\end{example}

\begin{example}
TSP-DECIS has inputs: graph $G = (V, E)$, $w: E\implies \mathbb{N}$, number $w$ and output: yes if and only if there exists a cycle which vists every vertex once and has total weight less than or equal to $w$.

We define TSP-DECIS $\in NP$ with certificate cycle $C$: $O(n)$ size and verification $C$ vists every vertex exactly once and has total weight $\leq w$: $O(1 + n)$ time.
\end{example}

Note that $NP$ does not stand for ``non-polynomial'' but for ``non-deterministic polynomial''. Many real-life problems are $NP$ (answers are easy to verify but hard to find).

\begin{theorem}
These problems are subsets of each other: $P \subseteq NP \subseteq EXPTIME$.
\end{theorem}

\begin{proof}
$P \subseteq NP$: ignore certificate. $NP \subseteq EXPTIME$: brute force, try all certificates of polysize $p(n)$: $2^{p(n)}$.
\end{proof}

Though we have not yet proved it, we assume $P \neq NP$.

\subsubsection{NPC-Problems}
$NPC$ problems ar $NP$-complete.

We must define the ``hardest'' problem in $NP$\dots but what does ``$L_1$ is easier than $L_2$ mean''? One way to define this is as $L_1$ ``reduces'' to $L_2$ (selection reduces to sorting).

\begin{definition}
Given decision problems $L_1, L_2$, a reduction from $L_1$ to $L_2$ is an algorithm $f$ such that the output of $L_1$ on $x$ is ``yes'' if and only if the output of $L_2$ on $f(x)$ is ``yes''. If such a reduction exists, $L_1 \leq_p L_2$.
\end{definition}

Then, to solve $L_1$: $x \to f(x) \to L_2[f(x)]$.

\begin{lemma}
If $L_1 \leq_p L_2$ and $L_2 \in P$, then $L_1 \in P$.
\end{lemma}

\begin{lemma}
If $L1 \leq_p L_2$ and $L_2 \leq_p L_3$, then $L_1 \leq_p L_3$.
\end{lemma}

\begin{definition}
$L$ is $NP$-complete if  and only if $L \in NP$ and $\forall L^\prime \in NP, L^\prime \leq_p L$.
\end{definition}

\begin{lemma}
Let $L$ be an $NP$-complete problem. Then $L \not\in P \iff P \neq NP$.
\end{lemma}

\begin{proof}
Suppose $L \not\in P$. Then $L\in NP$ so $P \neq NP$. Suppose $L \in P$. Then $\forall L^\prime \in NP \implies L^\prime \leq_p L$ sp $L^\prime \in P$ and thus $NP = P$.
\end{proof}

\end{document}
