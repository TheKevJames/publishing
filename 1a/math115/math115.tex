\documentclass[12pt]{article}
\usepackage{amsmath,amssymb,parskip,custom}
\usepackage[margin=1in]{geometry}

\begin{document}

\title{MATH 115 - Linear Algebra for Engineers}
\author{Kevin Carruthers}
\date{\vspace{-2ex}Fall 2012}
\maketitle\HRule

\section*{Vectors}
{\bf Vectors} have a magnitude and a direction, are denoted by vector arrows, and are said to be in $\R{n}$

\subsection*{Two Main Operations}
You can perform two main operations with vectors: {\bf vector addition}, which is the algebraic tail to tip addition of vectors, and {\bf scalar multiplication}, which is $t\vec{v}$ where $t\ \epsilon\ \mathbb{R}$ and $\vec{v}\ \epsilon\ \R{n}$

\subsection*{Linear Combinations}
The linear combinatio of a set of vectors $(\vec{v_0},\dots \vec{v_n}$ is any vector which can be obtained from these vectors through vector addition and scalar multiplication. It has the form $a_0\vec{v_0} + \dots + a_n\vec{v_n}$ where $a_0,\dots a_n\ \epsilon\ \mathbb{R}$

Example: for $\vec{a} = \vectwo{1}{0}$ and $\vec{b} = \vectwo{0}{1}$
\[ 3\vec{a} - 2\vec{b} = \vectwo{3}{-2} \]

Any vector in $\R{2}$ is a linear combination of this set.

\subsection*{Dot Product}
In $\R{n}$, the dot product of $\vec{u} = \vecthree{a_0}{\dots}{a_n}$ and $\vec{v} = \vecthree{b_0}{\dots}{b_n}$ is a scalar defined to be \[ \vec{a}\circ\vec{b} = a_0b_0 + \dots + a_nb_n \] thus if $\vec{a} = \vectwo{3}{5}$ and $\vec{v} = \vectwo{2}{6}$ \[ \vec{u}\circ\vec{v} = 3*2 + 5*6 = 36 \]

\subsubsection*{Properties}
\begin{itemize}
\item $\vec{u}\circ\vec{v} = \vec{v}\circ\vec{u}$
\item $t\vec{u}\circ\vec{v} = t(\vec{u}\circ\vec{v})$, for any scalar $t$
\item $\vec{u}\circ(\vec{v} + \vec{w}) = \vec{u}\circ\vec{v} + \vec{u}\circ\vec{w}$
\end{itemize}

\subsection*{Magnitude}
The {\bf magnitude} of a vector is its length.

Example: for $\vec{v} = \begin{bmatrix}1\\0\\1\\1\\2\end{bmatrix}$ \[ \magv{v} = \sqrt{3^2+4^2} = 5 \]

More generally \[ \magv{v} = \sqrt{\vec{v}\circ\vec{v}} \] and \[ \magv{v}^2 = \vec{v}\circ\vec{v} \]

\subsubsection*{Unit Vector}
The {\bf unit vector} is a vector of length one. Given $\vec{v}$, the unit vector with the same direction is \[ \frac{\vec{v}}{\magv{v}} \]

This is called {\bf normalization}.

\subsubsection*{Distance Between Points}
For $P$ and $Q$, the {\bf distance} between them is $\magv{PQ}$

\subsubsection*{Angle Between Two Vectors}
For $\vec{u}$ and $\vec{v}$, the {\bf angle} between them is $\theta$.

Deriving from the {\bf cosine law} $c^2 = a^2 + b^2 - 2ab\cos\theta$, we get
\begin{align*}
\mags{\vec{u}-\vec{v}}^2 &= \magv{u}^2 + \magv{v}^2 - 2\magv{u}\magv{v}\cos\theta\\
&= (\vec{u}-\vec{v})\circ(\vec{u}-\vec{v})\\
&= \vec{u}\circ\vec{u} - \vec{u}\circ\vec{v} - \vec{v}\circ\vec{u} + \vec{v}\circ\vec{v}\\
&= \magv{u}^2 - 2\vec{u}\circ\vec{v} + \magv{v}^2\\
-2\magv{u}\magv{v}\cos\theta &= -2\vec{u}\circ\vec{v}\\
\cos{\theta}&=\frac{\vec{u}\circ\vec{v}}{\magv{u}\magv{v}}\\
\theta&=\cos^{-1}\bigg({\frac{\vec{u}\circ\vec{v}}{\magv{u}\magv{v}}}\bigg)
\end{align*}

\textit{Definition:} two vectors $\vec{u}$ and $\vec{v}$ are orthogonal if $\vec{u}\circ\vec{v} = 0$

\subsection*{Lines}
In $\R{2}, y = mx + b$, but in $\R{n}$ we must two vectors to represent a line. The equation for a line in $\R{n}$ follows the same forms as in $\R{2}$, but uses any vector on the line as its intercept, and any vector parallel to the line as its slope.

Example: \[ \vec{x} = (3,1,4,1) + t(2,0,3,2), t\isin\mathbb{R} \]

\subsubsection*{Parametric Form}
In {\bf parametric form}, we solve for the values of each variable in the resulting vector. For the above equation we have
\[ \vec{x} = 
  \begin{cases}
   x_0 = 3 + 2t\\
   x_1 = 1\\
   x_2 = 4 + 3t\\
   x_3 = 1 + 2t
  \end{cases}
\]
where $t\isin\mathbb{R}$

\subsection*{Planes (In $\R{3}$ only)}
Every plane in $\R{3}$ has a {\bf normal vector} orthogonal to the plane. $\vec{n}$ must be orthogonal to $\vec{PX}$, where $P$ and $X$ are both points on the plane, so \[ \vec{n}\circ\vec{PX}=0 \]

For any line in $\R{3}$ of the form $ax + by + cz = k$, $\vec{n} = \vecthree{a}{b}{c}$

\section*{Projections}
$\proj{v}{u}$ is the {\bf projection} of $\vec{v}$ onto $\vec{u}$
\begin{enumerate}
\item $\proj{v}{u}$ is a scalar multiple of $\vec{u}$
\item $\proj{v}{u}$ and $\vec{v} - \proj{v}{u}$ are orthogonal
\end{enumerate}

\begin{align*}
\proj{v}{u} &= \frac{\vec{u}\circ\vec{v}}{\magv{u}^2}\vec{u}\\
\perpp{v}{u} &= \vec{v} - proj_{\vec{u}}(\vec{v})\\
\perpp{v}{u}\circ\proj{v}{u} &= 0
\end{align*}

\subsection*{Shortest Distance}
The tip of $\proj{v}{u}$ is the {\bf closest point} to $\vec{v}$

Example: The shortest distance between the point $P$ and the line $\vec{QR}$ is \[ \mags{\perpp{QP}{QR}} \]

\subsection*{Projection onto a Plane}
For a point $P$ and a plane containing point $Q$, the {\bf projection} onto that plane is \[ \perpp{QP}{n} \]

\section*{Vector Algebra}
The definition of $\R{n}$ is $\{(a_0,\dots a_n)\suchthat a_0,\dots a_n\isin\mathbb{R}\}$

There are 10 properties of $\R{n}$. For any $\vec{x}, \vec{y}, \vec{z}\isin\R{n}$. $s,t\isin\mathbb{R}$
\begin{enumerate}
\item $\vec{x} + \vec{y}\isin\R{n} \leftarrow$ {\bf closure of addition}
\item $\vec{x} + \vec{y} = \vec{y} + \vec{x} \leftarrow$ {\bf commutivity}
\item $(\vec{x} + \vec{y}) + \vec{z} = \vec{x} + (\vec{y} + \vec{z}) \leftarrow {\bf associativity}$
\item There exists $\vec{0}\isin\R{n}$ such that $\vec{0} + \vec{x} = \vec{x}$
\item For every $\vec{x}\isin\R{n}$, there exists $-\vec{x}\isin\R{n}$ such that $\vec{x} + (-\vec{x} = \vec{0})$
\item $t\vec{x}\isin\R{n} \leftarrow$ {\bf closure under scalar multiplication}
\item $s(t\vec{x}) = (st)\vec{x}$
\item $(s+t)\vec{x} = s\vec{x} + t\vec{x}$
\item $t(\vec{x} + \vec{y}) = t\vec{x} + t\vec{y}$
\item $1*\vec{x} = \vec{x}$
\end{enumerate}
Any algebraic structure that satisfies these 10 properties will "act" like $\R{n}$ and be called {\bf vector spaces}. We can apply things in $\R{n}$ to these structures.

\subsection*{Subspaces}
Subspaces are vector spaces within $\R{n}$. To check if a subset of $\R{n}$ is a {\bf subspace}, we only need to check properties 1, 4, and 6; the other properties are inherited.

\textit{Definition:} A non-empty subset $S$ of $\R{n}$ is a subspace of $\R{n}$ if for all $\vec{x},\vec{y}\isin S, t\isin\mathbb{R}$ \[ \vec{x}+\vec{y}\isin S \text{ and } t\vec{x}\isin S \] Property 4 follows from the fact that the set is non-empty.

Generally, any line containing $\vec{0}$ is a subspace and any line that does not contain $\vec{0}$ is not a subspace (not closed under scalar multiplication). Generally, any plane through the origin is a subspace.

\subsection*{Spanning Sets}
Recall: every element of $\R{n}$ in a plane can be written as a linear combination of $\{\vectwo{1}{0},\vectwo{0}{1}\}$

\textit{Definition:} the {\bf span} of $\{\vec{v_0},\dots \vec{v_k}\}$ is denoted $\spanv{\vec{v_0},\dots \vec{v_k}}$ is the set of all linear combinations of $\vec{v_0},\dots \vec{v_k}$ \[\spanv{\vec{v_0},\dots \vec{v_k}} = \{a_0\vec{v_0},\dots a_k\vec{v_k}\suchthat a_0,\dots a_k\isin\mathbb{R}\} \]

$\spanv{\vectwo{1}{2}}$ is the line through $0$ with distance $\vectwo{1}{2}$. If $\vec{x}\isin\spanv{\vectwo{1}{2}}$, then $\vec{x} = a_0\vectwo{1}{2}, a_0\isin\mathbb{R}$

\theorem{Theorem: if $\vec{v_0},\dots \vec{v_k}\isin\R{n}$, then $\spanv{\vec{v_0},\dots \vec{v_k}}$ is a subspace of $\R{n}$}

In $\R{2}$ we have $\R{2} = \spanv{\vectwo{1}{0},\vectwo{0}{1},\vectwo{1}{1}} = \spanv{\vectwo{1}{0},\vectwo{0}{1}}$ because $\vectwo{1}{1}$ is a linear combination of the other two vectors, and is thus redundant.

\theorem{Theorem: if $\vec{v_0},\dots \vec{v_k}\isin\R{n}$ and $\vec{v_k}$ is a linear combination of $\vec{v_0},\dots \vec{v_{k-1}}$, then $\spanv{\vec{v_0},\dots \vec{v_k}} = \spanv{\vec{v_0},\dots \vec{v_{k-1}}}$}

\subsection*{Linear Independence}
\textit{Definition:} The set $\{\vec{v_0},\dots \vec{v_k}\}$ is {\bf linearly dependent} if there exists $a_0,\dots a_k$ not all zero such that $a_0\vec{v_0} + \dots + a_k\vec{v_k} = \vec{0}$. Otherwise (ie, if $a_0,\dots a_k$ are all zero) the set is {\bf linearly independent}.

\subsubsection*{Basis}
\textit{Definition:} A {\bf basis} $B$ of a subspace $S$ is a linearly independent subset of $S$ such that $S = \spanv{B}$

\section*{Systems of Linear Equations}
A linear equation is in the form \[ a_0x_0 + \dots + a_nx_n = b \] where \[ a_1,\dots a_n, b\isin\mathbb{R} \text{ and } x_0,\dots x_n\isin\R{n} \]

A solution is a vector $\vec{x}$ that satisfies the equation.

\subsection*{Matrices}
A {\bf matrix} $A$ is $m$ x $n$ if it has $m$ rows and $n$ columns. The entry at row $i$ and column $j$ is the $ij$-th entry, denoted by $(A)_{ij}$ or $a_{ij}$. When $m=n$, it is a square matrix. The diagonal entries of a square matrix are $a_{11}, a_{22}, \dots $

\subsection*{Reduced Row-Echelon Form (RREF)}
A matrix is in {\bf row-echelon form (REF)} if
\begin{enumerate}
\item The firt nonzero entry in each row is a {\bf leading one} (1)
\item Rows of zeros are at the bottom of the matrix
\item Each leading one is to the right of the leading ones in all the rows above it
\end{enumerate}

In addition, if each column containing a leading one has 0's everywhere else, then it is in {\bf RREF}.

To get the complete solution from RREF
\begin{enumerate}
\item Assign each non-leading zero a parameter
\item Write solutions to leading variables in terms of these parameters
\end{enumerate}

\subsection*{Rank}
The {\bf rank} of a matrix is the number of leading ones in its RREF.

\subsection*{Consistency}
A system of linear equations is {\bf consistent} if it has at least one solution. Otherwise it is inconsistent.

\subsection*{RREF Facts}
\begin{enumerate}
\item Every augmented matrix can be reduced to RREF by {\bf elementary row operations} (vector addition, scalar multiplication, and row-swapping)
\item The RREF of an augmented matrix is always unique
\item rank$\{A\} \leq$ min$\{m,n\}$ where $m$ and $n$ are the number of rows and columns in the matrix
\item A system is inconsistent if and only if there is a row of the form $\begin{bmatrix}0&\dots&0&1\end{bmatrix}$ in its RREF
\item If a system is consistent, then the number of parameters in the solution set is the number of variables (columns) $n$ - rank$\{A\}$
\item A consistent system has a unique solution when $n =$ rank$\{A\}$ (i.e. no parameters)
\item A consistent system has infinitely many solutions when $n >$ rank$\{A\}$
\end{enumerate}

\subsection*{Homogeneous System}
A system is {\bf homogeneous} if all the constants in the right-most column are 0. By taking each var $= 0$, we get a solution to any homogeneous system. Any homogenous system has either only the trivial solution or infinitely many solutions

To guarantee consistency of a set spanning $\R{n}$, the span must contain at least $n$ vectors, where $n$ is the number of rows.

\subsection*{Bases}
In $\R{n}$ any basis has size $n$.

\theorem{Theorem: if $S$ is a subspace, then any basis for $S$ has the same size.}

\textit{Definition:} the dimension of a subspace is the size of its basis. \[ \text{dim}\{S\} = k \]
Example: the dimension of any plane is 2.

Note: $\{\vec{0}\}$ is not a basis for $\{\vec{0}\}$ because it is not linearly independant.

\subsection*{Special Matrices}
\begin{itemize}
\item In the {\bf Zero Matrix} every entry is 0 and the matrix is denoted by 0 or $0_{mn}$
\item A matrix is {\bf diagonal} if every off-diagonal entry is 0
\item In the {\bf Identity Matrix} every diagonal entry is 1 (the identity matrix is diagonal)
\item An {\bf Upper Triangular Matrix} is a square matrix where anything below the diagonal is 0
\item A {\bf Lower Triangular Matrix} is a square matrix where anything above the diagonal is 0
\end{itemize}

\subsection*{Two basic operations}
\begin{enumerate}
\item {\bf Matrix addition} $\rightarrow$ if $A$ and $B$ are two matrices of the same size, then we define $A + B$ by $(A + B)_{ij} = (A)_{ij} + (B)_{ij}$
\item {\bf Scalar Multiplication} $\rightarrow$ if $A$ is a matrix and $t$ is a scalar, then we define $tA$ as $(tA)_{ij} = t(A)_{ij}$
\end{enumerate}

The set of all $m$ x $n$ matrics together with these two operations satisfy the 10 properties of $\R{n}$ (eg. closure of addition, closure of scalar multiplication, commutativity, \dots ), so this is a vector space.

\subsection*{Transpositions}
The {\bf transpose} of a $m$ x $n$ matrix is an $n$ x $m$ matrix where $(A^T)_{ij} = (A)_{ji}$

\subsubsection*{Properties of a Transpose}
\begin{enumerate}
\item $(A^T)^T = A$
\item $(kA)^T = k(A^T)$
\item $(A+B)^T = A^T + B^T$
\end{enumerate}

\textit{Definition:} A square matrix $A$ is symmetric if $A^T = A$. It is skew-symmetric if $A^T = -A$

\subsection*{Matrix Multiplications}
Definition: let $A$ be an $a$ by $b$ matrix and $B$ be a $b$ by $c$ matrix. Then $AB$ is an $a$ by $c$ matrix defined by \[ (AB)_{ij} = A_i * B_j \]

Non-comutativity: $AB \neq BA$. Order of multiplication matters.

Cencellation law: If $AC = BC$, $A \neq B$

\subsubsection*{Properties of Matrix Multiplication}
\begin{enumerate}
\item If $A$ is $m$ x $n$, then $IA = AI = A$
\item $A(BC) = (AB)C = ABC$
\item $A(B+C) = AB + AC$
\item $(B+C)A = BA + CA$
\item $k(AB) = (kA)B = A(kB)$
\item $(AB)^T = B^TA^T$
\end{enumerate}

\subsection*{Linear Mappings}
A function is a {\bf linear mapping} if for any $\vec{x}, \vec{y}\isin\R{m}$, $f(\vec{x})\isin\R{n}$ and $t\isin\mathbb{R}$
\begin{enumerate}
\item $f(\vec{x} + \vec{y}) = f(\vec{x}) + f(\vec{y})$
\item $f(t\vec{x}) = tf(\vec{x})$
\end{enumerate}

A function $f(\vec{x}) = [f]\vec{x}$ is a {\bf matrix mapping} if for any $\vec{x}\isin\R{m}$, $[f]_{m\text{x}n}$, and $f(\vec{x})\isin\R{m}$. All matrix mappings are linear mappings and vice-versa.

\subsubsection*{Solving Mappings}
\begin{align*}
f(\vec{x}) &= f(x_0\vec{e_0} + \dots + x_n\vec{e_n})\\
&= x_0f(\vec{e_0}) + \dots + x_nf(\vec{e_n})\\
&= (f(\vec{e_0}),\dots f(\vec{e_n}))(x_0,\dots x_n)
\end{align*}

The standard matrix of $f$ is $[f] = \begin{bmatrix}f(\vec{e_0})&\dots&f(\vec{e_n})\end{bmatrix}$

If $f: \R{n} \to \R{m}$ is linear, $f$ is an $m$ x $n$ matrix.

\subsubsection*{Linear Combinations}
\textit{Definition:} let $f, g: \R{n} \to \R{m}$ be linear. We define $f + g : \R{n} \to \R{m}$ by \[ (f + g)(\vec{x}) = f(\vec{x}) + g(\vec{x}) \]

\textit{Definition:} if $t\isin\mathbb{R}$, we define $tf: \R{n} \to \R{m}$ by \[ (tf)(\vec{x}) = tf(\vec{x}) \]

Note: the set of all linear mappings $f: \mathbb{R}^n \rightarrow \mathbb{R}^m$ forms a vector space.

\subsubsection*{Compositions of Mappings}
for $f(x) = \cos x, g(x) = 1 - x^2$ \[ g \circ f(x) = 1 - \cos^2 x \] The {\bf codomain} of $f$ is the same as the domain of $g$

\textit{Definition:} let $f : \R{n} \to \R{m}$, and $g : \R{m} \to \R{p}$ be linear. We define $g\circ f : \R{n} \to \R{p}$ by $g\circ f(\vec{x}) = g(f(\vec{x}))$. If $f,g$ are linear, $g\circ f$ is as well
\begin{align*}
g\circ f(\vec{x}) &= g(f(\vec{x}))\\
&= g([f]\vec{x})\\
&= [g][f]\vec{x}\\
[g\circ f] &= [g][f]
\end{align*}

Suppose we want to take $\vec{x}$, rotate it $\frac{\pi}{4}$ around $x_2$, then project it onto $x_1 + x_2 + x_3 = 0$. For $f=$ rotation and $g=$ projection, this is \[ g\circ f(\vec{x}) = [g][f]\vec{x} \]

\subsubsection*{Geometric Mappings}
\begin{itemize}
\item{Rotation in $\R{2}$}
\begin{itemize}
\item{Let $R_\theta$ : $\R{2} \to \R{2}$ be the mapping that rotates the input vector an angle $\theta$ counter-clockwise around the origin. This is a linear mapping}
\end{itemize}
\item{Rotation in $\R{3}$}
\begin{itemize}
\item{Rotate an angle of $\theta$ around the $x_3$-axis in the $x_1,x_2$ direction. This is a linear mapping}
\end{itemize}
\item{Reflection over a line in $\R{2}$}
\begin{itemize}
\item{Let $f$ : $\R{2} \to \R{2}$. $f(\vec{x}) = \vec{x} - 2\ \perpp{x}{d} = \proj{x}{d} - \perpp{x}{d}$. This is a linear mapping}
\end{itemize}
\end{itemize}

\section*{Bases (again)}
For any subspace $S$ of dimension $k$, any set of $k$ linearly independant vectors in $S$ form a {\bf basis} (ie. span $S$).

Example: in the plane $P: x + y + x = 0$, which has a dimension of 2, two random linear independant vectors are $\vecthree{3}{1}{-4},\vecthree{0}{5}{-5}$. These vectors form a basis .

\textit{Proof:} if $\{\vec{v_1},\dots \vec{v_k}\}\isin S$ is linearly independant, but not a basis, there must be some $\vec{w}\isin S$ not in $\spanv{\vec{v_1},\dots \vec{v_k}}$. Then $\spanv{\vec{v_1},\dots \vec{v_k}, \vec{w}}$ is linearly dependant because it contains more then $k$ vectors. However, since $\vec{w}$ is not in the $\spanv{\vec{v_1},\dots \vec{v_k}}$, we know that $\spanv{\vec{v_1},\dots \vec{v_k}, \vec{w}}$ is linearly independant. As this is a contradiction, $\vec{w}$ must not exist, and $\{\vec{v_1},\dots \vec{v_k}\}\isin S$ must be a basis.

\section*{Inverses}
\textit{Definition:} let $A$ be a square matrix and $B$ be the {\bf inverse} of $A$ such that $BA = I$ and $AB = I$.where $AB = I$, $BA = I$, and $B$ is a unique matrix.

\subsection*{Finding an Inverse}
To find $A^{-1}$, we solve $\begin{bmatrix}\ \ A&\bigg|&I\ \ \end{bmatrix}$. For any invertible matrix
\begin{align*}
A\vec{x} &= \vec{b}\\
\vec{x} &= A^{-1}\vec{b}
\end{align*}

Note: This means that if $A$ can be row-reduced to I, then $A^{-1}$ exists.

\subsection*{Properties of an Invertible Matrix}
$A$ is invertible if and ony if $A$ has rank $n$ (where $A$ is $n$x$n$). If $A$ is invertible, then $A^{-1}$ is invertible and $(A^{-1})^{-1} = A$

If $A, B$ are $n$x$n$ invertible matrices and $t\isin\mathbb{R}$, then
\begin{itemize}
\item $(tA)^{-1} = \frac{1}{t}A^{-1}$
\item $(AB)^{-1} = B^{-1}A^{-1}$
\item $(A^T)^{-1} = (A^{-1})^T$
\end{itemize}

\section*{Determinants}
\textit{Definition}: The {\bf determinant} of an $n$x$n$ matrix $A$ is \[ \text{det}A = \sum_{i=1}^n \sum_{j=1}^n C_{ij}a_{ij} \] where \[ C_{ij} = (-1)^{i+j}\text{det}A(i,j) \]

For any $2$x$2$ matrix $\begin{bmatrix}a&b\\c&d\end{bmatrix}$, det$A = ad-bc$

Example: find the determinant of $\begin{bmatrix}0&-1&3\\3&1&5\\-3&2&0\end{bmatrix}$

\begin{align*}
\text{det}A &= C_{11}a_{11} + C_{12}a_{12} + C_{13}a_{13}\\
&= 0(1*0-5*2) + -(-1)[3*0-5*(-3)] + 3[3*2-1*(-3)]\\
&= 33
\end{align*}

The determinant of $A$ is denoted $\abs{A}$

\subsection*{Upper Triangular Matrices}
The determinant of an {\bf upper triangular matrix} is equal to the product of the numbers along its diagonal.

\subsection*{Row or Column Multiplication}
For any matrix $A$ which is equal to the matrix $B$, except for one row or column which has been {\bf multiplied by $k$} \[ \text{det}A = k\text{det}B \]

\subsection*{Row Swapping}
For any matrix $A$ which is equal to the matrix $B$, except for one row or column which has been {\bf switched with another} \[ \text{det}A = -\text{det}B \]

\subsection*{Row Addition}
{\bf Row addition} does not change the determinant of a matrix.

Example: $\begin{vmatrix}1&0\\0&1\end{vmatrix} = \begin{vmatrix}1&0\\121&1\end{vmatrix}$

\subsection*{Invertibility}
\theorem{Theorem: $A$ is invertible if and only if det$A \neq 0$}

\subsection*{Determinant of a Product}
\[ \text{det}(AB) = \text{det}(A)\text{det}(B) \]

\subsection*{Traces}
The {\bf trace} of a square matrix $A$ is the sum of its diagonal values.

\section*{Eigenvalues and Eigenvectors}
For a square matrix $A$, a non-zero vector $\vec{v}$ is an {\bf eigenvector} of $A$ if \[ A\vec{v} =\lambda\vec{v} \] for some constant $\lambda$, which is called an {\bf eigenvalue} of $A$. Eigenvectors are the non-zero solutions to \[ (A - \lambda I)\vec{v} = 0 \]

Any solutions of \[ \text{det}(A - \lambda I) = 0 \] are eigenvalues of $A$.

\subsection*{Eigenspaces}
\textit{Definition:} the {\bf eigenspace} of an eigenvalue $\lambda$ for $A$ is the set of all eigenvectors of $A$ with eigenvalue $\lambda$. This is a subspace since it is the solution set to the homogeneous system $(A - \lambda I)\vec{v} = 0$

\subsection*{Characteristic Polynomials}
\textit{Definition:} the {\bf characteristic polynomial} of $A$ is \[ \text{det}(A - \lambda I) \]

For any $n$x$n$ matrix $A$, the degree of its characteristic polynomial is $n$. A polynomial with degree $n$ has exactly $n$ roots (including complex and repeating roots). Thus, any $n$x$n$ matrix has $n$ eigenvalues. A multiplicity of a root is the number of times it appears as a root of its polynomial.

If $r$ is a root, $\lambda - r$ is a factor of the characteristic polynomial.

\subsection*{Diagonalization}
To find the powers of matrices, it can be helpful to {\bf diagonalize} them, since the square of a diagonal matrix is equal to that matrix with each of its entries squared, et cetera.

\textit{Definition:} A square matrix $A$ is diagonalizable if there exists an invertible matrix $P$ and a diagonal matrix $D$ such that \[ P^{-1}AP = D \]

Suppose $A$ is diagonalizable, and $P^{-1}AP = D$. Then \[ A^n = PD^nP^{-1} \]

\subsubsection*{Finding $P$ and $D$}
The matrix $D$ is a diagonal vector containing the eigenvalues of $A$. The matrix $P$ is a matrix composed of the eigenvectors of $A$, in the same order as their corresponding eigenvalues appear in $D$.

Example: for a 2x2 matrix $A$ with eigenvalues 3 and 1, and corresponding eigenvectors $\vectwo{2}{-1}$ and $\vectwo{-3}{1}$
\begin{align*}
D &= \begin{bmatrix}3&0\\0&1\end{bmatrix}\\
P &= \begin{bmatrix}2&-3\\-1&1\end{bmatrix}\\
P^{-1} &= \begin{bmatrix}-1&-3\\-1&-2\end{bmatrix}
\end{align*}

\section*{Recurrence}
% Didn't understand Martin Pei's notes, so made my own.
For the Fibonnaci sequence $1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89, 144, 233$ defined as \[ f(n+1) = f(n) + f(n-1) \] we can see that  \[ \begin{bmatrix}1&1\\1&0\end{bmatrix}\vectwo{f(n)}{f(n-1)} = \vectwo{f(n+1)}{f(n)} \] as this {\bf recurrence} is a linear relationship.

Through diagonalization, we can find \[ \begin{bmatrix}1&1\\1&0\end{bmatrix} = \begin{bmatrix}\frac{1+\sqrt{5}}{2}&\frac{1-\sqrt{5}}{2}\\1&1\end{bmatrix}\begin{bmatrix}\frac{1+\sqrt{5}}{2}&0\\0&\frac{1-\sqrt{5}}{2}\end{bmatrix}\begin{bmatrix}\frac{1}{\sqrt{5}}&-\frac{1-\sqrt{5}}{2\sqrt{5}}\\-\frac{1}{\sqrt{5}}&\frac{1+\sqrt{5}}{2\sqrt{5}}\end{bmatrix} \]

Reintroducing vectors and solving for the $n$th power gives us
\begin{align*}
\vectwo{f(n+1)}{f(n)} &= \begin{bmatrix}1&1\\1&0\end{bmatrix}^n\vectwo{f(1)}{f(0)}\\
&= \begin{bmatrix}\frac{1+\sqrt{5}}{2}&\frac{1-\sqrt{5}}{2}\\1&1\end{bmatrix}\begin{bmatrix}\frac{1+\sqrt{5}}{2}&0\\0&\frac{1-\sqrt{5}}{2}\end{bmatrix}^n\begin{bmatrix}\frac{1}{\sqrt{5}}&-\frac{1-\sqrt{5}}{2\sqrt{5}}\\-\frac{1}{\sqrt{5}}&\frac{1+\sqrt{5}}{2\sqrt{5}}\end{bmatrix}\vectwo{f(1)}{f(0)}
\end{align*}

Thus $f(n)$ is the second component of that equation \[ f(n) = \frac{1}{\sqrt{5}}\bigg[\bigg(\frac{1+\sqrt{5}}{2}\bigg)^n - \bigg(\frac{1-\sqrt{5}}{2}\bigg)^n\bigg] \]

\section*{Similar Matrices}
\textit{Definition:} Two $n$x$n$ matrices $A$ and $B$ are {\bf similar} if there exists an invertible $P$ such that $P^{-1}AP = B,$ or $A \sim B$. $A$ is diagonalizable if it is similar to a diagonalizable matrix.

\theorem{Theorem: If $A \sim B$, then they have the same determinant, characteristic polynomial, eigenvalues, rank, and trace.}

\theorem{Theorem: If $A$ is diagonalizable, det$A$ is the product of the eigenvalues of $A$ and tr$A$ is the sum of the eigenvalues of $A$.}

\section*{Orthogonality}
\textit{Definition:} A set $\{\vec{v_1},\dots \vec{v_k}\}$ is {\bf orthogonal} if $v_i \neq v_j$ whenever $i \neq j$

Example: the standard basis is an orthogonal set, ie. $\bigg\{\vecthree{1}{0}{0}, \vecthree{0}{1}{0}, \vecthree{0}{0}{1}\bigg\}$

If $\{\vec{v_1},\dots \vec{v_k}\}$ is an orthogonal set that does not include $\vec{0}$, then it is linearly independant.

Suppose $\{\vec{v_1},\dots \vec{v_k}\}$ is an orthogonal basis for a subspace $S$. Let $\vec{x}\isin S$. So $\vec{x} = a_1\vec{v_1} + \dots + a_k\vec{v_k}$ for some $a_i$'s. For some $i$, take the dot product of $\vec{v_i}$ on both sides to get \[ x\vec{v_i} = a_1\vec{v_1}\circ\vec{v_i} + \dots + a_i\vec{v_i}\circ\vec{v_i} + \dots + a_k\vec{v_k}\circ\vec{v_i} \] \[ a_i = \frac{x\vec{v_i}}{\magv{v_i}^2} \] \[ \vec{x} = \frac{xa_1}{\magv{a_1}^2}\vec{v_1} + \dots + \frac{xa_k}{\magv{a_k}^2}\vec{v_k} \]

Example: for $\bigg\{\vectwo{1}{1},\vectwo{1}{-1}\bigg\}$

\[ \vectwo{3}{4} = \frac{7}{2}\vectwo{1}{1} + \frac{-1}{2}\vectwo{1}{-1} \]

\textit{Definition:} A set $\{\vec{v_1}, \dots \vec{v_k}\}$ is {\bf orthonormal} if it is orthogonal and $\magv{v_i} = 1$ for each $i$ (ie. any orthogonal set where each vector has been normalized)

% This theorem may be incorrect or incomplete
\theorem{Theorem: If $\{\vec{v_1},\dots \vec{v_k}\}$ is an orthonormal basis for a subspace $S$ and $\vec{k}\isin S$, then \[ \vec{x} = (x\vec{v_1})\vec{v_1} + \dots + (x\vec{v_k})\vec{v_k} = kx \]}

\subsection*{Orthogonal Matrices}
A matrix is {\bf orthogonal} if $A^{-1} = A^T$, or $A^TA = I$. Along it's diagonal, $\magv{v_i} = 1$, for each $i$.

If $A$ is orthogonal, so is $A^T$. Also, each of its rows and columns are {\bf orthonormal}.


\subsection*{Orthogonal Complements}
\textit{Definiton:} let $S$ be the subspace of $\R{n}$. The {\bf orthogonal complement} of $S$, denoted $S^\perp$, is the set of all vectors orthogonal to every vector in $S$. \[ S^\perp = \bigg\{\frac{1}{x}\isin\R{n}\suchthat \vec{x}\vec{v} = 0 \text{ for all } \vec{v}\isin S\bigg\} \]

Example: if $P$ is a plane through the origin \[ P^\perp = \spanv{\vecthree{0}{0}{0}} \]

$S\cap S^\perp = \{\vec{0}\}$, ie the only vector in both $S$ and $S^\perp$ is $\vec{0}$

Example: let $S = \spanv{\begin{bmatrix}1\\1\\-1\\-1\\1\end{bmatrix},\begin{bmatrix}1\\-1\\1\\-1\\-1\end{bmatrix}}$, find $S^\perp$. For $S \neq S^\perp$ \[ \begin{bmatrix}1&1&-1&-1&1\\1&-1&1&-1&-1\end{bmatrix} = \begin{bmatrix}1&0&0&-1&0\\0&1&-1&0&1\end{bmatrix} \] \[ S^\perp = \spanv{s\begin{bmatrix}0\\1\\1\\0\\0\end{bmatrix} + t\begin{bmatrix}1\\0\\0\\1\\0\end{bmatrix} + u\begin{bmatrix}0\\-1\\0\\0\\-1\end{bmatrix}} \]

\subsubsection*{Properties of $S^\perp$, $S\isin\R{n}$}
\begin{enumerate}
\item $S^\perp$ is a subspace (solution set of a homogeneous system)
\item $(S^\perp)^\perp = S$
\item dim$(S^\perp) = n - $dim$(S)$, where dimension is equal to the number of parameters.
\item if $\{v_1,\dots v_k\}$ is an orthonormal basis for $S$ and $\{x_1,\dots x_k\}$ is an orthonormal basis for $S^\perp$, then $\{v_1,\dots v_k,x_1,\dots x_k\}$ is an orthonormal basis for $\R{n}$
\end{enumerate}

\section*{Projection onto Subspaces}
\[ \vec{x} = \proj{x}{S} + \perpp{x}{S^\perp} \]

\textit{Definition:} let $\{v_1,\dots v_k\}$ be an orthonormal basis of the subspace $S\isin\R{n}$. Then \[ \proj{x}{s} = \vec{x}\vec{v_1}\vec{v_1} + \dots + \vec{x}\vec{v_k}\vec{v_k} \]

If $\{\vec{v_1},\dots \vec{v_k}\}$ is orthogonal, then \[ \proj{x}{s} = \frac{\vec{x}\vec{v_1}}{\magv{v_1}^2}\vec{v_1} + \dots + \frac{\vec{x}\vec{v_k}}{\magv{v_k}^2}\vec{v_k} \]

Example: find $P = x -2y + 3z = 0$ projected onto $\vec{x} = \vecthree{3}{1}{2}$

An orthogonal basis is $\vec{v} = \bigg\{ \vecthree{1}{-1}{-1},\vecthree{5}{4}{1} \bigg\}$ thus \[ \proj{x}{p} = \frac{\vec{x}\vec{v_1}}{\magv{v_1}^2}\vec{v_1} = \frac{\vec{x}\vec{v_2}}{\magv{v_2}^2}\vec{v_2} \]

\theorem{Theorem: let $S$ be a subspace of $\R{n}$ and $\vec{x}$ be a vector in $\R{n}$. There exists a unique vector $\vec{s}\isin S$ such that we can find the minimum of $\mags{\vec{x}-\vec{s}}$, and that vector is \[ \vec{s} = \proj{x}{s} \]}

\subsection*{Gram-Schmidts Procedure}
Given a basis $\{w_1,\dots w_k\}$ for a subspace $S$, {\bf Gram-Schmidtz produce} an orthogonal basis $\{v_1,\dots v_k\}$ for $S_i$ where for each $i$ \[ \spanv{w_1,\dots w_i} = \spanv{v_1,\dots v_i}S_i \]

Suppose $\{w_1,\dots w_k\}$ is a basis for $S$. We define $S_i = \spanv{w_1,\dots w_i}$. To find an orthogonal basis, we calculate
\begin{enumerate}
\item $\vec{v_1} = \vec{w_1}$
\item $\vec{v_n} = \perpp{w_n}{S_n}$ where $\perpp{w_n}{S_n} = \perpp{w_n}{v_n-1} - \dots - \perpp{w_n}{v_1}$
\item if $\vec{v}\isin S$, $\vec{v}$ is an orthogonal basis
\end{enumerate}

\textit{Application:} Line fitting, curve fitting

\subsection*{Orthogonal Diagonaliazation}
For a "normal" diagonal, where $D$ is a diagonal matrix and $P$ is invertable $P^{-1}AP = D$

\textit{Definition:} A matrix $A$ is {\bf orthogonally diagonalizable} if there exists an orthogonal matrix $Q$ and a diagonal matrix $D$ such that \[ Q^{-1}AQ = D \]

We need an orthonormal basis of eigenvectors in $\R{n}$

Suppose $A$ is orthogonally diagonalizable. SO $Q^TAQ = D$ or $A = QDQ^T$
\begin{align*}
A^T &= (QDQ^T)^T\\
&= (Q^T)^TD^TQ^T\\
&= QDQ^T\\
&= A
\end{align*}

thus if a matrix is orthogonally diagonalizable, it must be symmetric.

\theorem{Principle Axis Theorem: if $A$ is symmetric, then $A$ is orthogonally diagonalizable.}

Example: $A = \begin{bmatrix}2&-2\\-2&5\end{bmatrix}$

\begin{align*}
\text{det}(A-\lambda I) &= \begin{vmatrix}2-\lambda&-2\\-2&5-\lambda\end{vmatrix}\\
&= (2-\lambda)(5-\lambda) - 4\\
&= (\lambda - 6)(\lambda - 1)
\end{align*}

\[ \text{For } \lambda = 6, A-6I = \begin{bmatrix}-4&-2\\-2&-1\end{bmatrix} = \begin{bmatrix}2&1\\0&0\end{bmatrix}, \vec{v_1} = \frac{1}{\sqrt{5}}\vectwo{1}{-2} \]

\[ \text{For } \lambda = 1, A-1I = \begin{bmatrix}1&-2\\-2&4\end{bmatrix} = \begin{bmatrix}1&-2\\0&0\end{bmatrix}, \vec{v_2} = \frac{1}{\sqrt{5}}\vectwo{2}{1} \]

\begin{align*}
D &= \begin{bmatrix}6&0\\0&1\end{bmatrix}\\
Q &= \begin{bmatrix}\frac{1}{\sqrt{5}}&\frac{2}{\sqrt{5}}\\\frac{-2}{\sqrt{5}}&\frac{1}{\sqrt{5}}\end{bmatrix}
\end{align*}

\theorem{Theorem: If $A$ is symmetric, the eigenvectors corresponding to distinct eigenvalues are orthogonal.}

Example: $A = \begin{bmatrix}0&-1&-1\\-1&0&1\\-1&1&0\end{bmatrix}$

\begin{align*}
\text{det}(A-\lambda I) &= -(\lambda+1)^2(\lambda-2)\\
\lambda &= -1,-1,2
\end{align*}

For \[ \lambda = 2, A-2I = \begin{bmatrix}-2&-1&-1\\-1&-2&-1\\-1&-&-2\end{bmatrix} = \begin{bmatrix}1&0&1\\0&1&-1\\0&0&0\end{bmatrix} \] an eigenvector is \[ \frac{1}{\sqrt{3}}\vecthree{-1}{1}{1} \]

For \[ \lambda = -1, A+1I = \begin{bmatrix}1&-1&-1\\-1&1&1\\-1&1&1\end{bmatrix} = \begin{bmatrix}1&-1&-1\\0&0&0\\0&0&0\end{bmatrix} \] a basis for its eigenspace is \[ \bigg\{\vecthree{1}{1}{0},\vecthree{1}{0}{1}\bigg\} \]

Gram-Schmidtz this: $\vecthree{1}{0}{1} - \frac{1}{2}\vecthree{1}{1}{0} = \frac{1}{\sqrt{2}}\vecthree{1}{-1}{2}$

Thus $A$ can be diagonalized by $P$ into $D$

Note: eigenvectors for distinct eigenvalues are already orthogonal. For eigenvectors of an eigenvalue of high multiplicity, use Grom-Schmidtz to orthogonalize them.

\textit{Application:} Graphing quadratic equations

\begin{align*}
A \to& 2x_1^2 - 4x_1x_2 + 5x_2^2 = 12\\
A &= \begin{bmatrix}x_1&x_2\end{bmatrix}\begin{bmatrix}2&-2\\-2&5\end{bmatrix}\vectwo{x_1}{x_2}\\
\end{align*}

Through orthogonal diagonalization we get \[ A \to 6y_1^2 + y_2^2 = 12 \] where $y_1$ and $y_2$ are specific vectors, thus we can graph $A$ by plotting these vectors on an $x-y$ graph.

\section*{Vector Spaces}
Example {\bf vector spaces} (sets obeying the 10 required properties):
\begin{itemize}
\item $\R{n}$
\item The set of all $m$x$n$ matrices
\item The set of all polynomials on x of max degree $n$
\item The set of all continuous functions on $[a,b]$
\end{itemize}

\subsection*{Subspaces of Vector Spaces}
\textit{Definition:} Let $\mathbb{V}$ be a vector space. A non-empty subset $S$ of $\mathbb{V}$ is a {\bf subspace} if for any $\vec{x}, \vec{y} \isin S, t \isin \mathbb{R}$
\begin{enumerate}
\item $\vec{x} + \vec{y} \isin S$
\item $t\vec{x} \isin S$
\end{enumerate}

Subspaces are vector spaces.

\theorem{Theorem: Let $\mathbb{V}$ be a vector space and let $\{v_1,\dots v_k\} \isin \mathbb{V}$. Then the span $\{v_1,\dots v_k\}$ is a subspace of $\mathbb{V}$}

\subsection*{Linear Independence}
\textit{Definition:} Let $\mathbb{V}$ be a vector space, let $\vec{v_1},\dots \vec{v_k} \isin \mathbb{V}$. Then $\{\vec{v_1},\dots \vec{v_k}\}$ is {\bf linearly independent} if the only solution to \[ a_1\vec{v_1} + \dots + a_k\vec{v_k} = 0 \] is the trivial solution.

\subsection*{Basis and Dimension}
\textit{Definition:} Let $\mathbb{V}$ be a vector space. Then a {\bf basis} is a linearly independant set that spans $\mathbb{V}$.

\theorem{Theorem: If $S$ and $T$ are bases for $\mathbb{V}$, then $S$ and $T$ have the same size.}

\textit{Definition:} The {\bf dimension} of a vector space is the size of its basis.

Let $\mathbb{V}$ be a vector space of dimension n. Then
\begin{enumerate}
\item Any set of more than $n$ vectors is linearly dependant
\item Any set of less than n vectors does not span $\mathbb{V}$
\item Any linearly independant set of n vectors is a basis of v
\end{enumerate}

\subsubsection*{Finding a Basis}
Given $S = \spanv{\vec{v_1},\dots \vec{v_k}}$ find a basis. If there is linear dependence, throw out dependant vectors until you have independence.

\theorem{Theorem: If $\vec{v_k}$ is a non-trivial linear combination of $\{\vec{v_1}, \dots \vec{v_k}\}$, then \[ \spanv{\vec{v_1},\dots \vec{v_k}} = \spanv{\vec{v_1},\dots \vec{v_{k-1}}} \]}

\subsubsection*{Extending a Basis}
Given a basis $B$ of a subspace $S$ of $\mathbb{V}$, extend $B$ to a basis for $\mathbb{V}$.

\theorem{Theorem: If $\vec{v_1},\dots \vec{v_k}$ is linearly independent and $\vec{v_{k+1}}$ is not in the $\spanv{\vec{v_1},\dots \vec{v_k}}$, then $\{\vec{v_1},\dots \vec{v_k}, \vec{v_{k+1}}\}$ is still linearly independent.}

Usually, we can consider adding vectors from the standard basis.

\subsection*{Inner Products}
A dot product in $\R{n}$ gives us the vector's length and orthogonality.

For $C[a,b]$, the {\bf inner product} is $f, g \isin [a,b]$ and \[ <f,g> = \dint{a}{b}{f(x)g(x)}{x} \]

If $<f,g> = 0$, $f$ and $g$ are orthogonal.

\[ \magv{f}^2 = <f,f> = \dint{a}{b}{f^2(x)}{x} \]

To project $f(x)$ onto $\{\sin x,\cos x\}$ we have \[ \frac{<f(x),\sin x>}{\magv{\sin x}^2}\sin x + \frac{<f(x),\cos x>}{\magv{\cos x}^2}\cos x \]

\section*{Complex Numbers}
\theorem{Fundamental Theorem of Algebra: The polynomial equation \[ a_nx^n + a_{n-1}x^{n-1} + \dots + a_1x + a_0 = 0  \] where each $a_i \isin \mathbb{C}$, $a_n \neq 0$ has at least one root in $\mathbb{C}$}

\textit{Definition:} A {\bf complex number} $z$ in standard form is $z = a + bi$ where $a, b \isin \mathbb{R}$. The set of all complex numbers is $\mathbb{C} = \{a + bi \suchthat a, b \isin \mathbb{R}\}$

The real part of $z$ is $a$ and the imaginary part is $b$, thus $\mathbb{R} \isin \mathbb{C}$.

We define two operations:
\begin{enumerate}
\item $a + bi + c + di = (a + c) + (b + d)i$
\item $(a + bi)(c + di) = (ac - bd) + (ad + bc)i$
\end{enumerate}

\subsection*{Division in $\mathbb{C}$}
FInd the inverse of $a + bi$.

The inverse of $a + bi$ is \[ (a + bi)^{-1} = \frac{a - bi}{a^2 + b^2} \] where $a-bi$ is the conjugate $\bar{z}$ and $a^2 + b^2$ is the length, squared.

\subsection*{Complex Conjugate}
\textit{Definition:} If $z = a + bi$, the {\bf conjugate} of $z$ is $\bar{z} = a - bi$

\subsubsection*{Properties of the Conjugate}
\begin{enumerate}
\item $\bar{z+w} = \bar{z} + \bar{w}$
\item $\bar{zw} = \bar{z}\bar{w}$
\item $\bar{\bar{z}} = z$
\item $z + \bar{z} = 2a$
\item $z - \bar{z} = 2bi$
\item $z\bar{z} = a^2 + b^2$
\item $z^{-1} = \frac{\bar{z}}{z\bar{z}}$
\end{enumerate}

\subsection*{Complex Plane}
We can plot complex numbers in the same way as any two-dimensional number, using Re and Im as our axes.

\subsection*{Modulus}
The {\bf modulus} of $z = a + bi$ is $\abs{z} = \sqrt{a^2+b^2}$

\begin{enumerate}
\item $\abs{z} \geq 0$, equality only holds when $ z = 0$
\item $\abs{\bar{z}} = \abs{z}$
\item $\abs{zw} = \abs{z}\abs{w}$
\item $\abs{z + w} \leq \abs{z} + \abs{w}$
\end{enumerate}

\subsection*{Complex Roots}
\begin{align*}
a + bi &= r(\cos\theta + i\sin\theta)\\
\bar{a+bi} &= r\bigl(\cos(-\theta) + i\sin(-\theta)\bigl)
\end{align*}

Example:
\begin{align*}
(1 + i)^{314} &= \sqrt{2}(\cos\frac{\pi}{4} + i\sin\frac{\pi}{4})\\
&= 2^{157}(\cos\frac{314\pi}{4} + i\sin\frac{314\pi}{4})\\
&= 2^{157}(\cos\frac{\pi}{2} + i\sin\frac{\pi}{2})\\
&= 2^{157}i
\end{align*}

\subsection*{Complex Exponentials}
For $f(\theta) = (\cos\theta + i\sin\theta)e^{-i\theta}$
\begin{align*}
f^\prime(\theta) &= (-\sin\theta + i\cos\theta)e^{-i\theta} + (\cos\theta + i\sin\theta)(-ie^{-i\theta})\\
&= e^{-i\theta}(-sin\theta + i\cos\theta - i\cos\theta + \sin\theta)\\
&= 0
\end{align*}
Thus $f(\theta) = C$ for some constant $C$. $f(0) = 1$, thus $C = 1$ and \[ \cos\theta + i\sin\theta = e^{i\theta} \] we can reduce this to Euler's Formula \[ e^{i\pi} = -1 \]

\subsection*{Roots of Complex Numbers}
$z^n = a$ where $a \isin \mathbb{C}$ has $n$ roots (Fundamental Theorem of Algebra states that every polynomial of degree $n \geq 1$ has at least 1 root in $\mathbb{C}$).

Example:
\begin{align*}
z^2 &= i\\
\bigg(\frac{1}{\sqrt{2}} + i\frac{1}{\sqrt{2}}\bigg)^2 &= i\\
e^{i\frac{\pi}{4}}z &= i\\
e^{i\frac{\pi}{2}} &= i\\
i &= i
\end{align*}
Since we can do this with $-\frac{1}{\sqrt{2}}$, we have $z = \frac{1}{\sqrt{2}} + i\frac{1}{\sqrt{2}}, -\frac{1}{\sqrt{2}} - i\frac{1}{\sqrt{2}}$

Example:
\begin{align*}
z^n = a\\
z^n = re^{i\theta}\\
\bigl(se^{i\phi}\bigl)^2 = re^{i\theta}\\
s^ne^{in\phi} = re^{i\theta}\\
\end{align*}
So $s^n = r$ and $n\phi = \theta + 2\pi\mathbb{Z}$, thus $s = \sqrt[n]{r}$ and $\phi = \frac{\theta + 2\pi(\mathbb{Z}+n)}{n}$. Every $n$th root of $\mathbb{Z}$ has the same angle, so we only need $k = 0,\dots n$

Notes:
\begin{enumerate}
\item All roots of $z^n =a$ have the same $r$, so they are on a circle.
\item The roots are equally spaced on the circle. $n$ roots will divide the circle into $n$ equal pieces.
\item The $n$ roots of $a$ are distinct.
\end{enumerate}




\end{document}
