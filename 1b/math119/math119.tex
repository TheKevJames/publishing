\documentclass[12pt]{article}
\usepackage{amsmath,amssymb,parskip,custom}
\usepackage[margin=1in]{geometry}

\begin{document}

\title{MATH 119 - Calculus 2 for Engineering}
\author{Kevin Carruthers}
\date{\vspace{-2ex}Winter 2013}
\maketitle\HRule

\section*{Appoximation Methods}
Some methods (i.e. unintegratable ones) must be approximated, since we can not find exact solutions. There are two such methods for approximation: analytic and numerical.

For {\bf analytic approximation} we make a simplification using the theory of calculus to recognize reasonable approximations, ie \[ \sin x^2 = x^2 \] for any small x.

{\bf Numerical approximation} is the brute force approach. We refer to the definition of a definite integral, and calculate the area of $n$ rectangles of width $\frac{x}{n}$, and height determined by our function.

Obviously, both of these methods can be useful. When using high-powered technology, the numerical approach can reach near-perfection, but analytical methods can still be useful to find approximations without assigning "random" variables or to determine whether a numerical analysis is giving a realistic result.

\subsection*{Linear Approximation}
Linear approximation is also known as Tangent Line Apprximation or Linearization. The definition of a derivative is \[ f^\prime(a) = \lim_{x\to a} \frac{f(x) - f(a)}{x - a} \] For values of $x$ near $a$, the tangent line gives a reasonable approximation to our function.

The linear approximation near $x = a$ is \[ L(x) = f(a) + f^\prime(a)(x-a) \] This can be useful when the function is easy to evaluate at $f(x)$, but difficult to work with at nearby points.

Note that this is similar to the differential approach $f(a + \Delta x) = f(a) + \Delta f$ where $\Delta f \approx f^\prime (a) \Delta x$.

When dealing with $e$, we can generalize our formula as with $f(x) = ae^{b(x+c)}$ we have \[ L(x) = a + ab(x+c) \]

\subsubsection*{Bisection Method}
The most straight-forward approach is to use the Intermediate Value Theorem. Repeated iterations of this will quickly approach the correct root.

Example: find where $x = e^{-x}$.

We have $f(x) = x - e^{-x}$ which is continuous. We see that it is negative at $x = 0$ and positive at $x = 1$, so we know that our answer is between 0 and 1. Since $f(.5) < 0$, we know that our answer is between 0.5 and 1. We then repeat this ad infinatum, until we have found a precise enough value.

\subsubsection*{Newton's Method}
Newton's Method is also know as the Newton-Raphson Procedure, and is based on a simple concept: If we can't solve $f(x) = 0$, solve $L(x) = 0$ instead.

Example: find a root of $x^3 - 2x - 5 = 0$.

With linear approximatinon we have \[ L(x) = f(x_0) + f^\prime(x_0)(x-x_0) \] and $f(2) \approx 0$ and $f^\prime(x) = 3x^2 - 2$ so \[ L(x) = -1 + 10(x-2) = 10x - 21 \] gives us $x = 2.1$ We then take $f(2.1) + f^\prime(2.1)(x-2.1)$ which leads to $x = 2.09457$ through repetition.

More formally, Newton's Method is defined as
\begin{enumerate}
\item Pick $x_0$
\item Do linear approximation at $x_0$
\item Set approximation to zero, solve for $x$
\item Repeat with new $x$.
\end{enumerate}

We can improve this method by calculating a general formula for the repetition step. This is given by \[ x_{n+1} = x_n + \frac{f(x_n)}{f^\prime(x_n)} \]

With a good first guess, this method can converge extremely quickly. If it fails to converge, use bisection to improve your initial guess.

\subsubsection*{Fixed Point Iteration}
A simpler alternative to Newton's Method is to rewrite $f(x) = 0$ as $x = g(x)$. Thus we find an approximate solution via \[ x_{n+1} = g(x_n) \] This converges slower than Newton's Method, but is simpler to calculate.

{\bf Theorem: Convergence of Fixed-Point Iteration}. Suppose that $f(x)$ is defined for all $x in \mathbb{R}$, difderntiable everywhere, and has a bounded derivative at all points. If $f(x) = x$ has a solution, and if $|f^\prime| < 1$ for all values of $x$ within some interval containing the fixed point, then the sequence generated by letting $x_{n+1} = f(x_n)$ will converge with \emph{any} choice of $x_0$.

\subsection*{Polynomial Interpolation}
Suppose we are given $n+1$ points $(x,y)$ and we want to find a polynomial of degree n passing through them. We could either solve this with matrices, or we can use {\bf Newton's Forward Difference Formula}.

With \[ \Delta^m y_n = \Delta^{m-1} y_{n+1} - \Delta^{m-1} y_n \] we can reduce the system to one of size $n-1$. A shorthand way to do this is to create a column of $y$-values, then create a new $n-1$ row of the differeneces between each row, etc. You should end up with a triangular shape.

By iterating through this method until we have an $n = 1$ system, we can solve for each of the coefficients by substituting them into the general polynomial. This will give us a general solution which can then be used for any dataset. This solution is of the form \[ y = y_0 + x\Delta y_0 + ... + x(x - 1)...(x - n + 1)\frac{\Delta^ny_0}{n!} \]

If we have non-unit spacing, this formula becomes \[ y = y_0 + \frac{x - x_0}{h}\Delta y_0 + ... + \frac{(x - x_0)...(x - x_{n-1})}{n!h^n}\Delta^n y_0 \]

Note that this is mostly a generalized version, and you may assume equal unit spacing by $x_z = z$ and $h = 1$. Also note theat $x_n = x_0 + nh$ where $h = \Delta x$.

If we have both non-unit and non-equal spacing, we use {\bf Newton's Divided Differences}, which is generalized from \[ \Delta^m f(x)_n = \frac{\Delta^{m-1} f(x)_{n+1} - \Delta^{m-1} f(x)_n}{x_{n+1} - x_n} \]

\subsection*{Linear Interpolation}
High-order polynomials are known to be innaccurate and oscillate wildly at each end. Based on this, we may sometimes wish to avoid calculating such polynomials. We can use Linear Interpolation for this, by simply using the closest two points to the value we are approximating.

The Lagrange Linear Interpolation Formula is \[ f(x) \approx \bigg(\frac{x - x_1}{x_0 - x_1}\bigg)f(x_0) + \bigg(\frac{x - x_0}{x_1 - x_0}\bigg)f(x_1) \]

\subsection*{Taylor Polynomials}
Taylor Polynomials are basically an extended version of the Linear Approximation formula given more than two points. This allows us to be (normally) more accurate, though high-order Taylor Polynomials completely break down. Note that the first-order Taylor Polynomial is equivalent to the Linear Approximation.

The $n$th order Taylor Polynomial is \[ P_{n,x_0}(x) = f(x_0) + (x - x_0)f^\prime(x_1) + \frac{(x - x_0)^2}{2!} f^{\prime\prime}(x_2) + ... + \frac{(x - x_0)^n}{n!}f^{n\prime}(x_n) \]

More generally, we have \[ P_{n,x_0}(x) = \sum_{k=0}^n \frac{f^k(x_0)}{k!}(x - x_0)^k \]

Note that using {\bf MacLaurin's Approach} we can derive this polynomial and prove that any Taylor Polynomial is unique. Thus if we ever find a polynomial which matches the values of $f$ and its first $n$ derivatives at $x_0$, this polynomial \emph{must} be a Taylor Polynomial, regardless of how we obtained it.

Since MacLaurin derived Taylor Polynomials centered at 0, we refer to such a polynomial as a {\bf MacLaurin Polynomial}, which has the form \[ P_{n,0}(x) = \sum_{k=0}^n \frac{f^k(0)}{k^2} x^k \]

\subsubsection*{Taylor's Theorem with Integral Remainders}
It's important to determine how accurate our approximations are. We can find the magnitude of the error as $|f(x) - P_{n,x_0}(x_0)|$, but since we do not know the value of $f(x)$, we cannot calculate this exactly. As such, we'll find the upper bound of the error.

If $f(x)$ has $n + 1$ derivatives at $x_0$, then \[ f(x) = \sum_{k=0}^n f^k(x_0)(x - x_0)^k + R_{n,x_0}(x) \] where \[ R_{n,x_0}(x) = \dint{x_0}{x}{\frac{(x-t)^n}{n!}f^{n+1}(t)}{t} \]

Unfortunately, we can't evaluate this! As such, we will find an upper bound for the error, which may or may not be approximately equal to the error. If we can bound \[ |f^{n+1}(t)| \leq K \] for all $t$ between $x$ and $x_0$ then we can find {\bf Taylor's Inequality} by
\begin{align*}
E &= |f(t) - P_{n,x_0}(x)|\\
  &= |R_n(x)|\\
  &= \bigg|\dint{x_0}{x}{\frac{(x-t)^n}{n!}f^{n+1}(t)}{t}\bigg|\\
  &\leq \dint{x_0}{x}{|\frac{(x-t)^n}{n!}f^{n+1}(t)}}{t}|\\
  &\leq \dint{x_0}{x}{\frac{{|x-t|}^n}{n!}|f^{n+1}(t)|}{t}\\
  &\leq K\dint{x_0}{x}{\frac{{|x-t|}^n}{n!}}{t}\\
  &\leq K\frac{{|x-t|}^{n+1}}{(n+1)!}\ \bigg|_{x_0}^x\\
  &\leq K\frac{{|x-x_0|}^{n+1}}{(n+1)!}\\
E = |R_n(x)| &\leq K\frac{{|x-x_0|}^{n+1}}{(n+1)!}
\end{align*}

\subsubsection*{Approximation of Integrals with Taylor Polynomials}
When we're dealing with integrals, it turns out we can use subsitution to simplify our work. For example, given the integral $\dint{0}{x}{e^{t^2}}{t}$, we can let $u = t^2$ and find the Taylor Polynomial for that. $P_{2,0} (u) = 1 + u + u^2$, so $P_{2,0}(t^2) = 1 + t^2 + t^4$. Thus we can approximate $\dint{0}{x}{e^{t^2}}{t} = \dint{0}{x}{1 + t^2 + t^4}{t}$ which is easy to evaluate.

We can introduce error into this as $e^u = P_{2,0}(u) + R_2(u)$ where $R$ is given by $R_2(u) \leq K \frac{|u|^3}{3!}$ where $|f^3(q)| \leq K$ for any $q$ between 0 and $u$.  Since $f(u) = e^u$, we have $|f^3(q)| = e^q$. We must bound this function, so we chose values approximately close to our desired answer. In this case, if we want to have $x = .5$, we must have $u in [0, .25]$.

To find an upper bound for this, we have $|f^3(u)| = e^u \leq e^{.25} < 2$. This can be used for our value of $K$, thus giving us $|R_2(u)| \leq 2\frac{|u|^3}{3!}$ on our interval. With substitution, we get $|R_2(u)| \leq \frac{1}{3}t^6$ and our absolute error is less than $\frac{1}{27}x^7$.

In summary, we have found $\dint{0}{x}{e^{t^2}}{t} = x + \frac{x^3}{3} + \frac{x^5}{10} \pm \frac{x^7}{21}$ for $x in [-\frac{1}{2},\frac{1}{2}]$. Given some specific value fo $x$, we can find this value numerically.

\section*{Infinite Series}
Assume we take the limit of some error term. If this limit approaches zero, we can see that by adding more terms to our polynomial, we increase the accuracy of our approximation to perfection. Based on this, we can see that some functions can expressed as {\bf infinite sums}, which are technically the limit of sums, not a sum itself.

For example, we have $\sin x = \displaystyle\sum_{k=0}^n \frac{(-1)^k x^{2k+1}}{(2k+1)!} + R_{2n+1}(x)$. Taking limits gives us $\sin x = \displaystyle\sum_{k=0}^\infty \frac{(-1)^k x^{2k+1}}{(2k+1)!}$. This is refered to as the Taylor Series centered at zero of sin$x$, or the MacLaurian Series of sin$x$.

Generally, since $f(x) = \displaystyle\sum_{k=0}^n \frac{(f)^k x_0}{k!}(x - x_0)^k + R_n(x)$, if the remainder approaches zero we have $f(x) = \displaystyle\sum_{k=0}^\infty \frac{(f)^k x_0}{k!}(x - x_0)^k$.

For some functions (example: $\frac{1}{1+x}$), this is only applicable on certain (between 0 and 1) intervals. With some work, we can see that $\frac{1}{1+x} = \displaystyle\sum_{k=0}^\infty (-1)^kx^k$ for $x in (0,1)$, but this only gives us a partial answer, and not easily at that!

\subsection*{Convergence of Infinite Series}
\definition An infinite series of constants $a_k$ is defined as \[ \sum_{k=0}^\infty a_k = \lim_{n\to\infty} \sum_{k=0}^n a_k \]

In other words, given a sequence of numbers $a_k$, we can construct the sequence of partial sums $s_n$ (i.e. $a_0, a_0 + a_1, a_0 + a_1 + a_2, ...$) If this sequence converges ($\lim_{n\to\infty} s_n = s$), then we say that the series $\displaystyle\sum_{k=0}^\infty a_k$ converges, and its sum is $s$. Otherwise, it diverges.

Note that we can also start at some $k \neq 0$, as this will not affect whether the series converges or not. However, it \emph{will} affect the value of the sum.

\subsection*{Determining Convergence}
\subsubsection*{Geometric Series}
A geometric series has the form $\displaystyle\sum_{k=0}^\infty ar^k = a + ar + ar^2 + ...$ We can redefine any geometric series with the equality  \[ \sum_{k=0}^\infty ar^k = \lim_{n\to\infty} \frac{a(1-r^n)}{1-r} \]

For any $|r| < 1$ the sequence converges, otherwise it will diverge. We can thus conclude $\displaystyle\sum_{k=0}^\infty ar^k = \frac{a}{1-r}$ if $|r| < 1$.

In case of the series having the wrong index to use this formula easily, we have two options: we can either reindex the equation (which is a useful but tedious skill), or think of our $a$ as the ``first term'' and $r$ as the ``common ratio'', and simply calculate those values.

We also note that a series can diverge even if $\displaystyle\lim_{k\to\infty} a_k = 0$. For example, the infinite series of $\frac{1}{k}$ diverges. Aside: this series is known as the harmonic series, and all harmonic series diverge.

\subsubsection*{Divergence Test}
Since $\sum a_k$ can only converge if $\displaystyle\lim_{k\to\infty} = 0$, we can say that if $\displaystyle\lim_{k\to\infty} a_k \neq 0$ then $\sum a_k$ diverges. We can use this test to determine whether a series will diverge, but not whether it will converge (i.e. the converse may or may not be true).

\subsubsection*{Integral Test}
$\displaystyle\sum_{k=k_0}^\infty a_k$ converges if and only if $\dint{k_0}{\infty}{f(x)}{x}$ converges, where $f(x) = a_k > 0$. For this test, we must chose $f$ carefully: it must be continuous and positive, with $f \to \infty$ as $x \to \infty$.

\subsubsection*{P-Series}
$\sum \frac{1}{k^p}$ converges if $p > 1$ and otherwise diverges. Note that the harmonic series is a (diverging) p-series.

\subsubsection*{Comparison Test}
Suppose we are given a series $\sum a_k$. If we can identify a second series $\sum b_k$ such that $a_k \leq b_k$ and $\sum b_k$ converges, then $\sum a_k$ also converges. If $a_k \geq b_k$ and $\sum b_k$ diverges, then $\sum a_k$ diverges as well.

Note: $\sum b_k$ needs to be a series whose behaviour we understand, and is usually a geometric or p-series.

Examples: $\sum \frac{\ln k}{k} > \sum \frac{1}{k}$, so both diverge. $\sum \frac{1}{k^2 + 2} < \frac{1}{k^2}$, so both converge.

\subsubsection*{Limit Comparison Test}
If $\displaystyle\lim_{k\to\infty} \frac{a_k}{b_k} = L$, where $L$ is a constant and $a_k \geq 0$, then $\sum a_k$ and $\sum b_k$ either both converge or both diverge.

Example: for $\sum \frac{1}{k^2 - 1}$, we can't use the comparison test. With this test, we see that $L = 1$, and so they both converge. Similarly, $\sum \frac{1}{3\sqrt{k}+2}$ diverges, since $\sum \frac{1}{\sqrt{k}}$ diverges and $L = \frac{1}{3}$.

\subsubsection*{Alternating Series Test (Leibniz Test)}
Consider a series $\sum (-1)^k a_k$ with terms $a_0 - a_1 + a_2 - a_3 ...$. If $\displaystyle\lim_{k\to\infty} a_k = 0$ and the series is eventually decreasing, then the series converges.

Example: $\sum \frac{(-1)^k)}{\sqrt{k}}$ converges, despite being quite similar to a diverging p-series.

\subsection*{Absolute Convergence vs Conditional Convergence}
A converging series is only absolutely convergent if $\sum |a_k|$ also converges, otherwise it is conditionally convergent.

Aside: if you re-order the sums of a conditionally converging series, you can make it converge to a different sum!

\subsubsection*{Ratio Test}
Suppose $\displaystyle\lim_{k\to\infty} \bigg|\frac{a_{k+1}}{a_k}\bigg| = L$. If $L < 1$, then the series is \emph{absolutely} convergent. If $L > 1$, then the series is divergent. If $L = 1$, then the test fails and we must use another.

Note that this test is usually all we need to determine the kind of series a given Taylor Series is, and that finding $L = 1$ is more or less the only reason we would need another test.

A subset of this test is the {\bf root test}, which uses the limit $\displaystyle\lim_{k\to\infty} {|a_k|}^\frac{1}{k}$. This test is useful when everything appears raised to the power of $k$, but that is a rare structure to encounter and we can mostly ignore this test.

\subsection*{Power Series}
A power series is the general form of the Taylor Series, given by \[ \sum_{k=0}^\infty c_k (x - x_0)^k = c_0 + c_1(x-x_0) + c_2(x-x_0)^2 + ... \]

Applying the ratio test, we see that the series converges absolutely if $|x - x_0| \displaystyle\lim_{k\to\infty} \bigg|\frac{c_{k+1}}{c_k}\bigg| < 1$. We can rearrange this to get $|x - x_0| < \displaystyle\lim_{k\to\infty} \bigg|\frac{c_k}{c_{k+1}}\bigg|$. If we refer to this limit as $R$, we see that
\begin{itemize}
\item If $R = 0$, the series converges only at $x = x_0$.
\item If $R = \infty$, the seres converges for all $x$.
\item If $0 < R < \infty$, the series absolutely converges for $x in (x_0 - R, x_0 + R)$ and diverges for $x in (-\infty,x_0 - R) \cup (x_0 + R, \infty)$. At $x = x_0 \pm R$, the test gives no information.
\end{itemize}

We refer to $R$ as the {\bf radius of convergence}. We also refer to the {\bf interval of convergence}, for which we will need to know the behaviour at the given boundaries (i.e. $x_0 \pm R$).

Example: For the series $\sum \frac{(x-3)^k}{k4^k}$, we use the ratio test to find $\displaystyle\lim_{k\to\infty} \bigg|\frac{a_{k+1}}{a_k}\bigg| = \frac{|x-3|}{4}$. Thus this series converges if $
\frac{|x-3|}{4} < 1$ or if $|x-3| < 4$ (so the radius of convergence is 4). Substituting in $x = -1, 7$, we find that this series converges on $x = [-1, 7)$.

\subsubsection*{Manipulation of Power Series}
If the series $\sum c_k (x-x_0)^k$ has a radius of convergence $R$, then we can
\begin{itemize}
\item differentiate it (term-by-term)
\item integrate it (term-by-term)
\item multiply it by a constant (term-by-term)
\item add it (term-by-term) to another series of radius of convergence greater than or equal to $R$
\end{itemize}
and the result will also have radius of convergence $R$.

Note that although we can perform any of these functions without changing the radius of convergence, the interval of convergence may change.

Furthermore, since all Taylor series are unique, if we perform these operations on a Taylor series, the results will be the Taylor series for the differentiated/integrated/multiplied/added functions!

For example, $\sum x^k = 1 + x + x^2 + ...$ has a sum of $\frac{1}{1-x}$. Through differentiation, we see that $\frac{1}{(1-x)^2} = \sum kx^{k-1} = 1 + 2x + 3x^2 + ...$, which has the same radius of convergence (1).

This theorem can be extended to include multiplication and division of two series, but these operations are not carried out term-by-term, so we will rarely be able to find the full series.

There are four extremely useful functions to remember, which can be used as the building blocks for most other functions:
\begin{itemize}
\item $\frac{1}{1-x} = \sum x^k$ for $|x| < 1$
\item $e^x = \sum \frac{x^k}{k!}$
\item $\sin x = \sum (-1)^k \frac{x^{2k+1}}{(2k+1)!}$
\item $\cos x = \sum (-1)^k \frac{x^{2k}}{(2k!)}$
\end{itemize}

\section*{Big-O Order Symbol}
\definition Given two function $f$ and $g$, we say that ``$f$ is of order $g$ as $x \to x_0$'' and write $f(x) = O(g(x))$ as $x \to x_0$ if there exists a constant $A$ greater than 0 such that $|f(x)| \leq A |g(x)|$ on some interval around (but not necessarily including) $x_0$.

This allows us to say things like $\sin x = O(x)$ and $\frac{\sin x}{x} = O(x)$, though it could be argued that this is a terrible misuse of the equals sign, and that $\epsilon$ should be used instead.

\subsection*{Order Notation and Taylor's Inequality}
We can change our definition of Taylor's Inequality to use Big-O notation with \[ f(x) = P_{n,x_0} (x) + O((x-x_0)^{n+1}) \]

This gives us the ability to do things like this: since $\sqrt{1 + x} = 1 + \frac{x}{2} + O(x^2)$ and $\sin x = x + O(x^3)$, we have $\sqrt{1+x} + \sin x = 1 + \frac{3x}{2} + O(x^2) + O(x^3)$, which can be simplified further as $O(x^2) + O(x^3) = O(x^2)$ (i.e. only the minimum term matters).

Note that substitution works ``normally'' ($\sqrt{1 + u^4} = 1 + \frac{u^4}{2} + O(u^8)$), as does multiplication, (long) division, etc.

Generally, the Big-O notation can be thought of as a placeholder for all the omitted terms in out series. Alternatively, we have
\begin{itemize}
\item $C O(x^n) = O(x^n)$
\item $O(x^m) + O(x^n) = O(x^{\min(m,n)})$
\item $O(x^m)(x^n) = O(x^{m+n})$
\item $O^n(x^m) = O(x^{mn})$
\item $\frac{O(x^m)}{x^n} = O(x^{m-n})$
\end{itemize}
Note that there is no general rule for simplifying $\frac{O(x^m)}{O(x^n)}$, since we cannot determine what the lowest value of $n$ is.

\subsubsection*{Limit Evaluation}
We can use Big-O notation to help us evaluate certain limits. For example \[ \lim_{x\to\infty} \frac{\sin x}{x} = \lim_{x\to\infty} \frac{x+O(x^3)}{x} = \lim_{x\to\infty} 1 + O(x^2) = 1 \]

\section*{Multivariate Calculus}
While so far we have dealt only with functions of one variable, we can also have a function dependant on multiple variables. Note that if we are attempting to draw graphs of these, we must either draw in three-or-more dimensions or use contour plots.

To create a contour plot, we simply set $f(x,y) = K$, and for each value of $K$ draw a level curve of $f$.

\subsection*{Multivariate Limits}
Limits can behave strangely when there are multiple variables. Take the function $f(x,y) = \frac{2xy}{x^2 + y^2}$. Though the limit (at 0) is 0 if we set x or y to zero and take the limit of the other, if we set $x = y$ the limit becomes 1. Similarly, if we set $y = x^2$ for some otherwise limitable functions, we get a new answer. We must conclude that the limits of these functions do not exist at those locations.

The only effective tool for determining whether these limits exist is the Squeeze Theorem, but this course will not be covering that topic.

\subsection*{Partial Derivatives}
We derive a function of two variables by pretending one of the variables is a constant and differentiating the resulting single-variable function the usual way. More technically, we have:

\definition The partial derivative of $f(x,y)$ with respect to $x$ at the point $(a,b)$ is \[ f_x(a,b) = \lim_{h\to 0} \frac{f(a+h, b) = f(a,b)}{h} \] if this limit exists (otherwise $f$ is not differentiable). Providing the limit exists, the partial derivative with respect to $y$ is found similarly.

Note: we can write $f_x$ as $\frac{\partial f}{\partial x}$, depending on which style of notation we prefer.

\subsubsection*{High-Order Partial Derivatives}
We can find higher-order derivatives such as $f_{xxx}, f_{xyxyxy}$, and $f_{yyyx}$ by deriving multiple times, in the same fashion. Important note: the order does not matter! $f_{xy} = f_{yx}$.

\subsection*{Taylor Series}
In single-variable calculus, the most basic application of the derivative is the construction of the tangent line. With two variables, we have the {\bf tangent plane}, though we continue to refer to the process as linearization.

The Taylor series for two variables is given as $f(x,y) = f(x_0,y_0) + f_x(x_0,y_0)(x-x_0) + f_y(x_0,y_0)(y-y_0) + \frac{1}{2!}f_{xx}(x_0,y_0)(x-x_0)^2 + \frac{1}{2!}f_{xy}(x_0,y_0)(x-x_0)(y-y_0) + f_{yy}(x_0,y_0)(y-y_0)^2 + ...$ If we let $P = (x_0, y_0)$, $h = (x-x_0)$, and $k = (y-y_0)$, we have the slightly more legible \[ f(x,y) = \bigg( f(P) \bigg) + \bigg( f_x(P)h + f_y(P)k \bigg) + \frac{1}{2!} \bigg( f_{xx}(P)h^2 + 2f_{xy}(P)hk + f_{yy}(P)k^2 \bigg) + ... \] where each set of terms has coefficients given by Pascal's Triangle.

When dealing with error, you may realize we may sometimes end up with $O(x^2) + O(y)$ etc. We commonly assume that $y$ is of order $x$ and thus write this simply as $O(x)$.

\subsection*{Tangent Plane and Differentials}
We have the equation for a tangent plane from our general Taylor Series equation given as \[ f(x,y) \approx f(P) + f_x(P)h + f_y(P)k \] though it is generally expressed in its differential form. We take $\Delta f = f(x,y) - f(x_0,y_0), \Delta x = x - x_0$, and $\Delta y = y - y_0$ to get \[ \Delta f \approx f_x(P) \Delta x + f_y(P) \Delta y \] or in a more exact and concise form \[ \dd f = f_x \dd x + f_y \dd y \] which is referred to as the {\bf total differential of $f$}.

\subsection*{Parametric Equations}
The parametric equation for a circle of radius $a$ is $x(t) = a\cos t, y(t) = a\sin t$. Note that this has the added benefit of defining the two functions without relying on each other, and of adding a time variable. We can combine these functions and write them as a vector function $\vec{r} = (x,y) = (a\cos t, a\sin t),$ $t in [0,2\pi]$. $\vec{r(t)}$ gives both the path (direction, speed, start and end points) and the curve (the line in $\mathbb{R}^2$).

\subsubsection*{Chain Rule}
The chain rule is a visualization of the reliance of variables on one another. When we transform scalar functions in a vector function, we change what each function relies on. For example, we maybe end up with $z$ relying on $x$ and $y$, each relying on their own $s$ and $t$.

Suppose we may write either $z = f(x,y)$ or $z = g(t)$. Then we have $\dd f = \frac{\partial f}{\partial x} \dd x + \frac{\partial f}{\partial y} \dd y$ which could equally be written with the $f$s replaced by $z$s. Diving by $\dd t$ gives \[ \frac{\dd z}{\dd t} = \frac{\partial z}{\partial x} \frac{\dd x}{\dd t} + \frac{\partial z}{\partial y} \frac{\dd y}{\dd t} \]

If $f$ depends on multiple variables, we can write this as $\frac{\partial z}{\partial t} = \frac{\partial z}{\partial x} \frac{\partial x}{\partial t} + \frac{\partial z}{\partial y} \frac{\partial y}{\partial t}$, $\frac{\partial z}{\partial s} = \frac{\partial z}{\partial x} \frac{\partial x}{\partial s} + \frac{\partial z}{\partial y} \frac{\partial y}{\partial s}$. This {\bf Non-Standard Chain Rule} is useful whenever our chain is non-uniform. For example, if $f$ relies on $x$ and $y$, $x$ relies on $s$ and $t$, and $y$ relies only on $s$.

\subsection*{Gradient Vector}
Since any function $f: \mathbb{R}^2 \to \mathbb{R}$ has two derivatives which are usually paired together, we write \[ \nabla f = (f_x, f_y) \] This notation allows us to rewrite a few of our formulaes
\begin{itemize}
\item The linear approximation of $f(x,y) \approx f(a,b) + f_x(a,b)(x-a) + f_y(a,b)(y-b)$ can be written as $f(\vec{r}) = f(\vec{a}) + \nabla f(\vec{a})(\vec{r} - \vec{a})$
\item The chain rule $\frac{\dd z}{\dd t} = \frac{\partial z}{\partial x} \frac{\dd x}{\dd t} + \frac{\partial z}{\partial y} \frac{\dd y}{\dd t}$ can be written as $\nabla f(\vec{r}(t)) \vec{r}^\prime (t)$
\end{itemize}

Basically, the gradient vector acts as the ``complete'' derivative (since it's comprised of the \emph{partial} derivatives).

\subsection*{Directional Derivatives}
\definition The directional derivative of $f(x,y)$ in the direction of a unit vector $\vec{u} = (u_0, u_1)$ at the point $a = (a,b)$ is denoted by $D_{\vec{u}} f(a,b)$ and defined as \[ D_{\vec{u}} f(a,b) = \lim_{h\to 0} \frac{f(a + hu_0, b + hu_1) - f(a,b)}{h} \] Note that $\inint{D_{\vec{u}} f(a,b)}{f} = f(a + hu_0, b + hu_1)$. We can also find \[ D_{\vec{u}} f(a,b) = \nabla f(x,y) \cdot \vec{u} \]

Example: for $f(x,y) = x^2 + y^2$, find the slope at point $(1,-1)$ in the direction of the vector $(3,4)$.

\begin{align*}
D_{\vec{u}} f(a,b) &= \nabla f \cdot \vec{u}\\
                   &= (2,-2) \cdot \bigl( \frac{3}{5}, \frac{4}{5} \bigl)\\
                   &= -\frac{2}{5}
\end{align*}

\subsubsection*{Meaning of the Gradient Vector}
Since $\vec{u}$ is a unit vector, we have $\nabla f \cdot \vec{u} = ||\nabla f|| \cos\theta$. Thus the directional vector is at its maximum when the unit vector is in the same direction as $\nabla f$. So we can see that, at any point, the direction and magnitude of the steepest slope of the graph of $f$ are given the vector $\nabla f$.

\subsection*{Optimization Techniques}
\subsubsection*{Unconstrained Optimization}
To seek the maxima and minima of a multivariate function, we must first redefine these in our new environment:

\definition A function $f(x,y)$ has a {\bf local maximum} at $(x_0, y_0)$ if $f(x_0, y_0) \geq (x_ y)$ for all $(x,y)$ in some disk around $(x_0,y_0)$. Similarly, a function $f(x,y)$ has a {\bf local minimum} at $(x_0, y_0)$ if $f(x_0, y_0) \leq (x_ y)$ for all $(x,y)$ in some disk around $(x_0,y_0)$.

Since these points must occur at such a place where the tangent plane is horizontal (i.e. $\nabla f = \vec{0}$) or undefined. Thus our critical points are points in which either \emph{both} $f_x$ and $f_y$ are zero or at least one of them is undefined.

By taking the Taylor series at $f$ centered at $(x_0,y_0)$, minpulating it into a quadratic equation, and applying the quadratic equation, we get the {\bf Second-Derivative Test for Local Extrema}:

Let $P$ be a critical point of $f(x,y)$, i.e. $(x_0,y_0)$ and let $D(x,y) = f_{xx}f_{yy} - f_{xy}^2$
\begin{itemize}
\item If $D(P) > 0$, then $f$ has an extremum at $P$. If $f_{xx}(P) < 0$, this extremum is a maximum, otherwise it is a minimum.
\item If $D(P) < 0$, then $f$ does not have an extremum at $P$ (i.e. it has a {\bf saddle point} instead).
\item If $D(P) = 0$, this test gives no conclusion. In this case, we may need to look at third-order derivatives, etc. Fortunately, in this case the extrema are usually obvious.
\end{itemize}

\subsubsection*{Constrained Optimization}
A constrained optimization is one in which we attempt to find the local extrema of a function $f(x,y)$ given the additonal constraint $g(x,y) = K$.

To solve these, we use the {\bf Method of Lestrange}: we solve for the values of $x$ and $y$ in the system of equations $\nabla f = \lambda \nabla g$ and $g(x,y) = K$. Note that though this method works with any number of inputs, it merely locates the critical points but does not classify them.

\subsection*{Iterated Integrals}
When integrating in multiple dimensions, it can sometimes be beneficial to integrate multiple times. For example, the volume of a solid is \[ \dintd{c}{d}{a}{b}{f(x,y)}{x}{y} \]

We call this an iterated integral, and evaluate our integrals one at a time. Since it is easier, we evaluate from the inside out, though, geometrically speaking, the order of evaluating our integrals does not matter. From this, we see that \[ \dintd{c}{d}{a}{b}{f(x,y)}{x}{y} = \dintd{a}{b}{c}{d}{f(x,y)}{y}{x} \]

It is also possible to factor our integrals, assuming we can create single-variable functions from our $f$. With this, we have \[ \dintd{c}{d}{a}{b}{g(x)h(y)}{x}{y} = \dint{a}{b}{g(x)}{x} \dint{c}{d}{h(y)}{y} \]

In practice, our integrals tend to be slightly more convoluted than this: at the very least, the limits of integration of the inner integral tend to be functions rather than integers. This can sometimes make it difficult to change the order of integration. We must always have constants as our bound for the outermost integral, since otherwise our integral will be indefinite.

Example: find the volume of the solid bounded by $z = xy$, $y = x$, and $y = \sqrt{x}$.

We have two options in this case: $\dint{R}{}{xy}{A} = \dintd{0}{1}{x}{\sqrt{x}}{xy}{y}{x} = \dintd{0}{1}{y^2}{y}{xy}{x}{y}$.

Example: evaluate $\dintd{0}{2}{x^2}{4}{x^3\sin(y^3)}{y}{x}$.

It is impossible to calculate $\inint{\sin(y^3)}{y}$, thus we must change the order of integration. We can see that the original domain is $x = (0,2), y = (x^2,4)$, thus we can see our new domain is $y = (0,4), x = (0, \sqrt{y})$. From this we have $\dintd{0}{2}{x^2}{4}{x^3\sin(y^3)}{y}{x} = \dintd{0}{4}{0}{\sqrt{y}}{x^3\sin(y^3)}{x}{y}$ which is easy to evaluate.

\subsubsection*{Double Integrals in Polar Coordinates}
Sometimes, it is easier to evaluate a function if it is expressed in the polar coordinate system. By translating our coordinates, we can see that \[ \inintd{f(x,y)}{x}{y} = \inintd{f(r\cos\theta, r\sin\theta)\cdot r}{r}{\theta} \]

\subsubsection*{The Change-of-Variable Formula}
Since it can be sometimes be beneficial to use axes other that $(x,y)$ or $(r,\theta)$, we can see how a generalized formula would be beneficial (for example, if our volume is bounded by such curves as $xy = 5, xy = 10, \frac{y}{x} = 1$, and $\frac{y}{x} = 2$, we could see how defining $u = xy, v = \frac{y}{x}$ and solving on $(u,v)$ could be useful).

This equation is generalized with \[ \inintd{f(x,y)}{x}{y} = \inintd{f[x(u,v),y(u,v)] \bigg|\frac{\partial (x,y)}{\partial (u,v)}\bigg|}{u}{v} \] where the {\bf Jacobian} of the transformation $J$ is \[ \frac{\partial (x,y)}{\partial (u,v)} = \begin{vmatrix}\frac{\partial x}{\partial u} & \frac{\partial x}{\partial v} \\ \frac{\partial y}{\partial u} & \frac{\partial y}{\partial v}\end{vmatrix} = \frac{\partial x}{\partial u}\frac{\partial y}{\partial v} - \frac{\partial x}{\partial v}\frac{\partial y}{\partial u} \] Note that $\bigg|\frac{\partial (x,y)}{\partial (u,v)}\bigg|$ is the \emph{absolute value} of the determinant of the matrix of partial derivatives. We can think of the Jacobian as a necessary factor which compensates for the distortion of the domain which occurs when we move from one coordinate system to another.

One important restriction to this is that the transformation must be invertible on the domain of integration, or, in other words, we must not have $J = 0$ at any point within our integration domain.

All of the above-mentioned ideas can be applied to more-than-double integration. For example, we can use triple integrals to find the volume of a sphere.

Obviously, a spherical coordinate system would be the most valuable tool for this. We have
\begin{align*}
V &= \int\!\!\!\!\int\!\!\!\!\intone{V}\\
  &= \int\!\!\!\!\int\!\!\!\!\inint{\rho^2 \sin\phi}{\rho}{\rm d}\theta{\rm d}\phi\\
  &= \int_0^\pi\!\! \int_0^{2\pi}\!\! \dint{0}{r}{\rho^2 \sin\phi}{\rho}{\rm d}\theta{\rm d}\phi\\
  &= \dint{0}{\pi}{\sin\phi}{\phi} \displaystyle\int_0^{2\pi} \theta \dint{0}{r}{\rho^2}{\rho}\\
  &= \bigl(2\bigl) \bigl(2\pi\bigl) \bigg(\frac{r^3}{3}\bigg) = \frac{4\pi r^3}{3}
\end{align*}
\end{document}
